WARNING:tensorflow:

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U "tensorflow==1.*"`

  Otherwise your code may be broken by the change.

  
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I1001 09:57:40.287593 140265095153472 model_imports.py:47] Importing lingvo.tasks.asr.params
I1001 09:57:40.326028 140265095153472 model_registry.py:124] Registering models from module: lingvo.tasks.asr.params.librispeech
I1001 09:57:40.332140 140265095153472 model_imports.py:47] Importing lingvo.tasks.car.params
I1001 09:57:40.385099 140265095153472 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.kitti
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

W1001 09:57:40.403188 140265095153472 module_wrapper.py:137] From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

I1001 09:57:40.457968 140265095153472 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.waymo
I1001 09:57:40.463727 140265095153472 model_imports.py:47] Importing lingvo.tasks.image.params
I1001 09:57:40.466818 140265095153472 model_registry.py:124] Registering models from module: lingvo.tasks.image.params.mnist
I1001 09:57:40.466947 140265095153472 model_imports.py:47] Importing lingvo.tasks.lm.params
I1001 09:57:40.471875 140265095153472 model_registry.py:124] Registering models from module: lingvo.tasks.lm.params.one_billion_wds
I1001 09:57:40.475248 140265095153472 model_imports.py:47] Importing lingvo.tasks.mt.params
I1001 09:57:40.481979 140265095153472 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmt14_en_de
I1001 09:57:40.489315 140265095153472 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmtm16_en_de
I1001 09:57:40.490495 140265095153472 model_imports.py:47] Importing lingvo.tasks.punctuator.params
I1001 09:57:40.493106 140265095153472 model_registry.py:124] Registering models from module: lingvo.tasks.punctuator.params.codelab
2019-10-01 09:57:40.493745: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-01 09:57:40.521804: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300050000 Hz
2019-10-01 09:57:40.523155: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6007980 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-10-01 09:57:40.523179: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-10-01 09:57:40.529144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-10-01 09:57:40.638364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 09:57:40.639257: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x60cf240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-10-01 09:57:40.639284: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2019-10-01 09:57:40.640554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 09:57:40.641270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2019-10-01 09:57:40.645448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-10-01 09:57:40.727559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-10-01 09:57:40.766770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-10-01 09:57:40.783064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-10-01 09:57:40.874514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-10-01 09:57:40.933762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-10-01 09:57:41.093950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-10-01 09:57:41.094257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 09:57:41.095152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 09:57:41.095843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-01 09:57:41.097000: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-10-01 09:57:41.099373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-01 09:57:41.099395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-10-01 09:57:41.099403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-10-01 09:57:41.100509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 09:57:41.101285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-10-01 09:57:41.102013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:local/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
2019-10-01 09:57:41.117785: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job local -> {0 -> localhost:34393}
2019-10-01 09:57:41.121654: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:34393
I1001 09:57:41.121944 140265095153472 trainer.py:1515] Job controller start
I1001 09:57:41.157405 140265095153472 base_runner.py:57] ============================================================
I1001 09:57:41.180541 140265095153472 base_runner.py:59] allow_implicit_capture : NoneType
I1001 09:57:41.181033 140265095153472 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I1001 09:57:41.181123 140265095153472 base_runner.py:59] cluster.add_summary : NoneType
I1001 09:57:41.181204 140265095153472 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I1001 09:57:41.181281 140265095153472 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I1001 09:57:41.181374 140265095153472 base_runner.py:59] cluster.controller.devices_per_split : 1
I1001 09:57:41.181439 140265095153472 base_runner.py:59] cluster.controller.gpus_per_replica : 0
I1001 09:57:41.181523 140265095153472 base_runner.py:59] cluster.controller.name : '/job:local'
I1001 09:57:41.181582 140265095153472 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I1001 09:57:41.181671 140265095153472 base_runner.py:59] cluster.controller.replicas : 1
I1001 09:57:41.181758 140265095153472 base_runner.py:59] cluster.controller.targets : ''
I1001 09:57:41.181844 140265095153472 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I1001 09:57:41.181923 140265095153472 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I1001 09:57:41.181997 140265095153472 base_runner.py:59] cluster.decoder.devices_per_split : 1
I1001 09:57:41.182062 140265095153472 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I1001 09:57:41.182133 140265095153472 base_runner.py:59] cluster.decoder.name : '/job:local'
I1001 09:57:41.182200 140265095153472 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I1001 09:57:41.182275 140265095153472 base_runner.py:59] cluster.decoder.replicas : 1
I1001 09:57:41.182345 140265095153472 base_runner.py:59] cluster.decoder.targets : ''
I1001 09:57:41.182414 140265095153472 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I1001 09:57:41.182484 140265095153472 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I1001 09:57:41.182545 140265095153472 base_runner.py:59] cluster.evaler.devices_per_split : 1
I1001 09:57:41.182627 140265095153472 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I1001 09:57:41.182684 140265095153472 base_runner.py:59] cluster.evaler.name : '/job:local'
I1001 09:57:41.182770 140265095153472 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I1001 09:57:41.182845 140265095153472 base_runner.py:59] cluster.evaler.replicas : 1
I1001 09:57:41.182918 140265095153472 base_runner.py:59] cluster.evaler.targets : ''
I1001 09:57:41.183006 140265095153472 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I1001 09:57:41.183079 140265095153472 base_runner.py:59] cluster.input.cpus_per_replica : 1
I1001 09:57:41.183190 140265095153472 base_runner.py:59] cluster.input.devices_per_split : 1
I1001 09:57:41.183262 140265095153472 base_runner.py:59] cluster.input.gpus_per_replica : 0
I1001 09:57:41.183332 140265095153472 base_runner.py:59] cluster.input.name : '/job:local'
I1001 09:57:41.183390 140265095153472 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I1001 09:57:41.183466 140265095153472 base_runner.py:59] cluster.input.replicas : 0
I1001 09:57:41.183528 140265095153472 base_runner.py:59] cluster.input.targets : ''
I1001 09:57:41.183597 140265095153472 base_runner.py:59] cluster.input.tpus_per_replica : 0
I1001 09:57:41.183669 140265095153472 base_runner.py:59] cluster.job : 'controller'
I1001 09:57:41.183750 140265095153472 base_runner.py:59] cluster.logdir : ''
I1001 09:57:41.183809 140265095153472 base_runner.py:59] cluster.mode : 'sync'
I1001 09:57:41.183882 140265095153472 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I1001 09:57:41.183950 140265095153472 base_runner.py:59] cluster.ps.devices_per_split : 1
I1001 09:57:41.184015 140265095153472 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I1001 09:57:41.184088 140265095153472 base_runner.py:59] cluster.ps.name : '/job:local'
I1001 09:57:41.184152 140265095153472 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I1001 09:57:41.184228 140265095153472 base_runner.py:59] cluster.ps.replicas : 1
I1001 09:57:41.184282 140265095153472 base_runner.py:59] cluster.ps.targets : ''
I1001 09:57:41.184361 140265095153472 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I1001 09:57:41.184426 140265095153472 base_runner.py:59] cluster.task : 0
I1001 09:57:41.184494 140265095153472 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I1001 09:57:41.184554 140265095153472 base_runner.py:59] cluster.worker.devices_per_split : 1
I1001 09:57:41.184636 140265095153472 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I1001 09:57:41.184701 140265095153472 base_runner.py:59] cluster.worker.name : '/job:local'
I1001 09:57:41.184769 140265095153472 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I1001 09:57:41.184841 140265095153472 base_runner.py:59] cluster.worker.replicas : 1
I1001 09:57:41.184902 140265095153472 base_runner.py:59] cluster.worker.targets : ''
I1001 09:57:41.184982 140265095153472 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I1001 09:57:41.185036 140265095153472 base_runner.py:59] dtype : float32
I1001 09:57:41.185116 140265095153472 base_runner.py:59] fprop_dtype : NoneType
I1001 09:57:41.185200 140265095153472 base_runner.py:59] inference_driver_name : NoneType
I1001 09:57:41.185279 140265095153472 base_runner.py:59] input.allow_implicit_capture : NoneType
I1001 09:57:41.185347 140265095153472 base_runner.py:59] input.bucket_adjust_every_n : 0
I1001 09:57:41.185408 140265095153472 base_runner.py:59] input.bucket_batch_limit : [32]
I1001 09:57:41.185471 140265095153472 base_runner.py:59] input.bucket_upper_bound : [1024]
I1001 09:57:41.185541 140265095153472 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I1001 09:57:41.185614 140265095153472 base_runner.py:59] input.dtype : float32
I1001 09:57:41.185674 140265095153472 base_runner.py:59] input.file_buffer_size : 10000000
I1001 09:57:41.185754 140265095153472 base_runner.py:59] input.file_datasource : NoneType
I1001 09:57:41.185808 140265095153472 base_runner.py:59] input.file_parallelism : 10
I1001 09:57:41.185889 140265095153472 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I1001 09:57:41.185952 140265095153472 base_runner.py:59] input.file_random_seed : 301
I1001 09:57:41.186017 140265095153472 base_runner.py:59] input.fixed_input_shape : True
I1001 09:57:41.186105 140265095153472 base_runner.py:59] input.flush_every_n : 0
I1001 09:57:41.186187 140265095153472 base_runner.py:59] input.fprop_dtype : NoneType
I1001 09:57:41.186261 140265095153472 base_runner.py:59] input.inference_driver_name : NoneType
I1001 09:57:41.186320 140265095153472 base_runner.py:59] input.is_eval : NoneType
I1001 09:57:41.186392 140265095153472 base_runner.py:59] input.is_inference : NoneType
I1001 09:57:41.186457 140265095153472 base_runner.py:59] input.name : '1bwds_train_set'
I1001 09:57:41.186532 140265095153472 base_runner.py:59] input.num_batcher_threads : 16
I1001 09:57:41.186585 140265095153472 base_runner.py:59] input.num_samples : 0
I1001 09:57:41.186665 140265095153472 base_runner.py:59] input.pad_to_max_seq_length : False
I1001 09:57:41.186725 140265095153472 base_runner.py:59] input.params_init.method : 'xavier'
I1001 09:57:41.186795 140265095153472 base_runner.py:59] input.params_init.scale : 1.000001
I1001 09:57:41.186865 140265095153472 base_runner.py:59] input.params_init.seed : NoneType
I1001 09:57:41.186934 140265095153472 base_runner.py:59] input.random_seed : NoneType
I1001 09:57:41.187007 140265095153472 base_runner.py:59] input.remote.max_inflights_per_target : 32
I1001 09:57:41.187063 140265095153472 base_runner.py:59] input.remote.shardable_batch : False
I1001 09:57:41.187161 140265095153472 base_runner.py:59] input.require_sequential_order : False
I1001 09:57:41.187221 140265095153472 base_runner.py:59] input.skip_lp_regularization : NoneType
I1001 09:57:41.187303 140265095153472 base_runner.py:59] input.source_max_length : NoneType
I1001 09:57:41.187358 140265095153472 base_runner.py:59] input.target_max_length : 1024
I1001 09:57:41.187438 140265095153472 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I1001 09:57:41.187499 140265095153472 base_runner.py:59] input.tokenizer.append_eos : True
I1001 09:57:41.187571 140265095153472 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I1001 09:57:41.187643 140265095153472 base_runner.py:59] input.tokenizer.dtype : float32
I1001 09:57:41.187708 140265095153472 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I1001 09:57:41.187796 140265095153472 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I1001 09:57:41.187879 140265095153472 base_runner.py:59] input.tokenizer.is_eval : NoneType
I1001 09:57:41.187955 140265095153472 base_runner.py:59] input.tokenizer.is_inference : NoneType
I1001 09:57:41.188009 140265095153472 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I1001 09:57:41.188079 140265095153472 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I1001 09:57:41.188153 140265095153472 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I1001 09:57:41.188219 140265095153472 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I1001 09:57:41.188281 140265095153472 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I1001 09:57:41.188357 140265095153472 base_runner.py:59] input.tokenizer.random_seed : NoneType
I1001 09:57:41.188412 140265095153472 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I1001 09:57:41.188495 140265095153472 base_runner.py:59] input.tokenizer.target_eos_id : 2
I1001 09:57:41.188548 140265095153472 base_runner.py:59] input.tokenizer.target_sos_id : 1
I1001 09:57:41.188625 140265095153472 base_runner.py:59] input.tokenizer.target_unk_id : 0
I1001 09:57:41.188688 140265095153472 base_runner.py:59] input.tokenizer.vn.global_vn : False
I1001 09:57:41.188756 140265095153472 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I1001 09:57:41.188827 140265095153472 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I1001 09:57:41.188885 140265095153472 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I1001 09:57:41.188965 140265095153472 base_runner.py:59] input.tokenizer.vocab_size : 32000
I1001 09:57:41.189018 140265095153472 base_runner.py:59] input.tokenizer_dict : {}
I1001 09:57:41.189096 140265095153472 base_runner.py:59] input.tpu_infeed_parallelism : 1
I1001 09:57:41.189157 140265095153472 base_runner.py:59] input.use_chaining : False
I1001 09:57:41.189226 140265095153472 base_runner.py:59] input.use_per_host_infeed : False
I1001 09:57:41.189297 140265095153472 base_runner.py:59] input.use_within_batch_mixing : False
I1001 09:57:41.189360 140265095153472 base_runner.py:59] input.vn.global_vn : False
I1001 09:57:41.189437 140265095153472 base_runner.py:59] input.vn.per_step_vn : False
I1001 09:57:41.189494 140265095153472 base_runner.py:59] input.vn.scale : NoneType
I1001 09:57:41.189574 140265095153472 base_runner.py:59] input.vn.seed : NoneType
I1001 09:57:41.189629 140265095153472 base_runner.py:59] is_eval : NoneType
I1001 09:57:41.189706 140265095153472 base_runner.py:59] is_inference : NoneType
I1001 09:57:41.189768 140265095153472 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:188'
I1001 09:57:41.189838 140265095153472 base_runner.py:59] name : ''
I1001 09:57:41.189909 140265095153472 base_runner.py:59] params_init.method : 'xavier'
I1001 09:57:41.189970 140265095153472 base_runner.py:59] params_init.scale : 1.000001
I1001 09:57:41.190046 140265095153472 base_runner.py:59] params_init.seed : NoneType
I1001 09:57:41.190100 140265095153472 base_runner.py:59] random_seed : NoneType
I1001 09:57:41.190182 140265095153472 base_runner.py:59] skip_lp_regularization : NoneType
I1001 09:57:41.190239 140265095153472 base_runner.py:59] task.allow_implicit_capture : NoneType
I1001 09:57:41.190315 140265095153472 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I1001 09:57:41.190381 140265095153472 base_runner.py:59] task.decoder : NoneType
I1001 09:57:41.190442 140265095153472 base_runner.py:59] task.dtype : float32
I1001 09:57:41.190531 140265095153472 base_runner.py:59] task.encoder : NoneType
I1001 09:57:41.190614 140265095153472 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I1001 09:57:41.190702 140265095153472 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I1001 09:57:41.190793 140265095153472 base_runner.py:59] task.eval.samples_per_summary : 0
I1001 09:57:41.190865 140265095153472 base_runner.py:59] task.eval.start_decoder_after : 0
I1001 09:57:41.190918 140265095153472 base_runner.py:59] task.eval.start_eval_after : 0
I1001 09:57:41.190989 140265095153472 base_runner.py:59] task.fprop_dtype : NoneType
I1001 09:57:41.191054 140265095153472 base_runner.py:59] task.inference_driver_name : NoneType
I1001 09:57:41.191142 140265095153472 base_runner.py:59] task.input : NoneType
I1001 09:57:41.191202 140265095153472 base_runner.py:59] task.is_eval : NoneType
I1001 09:57:41.191282 140265095153472 base_runner.py:59] task.is_inference : NoneType
I1001 09:57:41.191335 140265095153472 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I1001 09:57:41.191415 140265095153472 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I1001 09:57:41.191476 140265095153472 base_runner.py:59] task.lm.dtype : float32
I1001 09:57:41.191542 140265095153472 base_runner.py:59] task.lm.fprop_dtype : NoneType
I1001 09:57:41.191629 140265095153472 base_runner.py:59] task.lm.inference_driver_name : NoneType
I1001 09:57:41.191712 140265095153472 base_runner.py:59] task.lm.is_eval : NoneType
I1001 09:57:41.191798 140265095153472 base_runner.py:59] task.lm.is_inference : NoneType
I1001 09:57:41.191881 140265095153472 base_runner.py:59] task.lm.name : 'transformerlm'
I1001 09:57:41.191941 140265095153472 base_runner.py:59] task.lm.params_init.method : 'xavier'
I1001 09:57:41.192010 140265095153472 base_runner.py:59] task.lm.params_init.scale : 1.000001
I1001 09:57:41.192078 140265095153472 base_runner.py:59] task.lm.params_init.seed : NoneType
I1001 09:57:41.192147 140265095153472 base_runner.py:59] task.lm.random_seed : NoneType
I1001 09:57:41.192224 140265095153472 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I1001 09:57:41.192300 140265095153472 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I1001 09:57:41.192365 140265095153472 base_runner.py:59] task.lm.stack.batch_dim : 1
I1001 09:57:41.192432 140265095153472 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I1001 09:57:41.192505 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.192566 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1001 09:57:41.192643 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I1001 09:57:41.192698 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I1001 09:57:41.192778 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I1001 09:57:41.192835 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I1001 09:57:41.192902 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I1001 09:57:41.192995 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I1001 09:57:41.193078 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I1001 09:57:41.193162 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I1001 09:57:41.193241 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I1001 09:57:41.193308 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.193364 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:57:41.193431 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I1001 09:57:41.193492 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I1001 09:57:41.193568 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:57:41.193621 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:57:41.193711 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I1001 09:57:41.193796 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I1001 09:57:41.193884 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I1001 09:57:41.193968 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I1001 09:57:41.194055 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:57:41.194138 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:57:41.194214 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:57:41.194267 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I1001 09:57:41.194337 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.194398 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I1001 09:57:41.194472 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:57:41.194530 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I1001 09:57:41.194601 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I1001 09:57:41.194669 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I1001 09:57:41.194732 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I1001 09:57:41.194808 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I1001 09:57:41.194869 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I1001 09:57:41.194944 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I1001 09:57:41.195000 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I1001 09:57:41.195076 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I1001 09:57:41.195162 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I1001 09:57:41.195244 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I1001 09:57:41.195305 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.195374 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I1001 09:57:41.195445 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1001 09:57:41.195509 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.195586 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1001 09:57:41.195646 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1001 09:57:41.195726 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.195782 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1001 09:57:41.195864 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1001 09:57:41.195924 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1001 09:57:41.195991 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1001 09:57:41.196082 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1001 09:57:41.196165 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1001 09:57:41.196259 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1001 09:57:41.196340 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I1001 09:57:41.196410 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1001 09:57:41.196469 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1001 09:57:41.196533 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1001 09:57:41.196604 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1001 09:57:41.196676 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1001 09:57:41.196736 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1001 09:57:41.196816 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1001 09:57:41.196871 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1001 09:57:41.196953 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1001 09:57:41.197012 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1001 09:57:41.197085 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1001 09:57:41.197153 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1001 09:57:41.197218 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1001 09:57:41.197293 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1001 09:57:41.197353 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1001 09:57:41.197433 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1001 09:57:41.197487 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1001 09:57:41.197566 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1001 09:57:41.197628 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1001 09:57:41.197697 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1001 09:57:41.197768 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1001 09:57:41.197829 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1001 09:57:41.197907 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1001 09:57:41.197964 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1001 09:57:41.198045 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1001 09:57:41.198102 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1001 09:57:41.198187 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1001 09:57:41.198251 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1001 09:57:41.198316 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1001 09:57:41.198405 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1001 09:57:41.198488 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1001 09:57:41.198576 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1001 09:57:41.198658 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1001 09:57:41.198729 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1001 09:57:41.198782 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1001 09:57:41.198854 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1001 09:57:41.198911 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1001 09:57:41.198992 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1001 09:57:41.199047 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1001 09:57:41.199143 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1001 09:57:41.199204 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1001 09:57:41.199279 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1001 09:57:41.199342 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1001 09:57:41.199411 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1001 09:57:41.199482 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1001 09:57:41.199542 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.199622 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1001 09:57:41.199679 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1001 09:57:41.199760 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1001 09:57:41.199815 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1001 09:57:41.199893 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1001 09:57:41.199955 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1001 09:57:41.200024 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1001 09:57:41.200097 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I1001 09:57:41.200160 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I1001 09:57:41.200236 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1001 09:57:41.200293 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1001 09:57:41.200381 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I1001 09:57:41.200436 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I1001 09:57:41.200518 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I1001 09:57:41.200576 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.200650 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:57:41.200716 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1001 09:57:41.200784 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1001 09:57:41.200857 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:57:41.200918 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:57:41.200997 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1001 09:57:41.201053 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1001 09:57:41.201134 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1001 09:57:41.201190 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1001 09:57:41.201268 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:57:41.201330 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:57:41.201403 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:57:41.201472 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1001 09:57:41.201535 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.201611 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1001 09:57:41.201669 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:57:41.201749 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1001 09:57:41.201803 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1001 09:57:41.201883 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I1001 09:57:41.201944 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I1001 09:57:41.202016 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I1001 09:57:41.202084 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I1001 09:57:41.202150 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1001 09:57:41.202225 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1001 09:57:41.202286 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1001 09:57:41.202365 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I1001 09:57:41.202419 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1001 09:57:41.202499 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.202587 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:57:41.202683 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 09:57:41.202768 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1001 09:57:41.202847 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 09:57:41.202902 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 09:57:41.202975 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 09:57:41.203040 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 09:57:41.203129 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 09:57:41.203190 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1001 09:57:41.203273 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 09:57:41.203329 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:57:41.203405 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 09:57:41.203470 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 09:57:41.203536 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 09:57:41.203610 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 09:57:41.203670 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.203752 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 09:57:41.203824 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 09:57:41.203895 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 09:57:41.203955 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 09:57:41.204033 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.204092 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I1001 09:57:41.204175 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I1001 09:57:41.204236 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1001 09:57:41.204306 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1001 09:57:41.204377 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1001 09:57:41.204445 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I1001 09:57:41.204519 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1001 09:57:41.204592 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.204660 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1001 09:57:41.204725 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I1001 09:57:41.204807 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1001 09:57:41.204889 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.204951 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1001 09:57:41.205020 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1001 09:57:41.205091 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1001 09:57:41.205157 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1001 09:57:41.205231 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:57:41.205285 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1001 09:57:41.205366 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1001 09:57:41.205426 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1001 09:57:41.205503 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1001 09:57:41.205576 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1001 09:57:41.205634 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1001 09:57:41.205704 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1001 09:57:41.205771 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1001 09:57:41.205841 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1001 09:57:41.205904 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1001 09:57:41.205980 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1001 09:57:41.206038 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1001 09:57:41.206120 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1001 09:57:41.206174 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1001 09:57:41.206249 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1001 09:57:41.206315 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1001 09:57:41.206383 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1001 09:57:41.206455 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1001 09:57:41.206513 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1001 09:57:41.206593 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1001 09:57:41.206666 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1001 09:57:41.206736 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1001 09:57:41.206809 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1001 09:57:41.206882 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1001 09:57:41.206937 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1001 09:57:41.207018 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1001 09:57:41.207074 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1001 09:57:41.207175 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1001 09:57:41.207233 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1001 09:57:41.207308 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1001 09:57:41.207374 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1001 09:57:41.207441 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1001 09:57:41.207515 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1001 09:57:41.207573 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1001 09:57:41.207654 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1001 09:57:41.207708 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1001 09:57:41.207786 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1001 09:57:41.207859 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1001 09:57:41.207937 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1001 09:57:41.208001 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1001 09:57:41.208068 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1001 09:57:41.208141 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1001 09:57:41.208199 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1001 09:57:41.208280 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1001 09:57:41.208333 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1001 09:57:41.208411 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1001 09:57:41.208473 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1001 09:57:41.208538 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1001 09:57:41.208611 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1001 09:57:41.208669 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1001 09:57:41.208750 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1001 09:57:41.208818 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1001 09:57:41.208894 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1001 09:57:41.208951 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1001 09:57:41.209027 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1001 09:57:41.209094 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1001 09:57:41.209159 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1001 09:57:41.209234 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1001 09:57:41.209297 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1001 09:57:41.209373 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.209428 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1001 09:57:41.209507 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1001 09:57:41.209569 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1001 09:57:41.209639 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1001 09:57:41.209712 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1001 09:57:41.209773 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1001 09:57:41.209851 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I1001 09:57:41.209904 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1001 09:57:41.209984 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I1001 09:57:41.210045 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1001 09:57:41.210113 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1001 09:57:41.210185 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.210237 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:57:41.210323 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1001 09:57:41.210408 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1001 09:57:41.210495 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:57:41.210579 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:57:41.210658 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1001 09:57:41.210721 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1001 09:57:41.210790 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1001 09:57:41.210852 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1001 09:57:41.210910 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:57:41.211008 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:57:41.211112 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:57:41.211185 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1001 09:57:41.211253 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.211320 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1001 09:57:41.211388 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:57:41.211461 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1001 09:57:41.211520 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1001 09:57:41.211600 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I1001 09:57:41.211654 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I1001 09:57:41.211734 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1001 09:57:41.211793 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1001 09:57:41.211864 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1001 09:57:41.211933 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1001 09:57:41.211998 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1001 09:57:41.212071 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1001 09:57:41.212131 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1001 09:57:41.212205 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.212265 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1001 09:57:41.212336 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1001 09:57:41.212403 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1001 09:57:41.212472 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1001 09:57:41.212544 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1001 09:57:41.212602 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1001 09:57:41.212683 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1001 09:57:41.212736 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1001 09:57:41.212813 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1001 09:57:41.212883 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1001 09:57:41.212970 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1001 09:57:41.213056 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1001 09:57:41.213146 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1001 09:57:41.213230 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1001 09:57:41.213320 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1001 09:57:41.213382 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1001 09:57:41.213455 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1001 09:57:41.213516 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1001 09:57:41.213585 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.213658 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1001 09:57:41.213717 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1001 09:57:41.213797 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1001 09:57:41.213850 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1001 09:57:41.213929 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1001 09:57:41.213991 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1001 09:57:41.214059 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.214131 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:57:41.214191 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 09:57:41.214272 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1001 09:57:41.214326 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 09:57:41.214405 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 09:57:41.214468 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 09:57:41.214537 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 09:57:41.214609 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 09:57:41.214668 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1001 09:57:41.214749 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 09:57:41.214803 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:57:41.214884 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 09:57:41.214943 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 09:57:41.215013 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 09:57:41.215084 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 09:57:41.215181 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.215252 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 09:57:41.215322 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 09:57:41.215394 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 09:57:41.215462 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 09:57:41.215528 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.215601 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1001 09:57:41.215664 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1001 09:57:41.215740 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1001 09:57:41.215800 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1001 09:57:41.215879 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I1001 09:57:41.215935 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I1001 09:57:41.216018 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I1001 09:57:41.216075 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I1001 09:57:41.216156 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I1001 09:57:41.216217 140265095153472 base_runner.py:59] task.lm.stack.dtype : float32
I1001 09:57:41.216294 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I1001 09:57:41.216358 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.216437 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I1001 09:57:41.216505 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I1001 09:57:41.216575 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I1001 09:57:41.216651 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.216719 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:57:41.216789 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I1001 09:57:41.216861 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I1001 09:57:41.216931 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I1001 09:57:41.217009 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I1001 09:57:41.217078 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I1001 09:57:41.217146 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I1001 09:57:41.217218 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I1001 09:57:41.217289 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I1001 09:57:41.217360 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I1001 09:57:41.217431 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:57:41.217490 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I1001 09:57:41.217571 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I1001 09:57:41.217632 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I1001 09:57:41.217699 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I1001 09:57:41.217771 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.217838 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I1001 09:57:41.217912 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I1001 09:57:41.217970 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I1001 09:57:41.218055 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I1001 09:57:41.218108 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I1001 09:57:41.218189 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I1001 09:57:41.218248 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I1001 09:57:41.218317 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I1001 09:57:41.218387 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I1001 09:57:41.218448 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I1001 09:57:41.218526 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I1001 09:57:41.218579 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I1001 09:57:41.218657 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I1001 09:57:41.218719 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I1001 09:57:41.218786 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I1001 09:57:41.218859 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I1001 09:57:41.218916 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I1001 09:57:41.218998 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I1001 09:57:41.219052 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I1001 09:57:41.219152 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I1001 09:57:41.219211 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I1001 09:57:41.219289 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I1001 09:57:41.219352 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I1001 09:57:41.219421 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I1001 09:57:41.219493 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I1001 09:57:41.219563 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I1001 09:57:41.219634 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I1001 09:57:41.219692 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I1001 09:57:41.219772 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I1001 09:57:41.219826 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I1001 09:57:41.219903 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I1001 09:57:41.219974 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I1001 09:57:41.220053 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I1001 09:57:41.220106 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I1001 09:57:41.220185 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I1001 09:57:41.220251 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I1001 09:57:41.220326 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I1001 09:57:41.220393 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I1001 09:57:41.220458 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I1001 09:57:41.220534 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I1001 09:57:41.220590 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I1001 09:57:41.220672 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.220727 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I1001 09:57:41.220802 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I1001 09:57:41.220868 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I1001 09:57:41.220936 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I1001 09:57:41.221008 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I1001 09:57:41.221067 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I1001 09:57:41.221146 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I1001 09:57:41.221200 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I1001 09:57:41.221281 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I1001 09:57:41.221339 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I1001 09:57:41.221410 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I1001 09:57:41.221479 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I1001 09:57:41.221543 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I1001 09:57:41.221618 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I1001 09:57:41.221677 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I1001 09:57:41.221757 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I1001 09:57:41.221811 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I1001 09:57:41.221890 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I1001 09:57:41.221951 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I1001 09:57:41.222020 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I1001 09:57:41.222090 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I1001 09:57:41.222151 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I1001 09:57:41.222229 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I1001 09:57:41.222283 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I1001 09:57:41.222363 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I1001 09:57:41.222422 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I1001 09:57:41.222492 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I1001 09:57:41.222561 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I1001 09:57:41.222623 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.222700 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1001 09:57:41.222764 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I1001 09:57:41.222845 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I1001 09:57:41.222899 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I1001 09:57:41.222975 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I1001 09:57:41.223040 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I1001 09:57:41.223129 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I1001 09:57:41.223194 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I1001 09:57:41.223259 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I1001 09:57:41.223334 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I1001 09:57:41.223392 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.223473 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:57:41.223527 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I1001 09:57:41.223601 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I1001 09:57:41.223666 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:57:41.223732 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:57:41.223805 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I1001 09:57:41.223864 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I1001 09:57:41.223944 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I1001 09:57:41.223997 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I1001 09:57:41.224075 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:57:41.224137 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:57:41.224206 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:57:41.224277 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I1001 09:57:41.224333 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.224413 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I1001 09:57:41.224467 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:57:41.224539 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I1001 09:57:41.224604 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I1001 09:57:41.224667 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I1001 09:57:41.224742 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I1001 09:57:41.224796 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I1001 09:57:41.224876 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I1001 09:57:41.224935 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I1001 09:57:41.225006 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I1001 09:57:41.225073 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I1001 09:57:41.225139 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I1001 09:57:41.225212 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I1001 09:57:41.225279 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.225358 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I1001 09:57:41.225412 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1001 09:57:41.225485 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.225572 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1001 09:57:41.225658 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1001 09:57:41.225742 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.225824 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1001 09:57:41.225892 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1001 09:57:41.225953 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1001 09:57:41.226020 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1001 09:57:41.226082 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1001 09:57:41.226159 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1001 09:57:41.226215 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1001 09:57:41.226295 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I1001 09:57:41.226350 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1001 09:57:41.226422 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1001 09:57:41.226488 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1001 09:57:41.226551 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1001 09:57:41.226626 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1001 09:57:41.226688 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1001 09:57:41.226763 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1001 09:57:41.226819 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1001 09:57:41.226893 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1001 09:57:41.226959 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1001 09:57:41.227024 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1001 09:57:41.227116 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1001 09:57:41.227190 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1001 09:57:41.227264 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1001 09:57:41.227329 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1001 09:57:41.227409 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1001 09:57:41.227463 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1001 09:57:41.227543 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1001 09:57:41.227602 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1001 09:57:41.227672 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1001 09:57:41.227742 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1001 09:57:41.227804 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1001 09:57:41.227881 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1001 09:57:41.227934 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1001 09:57:41.228015 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1001 09:57:41.228074 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1001 09:57:41.228143 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1001 09:57:41.228214 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1001 09:57:41.228275 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1001 09:57:41.228352 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1001 09:57:41.228406 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1001 09:57:41.228489 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1001 09:57:41.228545 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1001 09:57:41.228618 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1001 09:57:41.228684 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1001 09:57:41.228750 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1001 09:57:41.228823 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1001 09:57:41.228891 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1001 09:57:41.228964 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1001 09:57:41.229032 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1001 09:57:41.229105 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1001 09:57:41.229163 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1001 09:57:41.229244 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1001 09:57:41.229297 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1001 09:57:41.229380 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1001 09:57:41.229443 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.229512 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1001 09:57:41.229583 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1001 09:57:41.229643 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1001 09:57:41.229722 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1001 09:57:41.229775 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1001 09:57:41.229854 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1001 09:57:41.229915 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1001 09:57:41.229984 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I1001 09:57:41.230055 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I1001 09:57:41.230115 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1001 09:57:41.230192 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1001 09:57:41.230245 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I1001 09:57:41.230324 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I1001 09:57:41.230384 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I1001 09:57:41.230452 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.230524 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:57:41.230586 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1001 09:57:41.230662 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1001 09:57:41.230715 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:57:41.230795 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:57:41.230855 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1001 09:57:41.230925 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1001 09:57:41.230994 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1001 09:57:41.231049 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1001 09:57:41.231148 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:57:41.231208 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:57:41.231290 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:57:41.231347 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1001 09:57:41.231421 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.231484 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1001 09:57:41.231561 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:57:41.231630 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1001 09:57:41.231699 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1001 09:57:41.231772 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I1001 09:57:41.231832 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I1001 09:57:41.231911 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I1001 09:57:41.231978 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I1001 09:57:41.232047 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1001 09:57:41.232106 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1001 09:57:41.232176 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1001 09:57:41.232247 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I1001 09:57:41.232309 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1001 09:57:41.232386 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.232442 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:57:41.232523 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 09:57:41.232578 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1001 09:57:41.232657 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 09:57:41.232719 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 09:57:41.232788 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 09:57:41.232858 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 09:57:41.232919 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 09:57:41.232997 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1001 09:57:41.233051 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 09:57:41.233133 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:57:41.233194 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 09:57:41.233267 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 09:57:41.233334 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 09:57:41.233397 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 09:57:41.233474 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.233527 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 09:57:41.233606 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 09:57:41.233700 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 09:57:41.233789 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 09:57:41.233874 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.233959 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I1001 09:57:41.234030 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I1001 09:57:41.234100 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1001 09:57:41.234154 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1001 09:57:41.234229 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1001 09:57:41.234295 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I1001 09:57:41.234364 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1001 09:57:41.234438 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.234502 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1001 09:57:41.234581 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I1001 09:57:41.234635 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1001 09:57:41.234710 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.234776 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1001 09:57:41.234842 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1001 09:57:41.234917 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1001 09:57:41.234977 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1001 09:57:41.235057 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:57:41.235140 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1001 09:57:41.235215 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1001 09:57:41.235272 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1001 09:57:41.235352 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1001 09:57:41.235408 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1001 09:57:41.235486 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1001 09:57:41.235550 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1001 09:57:41.235620 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1001 09:57:41.235690 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1001 09:57:41.235753 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1001 09:57:41.235836 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1001 09:57:41.235896 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1001 09:57:41.235976 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1001 09:57:41.236030 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1001 09:57:41.236110 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1001 09:57:41.236172 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1001 09:57:41.236243 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1001 09:57:41.236312 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1001 09:57:41.236376 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1001 09:57:41.236451 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1001 09:57:41.236509 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1001 09:57:41.236589 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1001 09:57:41.236644 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1001 09:57:41.236732 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1001 09:57:41.236793 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1001 09:57:41.236857 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1001 09:57:41.236945 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1001 09:57:41.237028 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1001 09:57:41.237119 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1001 09:57:41.237201 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1001 09:57:41.237277 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1001 09:57:41.237330 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1001 09:57:41.237400 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1001 09:57:41.237461 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1001 09:57:41.237536 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1001 09:57:41.237592 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1001 09:57:41.237667 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1001 09:57:41.237731 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1001 09:57:41.237798 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1001 09:57:41.237875 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1001 09:57:41.237936 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1001 09:57:41.238028 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1001 09:57:41.238082 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1001 09:57:41.238156 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1001 09:57:41.238222 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1001 09:57:41.238290 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1001 09:57:41.238363 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1001 09:57:41.238424 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1001 09:57:41.238501 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1001 09:57:41.238555 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1001 09:57:41.238636 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1001 09:57:41.238693 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1001 09:57:41.238767 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1001 09:57:41.238832 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1001 09:57:41.238898 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1001 09:57:41.238971 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1001 09:57:41.239029 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1001 09:57:41.239120 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1001 09:57:41.239190 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1001 09:57:41.239271 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.239326 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1001 09:57:41.239407 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1001 09:57:41.239464 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1001 09:57:41.239525 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1001 09:57:41.239600 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1001 09:57:41.239659 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1001 09:57:41.239742 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I1001 09:57:41.239801 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1001 09:57:41.239876 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I1001 09:57:41.239943 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1001 09:57:41.240020 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1001 09:57:41.240084 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.240154 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:57:41.240226 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1001 09:57:41.240308 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1001 09:57:41.240363 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:57:41.240438 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:57:41.240503 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1001 09:57:41.240565 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1001 09:57:41.240641 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1001 09:57:41.240699 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1001 09:57:41.240778 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:57:41.240832 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:57:41.240904 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:57:41.240970 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1001 09:57:41.241036 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.241114 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1001 09:57:41.241176 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:57:41.241252 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1001 09:57:41.241310 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1001 09:57:41.241391 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I1001 09:57:41.241445 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I1001 09:57:41.241525 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1001 09:57:41.241585 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1001 09:57:41.241658 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1001 09:57:41.241725 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1001 09:57:41.241791 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1001 09:57:41.241864 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1001 09:57:41.241924 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1001 09:57:41.242007 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.242060 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1001 09:57:41.242147 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1001 09:57:41.242205 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1001 09:57:41.242280 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1001 09:57:41.242345 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1001 09:57:41.242414 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1001 09:57:41.242485 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1001 09:57:41.242546 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1001 09:57:41.242623 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1001 09:57:41.242680 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1001 09:57:41.242760 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1001 09:57:41.242814 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1001 09:57:41.242892 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1001 09:57:41.242963 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1001 09:57:41.243041 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1001 09:57:41.243120 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1001 09:57:41.243203 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1001 09:57:41.243258 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1001 09:57:41.243340 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.243396 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1001 09:57:41.243470 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1001 09:57:41.243535 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1001 09:57:41.243601 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1001 09:57:41.243674 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1001 09:57:41.243733 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1001 09:57:41.243812 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.243867 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:57:41.243948 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 09:57:41.244007 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1001 09:57:41.244078 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 09:57:41.244145 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 09:57:41.244230 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 09:57:41.244298 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 09:57:41.244363 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 09:57:41.244437 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1001 09:57:41.244495 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 09:57:41.244574 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:57:41.244633 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 09:57:41.244706 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 09:57:41.244770 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 09:57:41.244835 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 09:57:41.244891 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.244961 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 09:57:41.245032 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 09:57:41.245101 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 09:57:41.245172 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 09:57:41.245240 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.245311 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1001 09:57:41.245366 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1001 09:57:41.245446 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1001 09:57:41.245503 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1001 09:57:41.245576 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I1001 09:57:41.245642 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I1001 09:57:41.245709 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I1001 09:57:41.245782 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I1001 09:57:41.245843 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I1001 09:57:41.245921 140265095153472 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I1001 09:57:41.245976 140265095153472 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I1001 09:57:41.246057 140265095153472 base_runner.py:59] task.lm.stack.is_eval : NoneType
I1001 09:57:41.246114 140265095153472 base_runner.py:59] task.lm.stack.is_inference : NoneType
I1001 09:57:41.246188 140265095153472 base_runner.py:59] task.lm.stack.is_transparent : False
I1001 09:57:41.246253 140265095153472 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I1001 09:57:41.246322 140265095153472 base_runner.py:59] task.lm.stack.model_dim : 2048
I1001 09:57:41.246399 140265095153472 base_runner.py:59] task.lm.stack.name : ''
I1001 09:57:41.246458 140265095153472 base_runner.py:59] task.lm.stack.normalize_encoder : False
I1001 09:57:41.246542 140265095153472 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I1001 09:57:41.246597 140265095153472 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I1001 09:57:41.246679 140265095153472 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I1001 09:57:41.246736 140265095153472 base_runner.py:59] task.lm.stack.packed_input : False
I1001 09:57:41.246809 140265095153472 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I1001 09:57:41.246876 140265095153472 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I1001 09:57:41.246941 140265095153472 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I1001 09:57:41.247014 140265095153472 base_runner.py:59] task.lm.stack.random_seed : NoneType
I1001 09:57:41.247074 140265095153472 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I1001 09:57:41.247166 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.247226 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I1001 09:57:41.247307 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I1001 09:57:41.247360 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I1001 09:57:41.247442 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I1001 09:57:41.247501 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I1001 09:57:41.247573 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I1001 09:57:41.247640 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I1001 09:57:41.247709 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I1001 09:57:41.247781 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I1001 09:57:41.247838 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I1001 09:57:41.247920 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I1001 09:57:41.247974 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I1001 09:57:41.248052 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I1001 09:57:41.248115 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I1001 09:57:41.248185 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I1001 09:57:41.248256 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I1001 09:57:41.248317 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I1001 09:57:41.248394 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I1001 09:57:41.248450 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I1001 09:57:41.248531 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I1001 09:57:41.248587 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.248659 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I1001 09:57:41.248726 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I1001 09:57:41.248792 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I1001 09:57:41.248865 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I1001 09:57:41.248924 140265095153472 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I1001 09:57:41.249005 140265095153472 base_runner.py:59] task.lm.stack.state_dtype : float32
I1001 09:57:41.249059 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I1001 09:57:41.249141 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.249204 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I1001 09:57:41.249284 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1001 09:57:41.249346 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I1001 09:57:41.249418 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I1001 09:57:41.249487 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I1001 09:57:41.249552 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I1001 09:57:41.249626 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I1001 09:57:41.249686 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I1001 09:57:41.249765 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I1001 09:57:41.249826 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I1001 09:57:41.249900 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I1001 09:57:41.249960 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I1001 09:57:41.250031 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:57:41.250099 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I1001 09:57:41.250163 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I1001 09:57:41.250237 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I1001 09:57:41.250313 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I1001 09:57:41.250379 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.250447 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I1001 09:57:41.250518 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I1001 09:57:41.250580 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I1001 09:57:41.250657 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I1001 09:57:41.250714 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I1001 09:57:41.250795 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I1001 09:57:41.250850 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I1001 09:57:41.250926 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I1001 09:57:41.250989 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I1001 09:57:41.251053 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I1001 09:57:41.251140 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I1001 09:57:41.251207 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I1001 09:57:41.251280 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I1001 09:57:41.251341 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I1001 09:57:41.251423 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I1001 09:57:41.251481 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I1001 09:57:41.251562 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I1001 09:57:41.251616 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I1001 09:57:41.251696 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I1001 09:57:41.251755 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I1001 09:57:41.251828 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I1001 09:57:41.251895 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I1001 09:57:41.251958 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I1001 09:57:41.252033 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I1001 09:57:41.252092 140265095153472 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I1001 09:57:41.252171 140265095153472 base_runner.py:59] task.lm.stack.vn.global_vn : False
I1001 09:57:41.252231 140265095153472 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I1001 09:57:41.252305 140265095153472 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I1001 09:57:41.252363 140265095153472 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I1001 09:57:41.252436 140265095153472 base_runner.py:59] task.lm.vn.global_vn : False
I1001 09:57:41.252501 140265095153472 base_runner.py:59] task.lm.vn.per_step_vn : False
I1001 09:57:41.252570 140265095153472 base_runner.py:59] task.lm.vn.scale : NoneType
I1001 09:57:41.252641 140265095153472 base_runner.py:59] task.lm.vn.seed : NoneType
I1001 09:57:41.252702 140265095153472 base_runner.py:59] task.lm.vocab_size : 32000
I1001 09:57:41.252779 140265095153472 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I1001 09:57:41.252832 140265095153472 base_runner.py:59] task.online_encoder : NoneType
I1001 09:57:41.252913 140265095153472 base_runner.py:59] task.params_init.method : 'xavier'
I1001 09:57:41.252981 140265095153472 base_runner.py:59] task.params_init.scale : 1.000001
I1001 09:57:41.253057 140265095153472 base_runner.py:59] task.params_init.seed : NoneType
I1001 09:57:41.253115 140265095153472 base_runner.py:59] task.random_seed : NoneType
I1001 09:57:41.253196 140265095153472 base_runner.py:59] task.skip_lp_regularization : NoneType
I1001 09:57:41.253255 140265095153472 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I1001 09:57:41.253335 140265095153472 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I1001 09:57:41.253396 140265095153472 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I1001 09:57:41.253467 140265095153472 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I1001 09:57:41.253534 140265095153472 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I1001 09:57:41.253599 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I1001 09:57:41.253673 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I1001 09:57:41.253732 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I1001 09:57:41.253812 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I1001 09:57:41.253865 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I1001 09:57:41.253946 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I1001 09:57:41.254004 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I1001 09:57:41.254083 140265095153472 base_runner.py:59] task.train.early_stop.min_steps : 0
I1001 09:57:41.254152 140265095153472 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I1001 09:57:41.254215 140265095153472 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I1001 09:57:41.254275 140265095153472 base_runner.py:59] task.train.early_stop.verbose : True
I1001 09:57:41.254344 140265095153472 base_runner.py:59] task.train.early_stop.window : 0
I1001 09:57:41.254416 140265095153472 base_runner.py:59] task.train.ema_decay : 0.0
I1001 09:57:41.254476 140265095153472 base_runner.py:59] task.train.enqueue_max_steps : -1
I1001 09:57:41.254554 140265095153472 base_runner.py:59] task.train.gate_gradients : False
I1001 09:57:41.254608 140265095153472 base_runner.py:59] task.train.grad_aggregation_method : 1
I1001 09:57:41.254689 140265095153472 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I1001 09:57:41.254747 140265095153472 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I1001 09:57:41.254822 140265095153472 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I1001 09:57:41.254887 140265095153472 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I1001 09:57:41.254954 140265095153472 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I1001 09:57:41.255027 140265095153472 base_runner.py:59] task.train.learner : NoneType
I1001 09:57:41.255089 140265095153472 base_runner.py:59] task.train.learning_rate : 0.5
I1001 09:57:41.255185 140265095153472 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I1001 09:57:41.255255 140265095153472 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1001 09:57:41.255327 140265095153472 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I1001 09:57:41.255383 140265095153472 base_runner.py:59] task.train.lr_schedule.dtype : float32
I1001 09:57:41.255463 140265095153472 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I1001 09:57:41.255519 140265095153472 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I1001 09:57:41.255594 140265095153472 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I1001 09:57:41.255659 140265095153472 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I1001 09:57:41.255728 140265095153472 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I1001 09:57:41.255798 140265095153472 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I1001 09:57:41.255860 140265095153472 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I1001 09:57:41.255937 140265095153472 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I1001 09:57:41.255992 140265095153472 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I1001 09:57:41.256072 140265095153472 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I1001 09:57:41.256128 140265095153472 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I1001 09:57:41.256202 140265095153472 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I1001 09:57:41.256266 140265095153472 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I1001 09:57:41.256332 140265095153472 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I1001 09:57:41.256406 140265095153472 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I1001 09:57:41.256465 140265095153472 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I1001 09:57:41.256544 140265095153472 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I1001 09:57:41.256598 140265095153472 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I1001 09:57:41.256679 140265095153472 base_runner.py:59] task.train.max_steps : 4000000
I1001 09:57:41.256736 140265095153472 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I1001 09:57:41.256808 140265095153472 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I1001 09:57:41.256880 140265095153472 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I1001 09:57:41.256951 140265095153472 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I1001 09:57:41.257021 140265095153472 base_runner.py:59] task.train.optimizer.dtype : float32
I1001 09:57:41.257083 140265095153472 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I1001 09:57:41.257159 140265095153472 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I1001 09:57:41.257216 140265095153472 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I1001 09:57:41.257296 140265095153472 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I1001 09:57:41.257350 140265095153472 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I1001 09:57:41.257429 140265095153472 base_runner.py:59] task.train.optimizer.name : 'Adam'
I1001 09:57:41.257491 140265095153472 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I1001 09:57:41.257560 140265095153472 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I1001 09:57:41.257630 140265095153472 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I1001 09:57:41.257691 140265095153472 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I1001 09:57:41.257767 140265095153472 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I1001 09:57:41.257823 140265095153472 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I1001 09:57:41.257904 140265095153472 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I1001 09:57:41.257961 140265095153472 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I1001 09:57:41.258037 140265095153472 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I1001 09:57:41.258102 140265095153472 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I1001 09:57:41.258171 140265095153472 base_runner.py:59] task.train.save_interval_seconds : 600
I1001 09:57:41.258242 140265095153472 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I1001 09:57:41.258303 140265095153472 base_runner.py:59] task.train.save_max_to_keep : 100
I1001 09:57:41.258380 140265095153472 base_runner.py:59] task.train.start_up_delay_steps : 200
I1001 09:57:41.258436 140265095153472 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I1001 09:57:41.258517 140265095153472 base_runner.py:59] task.train.summary_interval_steps : 100
I1001 09:57:41.258573 140265095153472 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I1001 09:57:41.258651 140265095153472 base_runner.py:59] task.train.vn_start_step : 20000
I1001 09:57:41.258713 140265095153472 base_runner.py:59] task.train.vn_std : 0.0
I1001 09:57:41.258781 140265095153472 base_runner.py:59] task.vn.global_vn : False
I1001 09:57:41.258854 140265095153472 base_runner.py:59] task.vn.per_step_vn : False
I1001 09:57:41.258925 140265095153472 base_runner.py:59] task.vn.scale : NoneType
I1001 09:57:41.259014 140265095153472 base_runner.py:59] task.vn.seed : NoneType
I1001 09:57:41.259119 140265095153472 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I1001 09:57:41.259207 140265095153472 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I1001 09:57:41.259295 140265095153472 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I1001 09:57:41.259381 140265095153472 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I1001 09:57:41.259470 140265095153472 base_runner.py:59] train.early_stop.metric_history.minimize : True
I1001 09:57:41.259552 140265095153472 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I1001 09:57:41.259639 140265095153472 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I1001 09:57:41.259720 140265095153472 base_runner.py:59] train.early_stop.min_steps : 0
I1001 09:57:41.259808 140265095153472 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I1001 09:57:41.259899 140265095153472 base_runner.py:59] train.early_stop.tolerance : 0.0
I1001 09:57:41.259981 140265095153472 base_runner.py:59] train.early_stop.verbose : True
I1001 09:57:41.260065 140265095153472 base_runner.py:59] train.early_stop.window : 0
I1001 09:57:41.260133 140265095153472 base_runner.py:59] train.ema_decay : 0.0
I1001 09:57:41.260217 140265095153472 base_runner.py:59] train.enqueue_max_steps : -1
I1001 09:57:41.260287 140265095153472 base_runner.py:59] train.init_from_checkpoint_rules : {}
I1001 09:57:41.260365 140265095153472 base_runner.py:59] train.max_steps : 4000000
I1001 09:57:41.260440 140265095153472 base_runner.py:59] train.save_interval_seconds : 600
I1001 09:57:41.260521 140265095153472 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I1001 09:57:41.260603 140265095153472 base_runner.py:59] train.save_max_to_keep : 100
I1001 09:57:41.260675 140265095153472 base_runner.py:59] train.start_up_delay_steps : 200
I1001 09:57:41.260750 140265095153472 base_runner.py:59] train.summary_interval_steps : 100
I1001 09:57:41.260819 140265095153472 base_runner.py:59] train.tpu_steps_per_loop : 100
I1001 09:57:41.260902 140265095153472 base_runner.py:59] vn.global_vn : False
I1001 09:57:41.260973 140265095153472 base_runner.py:59] vn.per_step_vn : False
I1001 09:57:41.261048 140265095153472 base_runner.py:59] vn.scale : NoneType
I1001 09:57:41.261123 140265095153472 base_runner.py:59] vn.seed : NoneType
I1001 09:57:41.261200 140265095153472 base_runner.py:59] 
I1001 09:57:41.261388 140265095153472 base_runner.py:60] ============================================================
I1001 09:57:41.268188 140265095153472 base_runner.py:106] Starting ...
I1001 09:57:41.272574 140265095153472 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I1001 09:57:41.301754 140265095153472 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I1001 09:57:41.316479 140265095153472 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I1001 09:57:41.332971 140265095153472 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I1001 09:57:41.337757 140265095153472 base_input_generator.py:624] bucket_batch_limit [32]
I1001 09:57:41.405109 140265095153472 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I1001 09:57:41.405300 140265095153472 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I1001 09:57:41.405365 140265095153472 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I1001 09:57:41.405421 140265095153472 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I1001 09:57:41.405472 140265095153472 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I1001 09:57:41.405527 140265095153472 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I1001 09:57:41.405654 140265095153472 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I1001 09:57:41.405711 140265095153472 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I1001 09:57:41.405764 140265095153472 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I1001 09:57:41.405813 140265095153472 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I1001 09:57:41.405863 140265095153472 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I1001 09:57:41.405912 140265095153472 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I1001 09:57:41.405961 140265095153472 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I1001 09:57:41.406015 140265095153472 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I1001 09:57:41.406063 140265095153472 learner.py:351] Ignoring legacy param learner=None for optimization program
I1001 09:57:41.406149 140265095153472 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I1001 09:57:41.406201 140265095153472 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I1001 09:57:41.406680 140265095153472 learner.py:356] Learner params: allow_implicit_capture : NoneType
I1001 09:57:41.406760 140265095153472 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I1001 09:57:41.406820 140265095153472 learner.py:356] Learner params: bprop_variable_filter : NoneType
I1001 09:57:41.406874 140265095153472 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I1001 09:57:41.406926 140265095153472 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I1001 09:57:41.406976 140265095153472 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I1001 09:57:41.407025 140265095153472 learner.py:356] Learner params: colocate_gradients_with_ops : True
I1001 09:57:41.407074 140265095153472 learner.py:356] Learner params: dtype : float32
I1001 09:57:41.407148 140265095153472 learner.py:356] Learner params: fprop_dtype : NoneType
I1001 09:57:41.407200 140265095153472 learner.py:356] Learner params: gate_gradients : False
I1001 09:57:41.407250 140265095153472 learner.py:356] Learner params: grad_aggregation_method : 1
I1001 09:57:41.407299 140265095153472 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I1001 09:57:41.407347 140265095153472 learner.py:356] Learner params: grad_norm_tracker : NoneType
I1001 09:57:41.407406 140265095153472 learner.py:356] Learner params: inference_driver_name : NoneType
I1001 09:57:41.407457 140265095153472 learner.py:356] Learner params: is_eval : NoneType
I1001 09:57:41.407506 140265095153472 learner.py:356] Learner params: is_inference : NoneType
I1001 09:57:41.407555 140265095153472 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I1001 09:57:41.407603 140265095153472 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I1001 09:57:41.407651 140265095153472 learner.py:356] Learner params: learning_rate : 0.5
I1001 09:57:41.407700 140265095153472 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I1001 09:57:41.407749 140265095153472 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1001 09:57:41.407798 140265095153472 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I1001 09:57:41.407846 140265095153472 learner.py:356] Learner params: lr_schedule.dtype : float32
I1001 09:57:41.407894 140265095153472 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I1001 09:57:41.407943 140265095153472 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I1001 09:57:41.407991 140265095153472 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I1001 09:57:41.408040 140265095153472 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I1001 09:57:41.408089 140265095153472 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I1001 09:57:41.408138 140265095153472 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I1001 09:57:41.408187 140265095153472 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I1001 09:57:41.408236 140265095153472 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I1001 09:57:41.408285 140265095153472 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I1001 09:57:41.408334 140265095153472 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I1001 09:57:41.408383 140265095153472 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I1001 09:57:41.408431 140265095153472 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I1001 09:57:41.408479 140265095153472 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I1001 09:57:41.408528 140265095153472 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I1001 09:57:41.408576 140265095153472 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I1001 09:57:41.408625 140265095153472 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I1001 09:57:41.408674 140265095153472 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I1001 09:57:41.408723 140265095153472 learner.py:356] Learner params: name : 'loss'
I1001 09:57:41.408771 140265095153472 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I1001 09:57:41.408820 140265095153472 learner.py:356] Learner params: optimizer.beta1 : 0.9
I1001 09:57:41.408868 140265095153472 learner.py:356] Learner params: optimizer.beta2 : 0.997
I1001 09:57:41.408916 140265095153472 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I1001 09:57:41.408965 140265095153472 learner.py:356] Learner params: optimizer.dtype : float32
I1001 09:57:41.409014 140265095153472 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I1001 09:57:41.409062 140265095153472 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I1001 09:57:41.409110 140265095153472 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I1001 09:57:41.409158 140265095153472 learner.py:356] Learner params: optimizer.is_eval : NoneType
I1001 09:57:41.409207 140265095153472 learner.py:356] Learner params: optimizer.is_inference : NoneType
I1001 09:57:41.409255 140265095153472 learner.py:356] Learner params: optimizer.name : 'Adam'
I1001 09:57:41.409303 140265095153472 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I1001 09:57:41.409357 140265095153472 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I1001 09:57:41.409406 140265095153472 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I1001 09:57:41.409455 140265095153472 learner.py:356] Learner params: optimizer.random_seed : NoneType
I1001 09:57:41.409503 140265095153472 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I1001 09:57:41.409552 140265095153472 learner.py:356] Learner params: optimizer.vn.global_vn : False
I1001 09:57:41.409600 140265095153472 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I1001 09:57:41.409649 140265095153472 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I1001 09:57:41.409697 140265095153472 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I1001 09:57:41.409746 140265095153472 learner.py:356] Learner params: params_init.method : 'xavier'
I1001 09:57:41.409794 140265095153472 learner.py:356] Learner params: params_init.scale : 1.000001
I1001 09:57:41.409842 140265095153472 learner.py:356] Learner params: params_init.seed : NoneType
I1001 09:57:41.409891 140265095153472 learner.py:356] Learner params: random_seed : NoneType
I1001 09:57:41.409940 140265095153472 learner.py:356] Learner params: skip_lp_regularization : NoneType
I1001 09:57:41.409988 140265095153472 learner.py:356] Learner params: vn.global_vn : False
I1001 09:57:41.410037 140265095153472 learner.py:356] Learner params: vn.per_step_vn : False
I1001 09:57:41.410085 140265095153472 learner.py:356] Learner params: vn.scale : NoneType
I1001 09:57:41.410134 140265095153472 learner.py:356] Learner params: vn.seed : NoneType
I1001 09:57:41.410182 140265095153472 learner.py:356] Learner params: 
I1001 09:57:41.827218 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I1001 09:57:41.829254 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.849531 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I1001 09:57:41.851664 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.854402 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I1001 09:57:41.856040 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.863070 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I1001 09:57:41.865006 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.867696 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I1001 09:57:41.869308 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.876306 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I1001 09:57:41.878300 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.880921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I1001 09:57:41.882511 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.889495 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I1001 09:57:41.891433 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.894083 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I1001 09:57:41.895732 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.900075 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I1001 09:57:41.901684 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.905622 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I1001 09:57:41.907235 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.909871 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I1001 09:57:41.911501 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:41.921319 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:41.928167 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I1001 09:57:41.930342 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.933138 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I1001 09:57:41.934788 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:41.936929 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:41.943364 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I1001 09:57:41.945349 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.948213 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I1001 09:57:41.949845 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.954785 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I1001 09:57:41.956517 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.959242 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I1001 09:57:41.960870 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.982320 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I1001 09:57:41.984353 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.986963 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I1001 09:57:41.988700 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:41.995652 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I1001 09:57:41.997578 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.000287 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I1001 09:57:42.001894 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.008845 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I1001 09:57:42.011507 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.014109 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I1001 09:57:42.015749 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.022739 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I1001 09:57:42.024681 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.027303 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I1001 09:57:42.029025 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.032773 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I1001 09:57:42.034381 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.038376 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I1001 09:57:42.040024 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.042689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I1001 09:57:42.044342 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.054277 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.061372 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I1001 09:57:42.063642 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.066450 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I1001 09:57:42.068105 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.070901 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.077224 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I1001 09:57:42.079152 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.081818 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I1001 09:57:42.083459 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.088140 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I1001 09:57:42.089746 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.092436 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I1001 09:57:42.094032 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.114084 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I1001 09:57:42.116020 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.118598 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I1001 09:57:42.120831 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.127717 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I1001 09:57:42.129628 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.132292 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I1001 09:57:42.133913 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.140898 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I1001 09:57:42.142861 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.145437 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I1001 09:57:42.147064 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.154015 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I1001 09:57:42.156037 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.158672 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I1001 09:57:42.160446 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.164272 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I1001 09:57:42.165906 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.169871 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I1001 09:57:42.171521 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.174623 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I1001 09:57:42.176302 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.187163 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.194061 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I1001 09:57:42.196213 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.198890 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I1001 09:57:42.200544 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.202576 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.208940 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I1001 09:57:42.211024 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.214028 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I1001 09:57:42.215804 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.221178 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I1001 09:57:42.222984 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.226003 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I1001 09:57:42.227692 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.431534 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I1001 09:57:42.433631 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.436351 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I1001 09:57:42.437984 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.444912 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I1001 09:57:42.446935 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.449690 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I1001 09:57:42.451356 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.458353 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I1001 09:57:42.460302 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.462885 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I1001 09:57:42.464674 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.471671 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I1001 09:57:42.473661 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.476534 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I1001 09:57:42.478178 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.482017 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I1001 09:57:42.483693 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.487814 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I1001 09:57:42.490519 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.493338 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I1001 09:57:42.494990 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.504997 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.511594 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I1001 09:57:42.513698 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.516479 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I1001 09:57:42.518124 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.520228 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.526597 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I1001 09:57:42.528782 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.531534 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I1001 09:57:42.533230 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.538834 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I1001 09:57:42.540911 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.543529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I1001 09:57:42.545171 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.567301 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I1001 09:57:42.569427 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.572201 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I1001 09:57:42.573987 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.581194 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I1001 09:57:42.583398 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.586394 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I1001 09:57:42.588145 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.595458 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I1001 09:57:42.597591 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.600383 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I1001 09:57:42.602024 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.609768 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I1001 09:57:42.611828 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.614720 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I1001 09:57:42.616417 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.620269 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I1001 09:57:42.621928 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.625962 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I1001 09:57:42.627611 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.630318 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I1001 09:57:42.632002 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.642103 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.648800 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I1001 09:57:42.651000 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.653850 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I1001 09:57:42.655554 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.657693 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.664926 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I1001 09:57:42.667043 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.670045 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I1001 09:57:42.671749 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.676873 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I1001 09:57:42.678544 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.681273 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I1001 09:57:42.682934 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.704780 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I1001 09:57:42.707268 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.710106 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I1001 09:57:42.711900 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.719887 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I1001 09:57:42.721878 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.724585 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I1001 09:57:42.726278 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.733404 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I1001 09:57:42.735427 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.737999 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I1001 09:57:42.739674 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.746643 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I1001 09:57:42.748600 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.751233 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I1001 09:57:42.752995 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.756867 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I1001 09:57:42.758517 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.762529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I1001 09:57:42.764235 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.766965 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I1001 09:57:42.768667 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.779372 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.786263 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I1001 09:57:42.788397 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.791187 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I1001 09:57:42.792872 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.795029 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.801467 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I1001 09:57:42.803534 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.806319 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I1001 09:57:42.807997 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.812906 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I1001 09:57:42.814534 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.817285 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I1001 09:57:42.818922 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.839801 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I1001 09:57:42.841747 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.844335 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I1001 09:57:42.846096 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.853038 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I1001 09:57:42.854981 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.857720 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I1001 09:57:42.859398 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.866308 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I1001 09:57:42.868347 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.870924 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I1001 09:57:42.872665 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.879644 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I1001 09:57:42.881590 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.884212 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I1001 09:57:42.885965 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.889782 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I1001 09:57:42.891460 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.895487 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I1001 09:57:42.897125 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.900345 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I1001 09:57:42.902049 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.912614 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.919759 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I1001 09:57:42.921988 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.924950 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I1001 09:57:42.926668 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:42.928903 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:42.935438 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I1001 09:57:42.937562 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.940496 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I1001 09:57:42.942214 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.947408 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I1001 09:57:42.949105 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.951859 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I1001 09:57:42.953512 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.975553 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I1001 09:57:42.977713 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.980380 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I1001 09:57:42.982142 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.989171 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I1001 09:57:42.991141 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:42.993880 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I1001 09:57:42.995573 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.002659 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I1001 09:57:43.004805 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.007514 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I1001 09:57:43.009188 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.016860 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I1001 09:57:43.018875 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.021656 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I1001 09:57:43.023466 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.027319 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I1001 09:57:43.028996 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.033111 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I1001 09:57:43.034763 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.037485 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I1001 09:57:43.039165 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.049309 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.056627 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I1001 09:57:43.058943 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.061874 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I1001 09:57:43.063630 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.065858 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.073139 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I1001 09:57:43.075234 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.078062 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I1001 09:57:43.079760 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.084878 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I1001 09:57:43.086588 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.089403 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I1001 09:57:43.091121 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.168453 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I1001 09:57:43.170644 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.173572 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I1001 09:57:43.175264 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.182324 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I1001 09:57:43.185185 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.187963 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I1001 09:57:43.189660 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.196791 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I1001 09:57:43.198875 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.201763 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I1001 09:57:43.203580 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.210673 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I1001 09:57:43.212706 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.215511 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I1001 09:57:43.217213 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.221168 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I1001 09:57:43.222858 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.227020 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I1001 09:57:43.228718 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.231480 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I1001 09:57:43.233162 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.243163 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.250377 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I1001 09:57:43.456005 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.459317 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I1001 09:57:43.461076 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.463262 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.469728 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I1001 09:57:43.471755 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.474512 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I1001 09:57:43.476234 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.481220 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I1001 09:57:43.482908 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.485708 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I1001 09:57:43.487418 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.509948 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I1001 09:57:43.512150 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.514976 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I1001 09:57:43.516680 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.523783 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I1001 09:57:43.525841 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.528513 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I1001 09:57:43.530192 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.537325 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I1001 09:57:43.539341 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.541978 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I1001 09:57:43.543806 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.550858 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I1001 09:57:43.552874 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.555786 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I1001 09:57:43.557536 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.561409 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I1001 09:57:43.563121 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.567295 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I1001 09:57:43.568962 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.571698 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I1001 09:57:43.573377 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.584179 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.590750 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I1001 09:57:43.592930 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.595643 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I1001 09:57:43.597327 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.599508 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.606026 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I1001 09:57:43.608119 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.610915 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I1001 09:57:43.612617 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.617679 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I1001 09:57:43.619385 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.622117 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I1001 09:57:43.623813 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.645409 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I1001 09:57:43.647546 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.650301 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I1001 09:57:43.652134 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.659357 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I1001 09:57:43.661527 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.664430 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I1001 09:57:43.666155 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.673307 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I1001 09:57:43.675481 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.678161 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I1001 09:57:43.679881 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.686999 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I1001 09:57:43.689025 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.691770 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I1001 09:57:43.694234 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.698165 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I1001 09:57:43.699886 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.704111 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I1001 09:57:43.705789 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.708537 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I1001 09:57:43.710221 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.720250 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.726841 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I1001 09:57:43.728984 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.731683 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I1001 09:57:43.733358 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.735486 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.741866 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I1001 09:57:43.743859 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.746610 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I1001 09:57:43.748308 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.753309 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I1001 09:57:43.754977 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.758542 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I1001 09:57:43.760765 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.781651 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I1001 09:57:43.783864 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.786769 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I1001 09:57:43.788630 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.795825 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I1001 09:57:43.798009 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.800997 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I1001 09:57:43.802681 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.810678 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I1001 09:57:43.812876 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.815641 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I1001 09:57:43.817348 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.824501 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I1001 09:57:43.826510 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.829216 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I1001 09:57:43.831001 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.834904 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I1001 09:57:43.836635 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.840788 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I1001 09:57:43.842477 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.845214 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I1001 09:57:43.846903 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.857147 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.863708 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I1001 09:57:43.866562 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.869329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I1001 09:57:43.871029 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.873227 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:43.879612 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I1001 09:57:43.881664 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.884458 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I1001 09:57:43.886160 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.891279 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I1001 09:57:43.892969 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.895765 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I1001 09:57:43.897459 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.920303 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I1001 09:57:43.922552 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.925364 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I1001 09:57:43.927177 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.934240 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I1001 09:57:43.936344 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.939198 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I1001 09:57:43.940892 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.947958 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I1001 09:57:43.950040 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.952728 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I1001 09:57:43.954411 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.961628 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I1001 09:57:43.963679 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.966414 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I1001 09:57:43.968252 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.972163 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I1001 09:57:43.973857 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.978060 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I1001 09:57:43.979767 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:43.982496 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I1001 09:57:43.984229 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:43.994983 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.001809 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I1001 09:57:44.004061 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.006870 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I1001 09:57:44.008587 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.010803 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.017340 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I1001 09:57:44.019382 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.022155 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I1001 09:57:44.023871 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.028897 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I1001 09:57:44.030600 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.033411 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I1001 09:57:44.035112 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.057476 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I1001 09:57:44.059660 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.062370 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I1001 09:57:44.064211 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.071294 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I1001 09:57:44.073287 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.076074 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I1001 09:57:44.077758 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.084790 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I1001 09:57:44.086875 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.089580 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I1001 09:57:44.091286 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.098371 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I1001 09:57:44.100439 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.103188 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I1001 09:57:44.105654 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.109729 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I1001 09:57:44.111473 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.115943 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I1001 09:57:44.117692 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.120576 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I1001 09:57:44.122270 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.132522 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.139322 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I1001 09:57:44.141451 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.144149 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I1001 09:57:44.145850 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.148032 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.154377 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I1001 09:57:44.156438 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.159314 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I1001 09:57:44.161017 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.166143 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I1001 09:57:44.167891 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.171540 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I1001 09:57:44.173261 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.194658 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I1001 09:57:44.196848 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.199547 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I1001 09:57:44.201345 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.208458 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I1001 09:57:44.210484 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.213300 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I1001 09:57:44.214997 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.222880 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I1001 09:57:44.225016 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.227754 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I1001 09:57:44.229469 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.236687 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I1001 09:57:44.238770 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.241633 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I1001 09:57:44.243468 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.247478 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I1001 09:57:44.249208 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.253487 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I1001 09:57:44.255219 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.257990 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I1001 09:57:44.259732 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.269943 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.276531 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I1001 09:57:44.279475 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.282219 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I1001 09:57:44.283954 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.286151 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.292546 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I1001 09:57:44.294569 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.297371 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I1001 09:57:44.299085 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.304196 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I1001 09:57:44.305896 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.308688 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I1001 09:57:44.310393 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.333451 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I1001 09:57:44.335800 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.338576 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I1001 09:57:44.340424 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.347607 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I1001 09:57:44.349721 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.352687 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I1001 09:57:44.354391 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.361652 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I1001 09:57:44.363761 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.366446 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I1001 09:57:44.368202 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.375549 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I1001 09:57:44.377602 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.380397 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I1001 09:57:44.382207 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.386171 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I1001 09:57:44.387925 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.392177 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I1001 09:57:44.393882 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.396655 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I1001 09:57:44.398388 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.409358 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.416168 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I1001 09:57:44.418392 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.421278 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I1001 09:57:44.423017 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.425320 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.431737 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I1001 09:57:44.433801 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.436632 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I1001 09:57:44.438356 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.443550 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I1001 09:57:44.445256 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.448059 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I1001 09:57:44.449800 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.751130 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I1001 09:57:44.753332 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.756053 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I1001 09:57:44.757971 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.765047 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I1001 09:57:44.767055 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.769869 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I1001 09:57:44.771626 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.778625 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I1001 09:57:44.780771 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.783531 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I1001 09:57:44.785246 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.792450 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I1001 09:57:44.794556 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.797407 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I1001 09:57:44.799254 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.803869 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I1001 09:57:44.805649 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.809940 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I1001 09:57:44.811677 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.814433 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I1001 09:57:44.816173 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.826425 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.833331 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I1001 09:57:44.835575 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.838355 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I1001 09:57:44.840104 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.842356 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.848790 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I1001 09:57:44.850863 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.853703 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I1001 09:57:44.855460 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.860741 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I1001 09:57:44.862464 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.865308 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I1001 09:57:44.867021 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.888910 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I1001 09:57:44.891120 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.893852 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I1001 09:57:44.895736 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.902822 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I1001 09:57:44.904913 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.907740 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I1001 09:57:44.909462 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.916592 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I1001 09:57:44.919429 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.922191 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I1001 09:57:44.923993 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.931351 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I1001 09:57:44.933553 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.936474 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I1001 09:57:44.938311 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.942381 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I1001 09:57:44.944141 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.948476 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I1001 09:57:44.950198 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.953004 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I1001 09:57:44.954772 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.965323 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.972329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I1001 09:57:44.974538 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.977348 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I1001 09:57:44.979073 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:44.982026 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:44.988485 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I1001 09:57:44.990504 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:44.993290 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I1001 09:57:44.995007 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.000028 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I1001 09:57:45.001755 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.004568 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I1001 09:57:45.006285 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.026987 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I1001 09:57:45.029114 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.031872 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I1001 09:57:45.034378 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.041436 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I1001 09:57:45.043497 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.046284 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I1001 09:57:45.048069 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.055143 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I1001 09:57:45.057303 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.060158 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I1001 09:57:45.061908 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.069223 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I1001 09:57:45.071454 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.074435 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I1001 09:57:45.076347 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.080468 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I1001 09:57:45.082282 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.086773 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I1001 09:57:45.088580 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.091484 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I1001 09:57:45.093239 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.104369 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.111630 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I1001 09:57:45.114126 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.117300 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I1001 09:57:45.119364 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.121863 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.128787 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I1001 09:57:45.131087 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.134123 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I1001 09:57:45.135887 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.141139 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I1001 09:57:45.142885 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.145757 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I1001 09:57:45.147516 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.170333 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I1001 09:57:45.172644 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.175494 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I1001 09:57:45.177335 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.184562 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I1001 09:57:45.186646 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.189536 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I1001 09:57:45.191293 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.198460 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I1001 09:57:45.200647 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.203458 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I1001 09:57:45.205184 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.212382 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I1001 09:57:45.214437 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.217231 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I1001 09:57:45.219064 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.223695 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I1001 09:57:45.225432 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.229812 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I1001 09:57:45.231576 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.234385 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I1001 09:57:45.236162 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.246768 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.254121 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I1001 09:57:45.256392 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.259284 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I1001 09:57:45.261065 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.263459 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.270073 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I1001 09:57:45.272267 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.275256 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I1001 09:57:45.277033 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.282295 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I1001 09:57:45.284053 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.286845 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I1001 09:57:45.288612 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.310023 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I1001 09:57:45.312085 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.314781 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I1001 09:57:45.316721 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.323825 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I1001 09:57:45.325841 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.328647 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I1001 09:57:45.330377 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.337706 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I1001 09:57:45.340353 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.343033 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I1001 09:57:45.344797 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.351948 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I1001 09:57:45.353977 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.356725 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I1001 09:57:45.358617 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.362561 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I1001 09:57:45.364331 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.368659 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I1001 09:57:45.370477 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.373521 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I1001 09:57:45.375334 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.385929 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.393382 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I1001 09:57:45.395818 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.398724 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I1001 09:57:45.400500 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.403650 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.410220 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I1001 09:57:45.412433 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.415412 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I1001 09:57:45.417174 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.422523 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I1001 09:57:45.424385 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.427298 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I1001 09:57:45.429059 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.451070 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I1001 09:57:45.453374 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.456328 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I1001 09:57:45.458983 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.466178 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I1001 09:57:45.468333 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.471288 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I1001 09:57:45.473032 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.480314 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I1001 09:57:45.482722 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.485793 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I1001 09:57:45.487739 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.495242 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I1001 09:57:45.497535 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.500591 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I1001 09:57:45.502505 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.506711 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I1001 09:57:45.508557 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.513195 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I1001 09:57:45.515040 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.517939 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I1001 09:57:45.519711 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.531247 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.538583 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I1001 09:57:45.540913 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.543943 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I1001 09:57:45.545808 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.548311 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.554999 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I1001 09:57:45.557269 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.560401 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I1001 09:57:45.562172 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.567646 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I1001 09:57:45.569452 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.572322 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I1001 09:57:45.574070 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.596958 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I1001 09:57:45.599229 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.602071 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I1001 09:57:45.604032 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.611471 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I1001 09:57:45.613686 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.616640 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I1001 09:57:45.618397 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.625546 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I1001 09:57:45.627744 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.630553 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I1001 09:57:45.632329 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.639529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I1001 09:57:45.641576 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.644367 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I1001 09:57:45.646221 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.650811 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I1001 09:57:45.652612 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.657050 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I1001 09:57:45.658811 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.661729 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I1001 09:57:45.663572 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.674342 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.681690 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I1001 09:57:45.683979 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.686853 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I1001 09:57:45.688676 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.691089 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.697628 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I1001 09:57:45.699796 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.702703 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I1001 09:57:45.704480 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.709707 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I1001 09:57:45.711480 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.714328 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I1001 09:57:45.716091 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.737699 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I1001 09:57:45.739798 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.742621 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I1001 09:57:45.744591 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.751641 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I1001 09:57:45.753685 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.756538 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I1001 09:57:45.758349 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.765442 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I1001 09:57:45.768110 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.770863 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I1001 09:57:45.772633 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.779764 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I1001 09:57:45.781847 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.784702 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I1001 09:57:45.786592 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.790693 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I1001 09:57:45.792481 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.796922 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I1001 09:57:45.798685 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.801637 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I1001 09:57:45.803575 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.814530 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.821981 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I1001 09:57:45.824466 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.827539 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I1001 09:57:45.829319 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:45.832520 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:45.839154 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I1001 09:57:45.841521 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.846157 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I1001 09:57:45.848030 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.853488 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I1001 09:57:45.855282 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.858132 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I1001 09:57:45.859918 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.936856 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I1001 09:57:45.939183 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:45.941950 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I1001 09:57:45.943743 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.204288 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I1001 09:57:46.206826 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.209918 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I1001 09:57:46.211859 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.219196 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I1001 09:57:46.221368 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.224305 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I1001 09:57:46.226086 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.233266 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I1001 09:57:46.235449 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.238293 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I1001 09:57:46.240101 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.244246 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I1001 09:57:46.246161 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.250784 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I1001 09:57:46.252616 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.255499 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I1001 09:57:46.257281 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.268866 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.276333 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I1001 09:57:46.278597 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.281529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I1001 09:57:46.283338 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.285708 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.292272 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I1001 09:57:46.294483 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.297300 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I1001 09:57:46.299115 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.304596 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I1001 09:57:46.306593 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.309390 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I1001 09:57:46.311197 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.333086 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I1001 09:57:46.335345 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.338196 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I1001 09:57:46.340084 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.347236 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I1001 09:57:46.349312 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.352191 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I1001 09:57:46.353963 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.361182 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I1001 09:57:46.363263 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.366016 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I1001 09:57:46.367808 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.375036 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I1001 09:57:46.377143 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.380045 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I1001 09:57:46.381831 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.385898 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I1001 09:57:46.387702 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.392124 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I1001 09:57:46.393892 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.397183 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I1001 09:57:46.398959 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.409314 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.415966 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I1001 09:57:46.418341 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.421327 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I1001 09:57:46.423135 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.425531 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.432238 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I1001 09:57:46.434542 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.437660 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I1001 09:57:46.439489 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.444904 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I1001 09:57:46.446723 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.449807 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I1001 09:57:46.451658 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.475213 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I1001 09:57:46.477622 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.480592 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I1001 09:57:46.482476 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.489709 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I1001 09:57:46.491837 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.494727 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I1001 09:57:46.496512 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.503707 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I1001 09:57:46.505865 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.508732 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I1001 09:57:46.510509 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.518442 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I1001 09:57:46.520636 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.523607 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I1001 09:57:46.525496 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.529618 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I1001 09:57:46.531431 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.535955 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I1001 09:57:46.537724 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.540599 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I1001 09:57:46.542392 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.552927 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.560030 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I1001 09:57:46.562319 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.565285 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I1001 09:57:46.567085 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.569504 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.576972 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I1001 09:57:46.579302 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.582367 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I1001 09:57:46.584246 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.589774 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I1001 09:57:46.591659 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.594663 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I1001 09:57:46.596496 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.618989 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I1001 09:57:46.621389 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.624338 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I1001 09:57:46.626223 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.634217 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I1001 09:57:46.636316 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.639445 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I1001 09:57:46.641213 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.648243 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I1001 09:57:46.650355 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.653108 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I1001 09:57:46.654879 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.661970 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I1001 09:57:46.664086 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.666851 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I1001 09:57:46.668772 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.672894 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I1001 09:57:46.674667 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.679119 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I1001 09:57:46.680909 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.683762 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I1001 09:57:46.685533 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.696594 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.703115 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I1001 09:57:46.705274 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.708075 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I1001 09:57:46.709843 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.712131 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.718699 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I1001 09:57:46.721042 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.724193 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I1001 09:57:46.725995 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.731756 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I1001 09:57:46.733726 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.736962 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I1001 09:57:46.738808 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.761387 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I1001 09:57:46.763712 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.766625 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I1001 09:57:46.768606 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.776044 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I1001 09:57:46.778170 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.781062 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I1001 09:57:46.782916 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.790106 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I1001 09:57:46.792474 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.795411 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I1001 09:57:46.797234 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.804688 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I1001 09:57:46.806806 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.809664 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I1001 09:57:46.811588 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.815793 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I1001 09:57:46.817618 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.822306 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I1001 09:57:46.824214 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.828057 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I1001 09:57:46.830076 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.841152 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.848563 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I1001 09:57:46.851055 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.854169 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I1001 09:57:46.856097 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.858648 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.865443 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I1001 09:57:46.867753 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.870806 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I1001 09:57:46.872687 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.878454 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I1001 09:57:46.880374 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.883292 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I1001 09:57:46.885091 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.908214 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I1001 09:57:46.910683 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.913730 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I1001 09:57:46.915638 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.922868 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I1001 09:57:46.925002 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.927925 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I1001 09:57:46.929783 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.937293 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I1001 09:57:46.939837 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.942832 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I1001 09:57:46.944649 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.952758 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I1001 09:57:46.955247 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.958321 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I1001 09:57:46.960346 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.964493 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I1001 09:57:46.966308 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.970864 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I1001 09:57:46.972787 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:46.975795 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I1001 09:57:46.977620 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:46.988347 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:46.995430 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I1001 09:57:46.997685 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.000649 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I1001 09:57:47.002474 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.004925 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.012326 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I1001 09:57:47.014816 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.018032 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I1001 09:57:47.019911 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.025694 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I1001 09:57:47.027657 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.030688 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I1001 09:57:47.032522 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.054921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I1001 09:57:47.057233 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.060126 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I1001 09:57:47.062021 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.070035 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I1001 09:57:47.072156 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.075053 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I1001 09:57:47.076874 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.083999 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I1001 09:57:47.086237 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.089103 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I1001 09:57:47.090914 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.098158 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I1001 09:57:47.100314 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.103143 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I1001 09:57:47.105053 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.109205 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I1001 09:57:47.111021 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.115571 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I1001 09:57:47.117359 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.120252 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I1001 09:57:47.122084 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.133361 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.140026 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I1001 09:57:47.142212 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.145025 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I1001 09:57:47.146867 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.149441 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.156614 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I1001 09:57:47.159226 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.162461 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I1001 09:57:47.164404 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.170313 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I1001 09:57:47.172329 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.175477 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I1001 09:57:47.177314 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.200945 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I1001 09:57:47.203251 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.206041 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I1001 09:57:47.207967 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.215060 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I1001 09:57:47.217183 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.220090 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I1001 09:57:47.221894 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.229008 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I1001 09:57:47.231201 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.233965 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I1001 09:57:47.235785 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.242960 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I1001 09:57:47.245229 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.248204 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I1001 09:57:47.250124 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.254390 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I1001 09:57:47.256263 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.260906 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I1001 09:57:47.262726 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.266219 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I1001 09:57:47.268061 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.278781 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.285709 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I1001 09:57:47.288104 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.291063 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I1001 09:57:47.292949 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.295503 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.302040 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I1001 09:57:47.304230 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.307220 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I1001 09:57:47.309034 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.314603 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I1001 09:57:47.316488 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.319512 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I1001 09:57:47.321347 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.327124 140265095153472 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.333595 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I1001 09:57:47.335844 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.336695 140265095153472 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.343747 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I1001 09:57:47.345925 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.346797 140265095153472 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.353254 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I1001 09:57:47.355554 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.356492 140265095153472 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.363121 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I1001 09:57:47.365284 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.366150 140265095153472 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.372515 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I1001 09:57:47.374832 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.375734 140265095153472 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.382138 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I1001 09:57:47.384330 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.385185 140265095153472 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.391579 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I1001 09:57:47.393700 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.394557 140265095153472 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.400967 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I1001 09:57:47.403089 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.404066 140265095153472 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.410389 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I1001 09:57:47.412638 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.413529 140265095153472 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.420722 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I1001 09:57:47.422940 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.423967 140265095153472 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.430440 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I1001 09:57:47.432693 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.433568 140265095153472 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.440009 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I1001 09:57:47.442392 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.443267 140265095153472 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.449661 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I1001 09:57:47.451831 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.452711 140265095153472 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.459227 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I1001 09:57:47.461393 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.462275 140265095153472 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.468661 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I1001 09:57:47.470782 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:57:47.471681 140265095153472 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I1001 09:57:47.478096 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I1001 09:57:47.480230 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.483111 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I1001 09:57:47.485015 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.487779 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I1001 09:57:47.489579 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.493064 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I1001 09:57:47.494894 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.497735 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I1001 09:57:47.499567 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.502420 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I1001 09:57:47.504249 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.507004 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I1001 09:57:47.508833 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.511714 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I1001 09:57:47.513493 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.516283 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I1001 09:57:47.518193 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.520969 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I1001 09:57:47.522803 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.525705 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I1001 09:57:47.527505 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.530241 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I1001 09:57:47.532075 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.534941 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I1001 09:57:47.536770 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.539689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I1001 09:57:47.541520 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.544440 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I1001 09:57:47.546243 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.549041 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I1001 09:57:47.550965 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:47.553886 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I1001 09:57:47.555757 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.507685 140265095153472 py_utils.py:1484] === worker 0 ===
I1001 09:57:48.524759 140265095153472 py_utils.py:1474] worker 0: global_step                                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.524980 140265095153472 py_utils.py:1474] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525044 140265095153472 py_utils.py:1474] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525112 140265095153472 py_utils.py:1474] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525173 140265095153472 py_utils.py:1474] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525241 140265095153472 py_utils.py:1474] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525301 140265095153472 py_utils.py:1474] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525352 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525420 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525469 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525515 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525561 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525606 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525652 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525698 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525744 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525790 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525835 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525881 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525927 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.525973 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526019 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526065 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526110 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526156 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526207 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526254 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526301 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526347 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526393 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526439 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526488 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526536 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526582 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526629 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526675 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526721 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526768 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526814 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526860 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526907 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526952 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.526998 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527048 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527116 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527168 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527215 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527261 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527307 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527353 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527399 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527445 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527491 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527536 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527582 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527627 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527674 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527719 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527765 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527811 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527856 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527902 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527952 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.527999 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528044 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528090 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528136 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528182 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528227 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528272 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528317 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528363 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528408 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528453 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528499 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528544 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528589 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528635 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528680 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528726 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528775 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528822 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528867 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528913 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.528958 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529004 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529049 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529094 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529140 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529185 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529231 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529277 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529323 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529368 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529414 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529460 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529506 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529551 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529597 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529648 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529695 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529741 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529787 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529833 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529878 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529924 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.529970 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530015 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530060 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530104 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530149 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530194 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530239 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530284 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530329 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530375 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530421 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530470 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530516 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530561 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530606 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530652 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530697 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530742 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530786 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530831 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530876 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530921 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.530966 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531010 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531055 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531114 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531165 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531210 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531256 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531301 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531351 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531398 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531444 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531489 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531534 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531580 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531625 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531671 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531717 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531762 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531807 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531852 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531898 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531943 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.531988 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532034 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532079 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532124 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532177 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532224 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532269 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532314 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532359 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532404 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532449 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532494 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532539 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532584 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532629 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532674 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532719 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532764 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532809 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532855 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532900 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532946 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.532991 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533041 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533088 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533133 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533179 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533225 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533270 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533316 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533361 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533407 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533453 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533499 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533545 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533590 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533635 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533680 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533732 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533777 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533822 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533872 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533918 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.533963 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534008 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534054 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534099 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534145 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534190 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534235 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534280 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534325 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534370 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534416 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534461 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534507 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534553 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534597 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534643 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534688 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534738 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534784 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534829 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534875 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534920 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.534965 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535010 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535056 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535114 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535165 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535211 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535256 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535302 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535346 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535391 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535437 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535482 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535526 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535576 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535622 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535667 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535712 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535757 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535802 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535847 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535893 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535938 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.535983 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536028 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536074 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536119 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536164 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536209 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536253 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536298 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536343 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536388 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536437 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536483 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536527 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536577 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536623 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536668 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536712 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536757 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536802 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536847 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536891 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536936 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.536981 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537026 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537071 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537116 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537161 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537205 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537251 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537302 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537349 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537395 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537441 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537487 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537533 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537580 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537625 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537672 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537717 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537762 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537807 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537853 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537898 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537944 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.537988 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538034 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538080 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538129 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538175 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538220 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538265 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538311 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538356 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538408 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538453 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538498 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538542 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538588 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538633 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538679 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538724 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538769 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538814 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538859 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538904 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.538950 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539002 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539048 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539110 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539163 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539210 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539255 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539300 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539346 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539391 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539436 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539481 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539526 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539571 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539616 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539662 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539707 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539758 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539803 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539853 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539900 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539945 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.539990 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540036 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540081 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540126 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540171 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540216 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540262 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540307 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540353 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540398 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540443 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540488 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540533 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540578 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540624 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540669 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540719 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540766 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540810 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540855 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540900 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540945 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.540991 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541036 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541080 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541125 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541169 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541214 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541260 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541304 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541349 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541394 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541439 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541484 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541533 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541579 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541624 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541669 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541715 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541760 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541805 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541851 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541896 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541941 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.541987 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542032 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542078 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542123 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542169 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542213 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542259 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542304 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542350 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542399 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542446 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542492 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542536 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542582 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542627 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542672 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542716 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542761 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542805 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542850 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542895 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542940 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.542984 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543029 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543074 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543136 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543183 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543233 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543280 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543325 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543370 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543415 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543460 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543506 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543551 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543597 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543643 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543688 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543733 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543779 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543823 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543869 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543915 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.543959 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544004 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544050 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544100 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544146 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544192 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544237 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544282 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544328 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544373 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544419 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544464 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544509 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544555 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544601 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544646 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544692 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544738 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544783 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544829 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544874 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544924 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.544970 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545016 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545062 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545108 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545153 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545199 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545245 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545290 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545335 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545381 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545426 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545471 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545517 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545563 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545608 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545654 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545699 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545744 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545794 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545840 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545886 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545931 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.545976 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546022 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546067 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546112 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546158 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546203 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546248 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546293 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546339 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546385 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546431 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546476 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546530 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546575 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546630 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546677 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546723 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546768 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546814 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546859 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546904 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546949 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.546994 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547040 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547085 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547148 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547195 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547241 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547287 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547332 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547377 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547422 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547467 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547518 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547564 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547610 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547655 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547700 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547745 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547791 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547836 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547882 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547927 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.547972 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548017 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548063 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548109 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548154 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548200 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548246 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548292 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548341 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548388 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548435 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548481 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548527 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548573 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548619 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548665 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548710 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548756 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548801 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548847 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548892 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548938 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.548983 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549029 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549074 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549120 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549165 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549216 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549262 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549307 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549353 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549399 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549444 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549489 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549535 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549580 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549626 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549671 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549716 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549761 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549807 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549853 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549899 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549944 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.549989 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550034 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550084 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550130 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550176 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550221 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550267 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550312 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550357 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550402 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550447 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550493 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550539 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550584 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550630 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550675 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550721 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550766 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550812 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550857 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550906 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550951 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.550996 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551041 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551086 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551155 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551201 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551247 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551292 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551337 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551383 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551428 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551474 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551519 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551565 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551609 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551655 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551700 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551745 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551795 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551841 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551887 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551932 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.551977 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552022 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552067 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552113 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552158 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552203 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552248 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552293 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552339 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552385 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552430 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552475 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552520 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552565 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552615 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552662 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552707 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552753 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552798 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552844 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552889 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552935 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.552980 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553025 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553071 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553117 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553162 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553208 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553253 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553298 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553344 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553389 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553434 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553483 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553530 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553575 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553621 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553666 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553711 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553756 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553802 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553847 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553892 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553938 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.553983 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554028 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554073 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554119 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554164 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554210 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554255 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554305 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554351 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554396 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554441 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554487 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554532 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554578 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554623 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554668 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554714 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554759 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554805 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554850 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554894 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554940 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.554985 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555029 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555075 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555139 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555190 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555238 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555283 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555329 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555374 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555420 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555465 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555511 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555556 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555601 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555647 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555692 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555738 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555783 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555829 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555875 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555920 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.555966 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556016 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556062 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556108 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556153 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556199 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556245 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556290 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556335 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556381 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556426 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556472 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556517 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556562 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556607 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556656 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556703 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556748 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556793 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556839 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556890 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556937 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.556983 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557029 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557075 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557120 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557166 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557212 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557257 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557302 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557348 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557393 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557439 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557484 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557529 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557575 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557620 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557666 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557717 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557763 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557809 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557854 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557900 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557946 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.557991 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558037 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558083 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558128 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558173 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558219 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558264 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558310 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558356 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558401 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558446 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558492 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558537 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558587 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558633 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558678 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558724 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558770 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558816 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558861 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558907 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558953 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.558998 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559044 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559089 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559170 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559218 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559264 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559310 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559356 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559402 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559453 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559500 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559545 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559591 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559637 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559683 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559728 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559774 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559820 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559866 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.559926 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560003 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560079 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560150 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560198 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560244 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560289 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560335 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560380 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560431 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560478 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560524 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560569 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560615 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560661 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560706 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560752 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560797 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560842 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560887 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560933 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.560982 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561028 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561074 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561119 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561164 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561210 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561261 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561308 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561353 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561398 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561443 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561488 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561533 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561579 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561624 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561669 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561714 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561759 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561805 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561851 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561896 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561942 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.561987 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562032 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562079 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562129 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562175 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562221 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562266 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562311 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562356 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562402 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562447 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562492 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562539 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562584 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562630 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562675 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562721 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562766 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562811 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562857 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562902 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562947 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.562998 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563044 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563090 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563163 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563210 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563256 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563302 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563349 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563394 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563440 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563485 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563531 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563578 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563623 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563669 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563715 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563761 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563806 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563857 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563904 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563950 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.563996 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564042 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564087 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564133 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564178 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564224 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564269 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564315 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564360 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564406 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564452 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564497 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564543 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564589 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564634 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564680 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564730 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564777 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564823 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564868 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564914 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.564960 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565005 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565050 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565095 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565141 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565186 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565232 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565277 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565323 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565369 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565414 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565460 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565505 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565561 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565608 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565654 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565700 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565745 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565791 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565837 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565882 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565927 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.565971 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566016 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566061 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566106 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566152 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566197 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566242 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566287 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566333 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566377 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566427 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566474 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566520 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566566 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566612 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566658 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566708 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566755 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566802 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566848 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566894 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566940 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.566986 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567031 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567077 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567142 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567189 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567235 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567286 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567334 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567380 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567425 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567471 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567517 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567564 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567609 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567655 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567701 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567747 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567792 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567838 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567883 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567928 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.567973 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568018 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568063 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568108 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568158 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568205 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568250 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568295 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568341 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568387 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568433 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568478 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568524 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568570 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568615 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568660 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568706 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568751 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568796 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568842 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568888 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568933 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.568983 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569029 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569075 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569121 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569166 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569212 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569257 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569303 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569348 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569394 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569440 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569485 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569531 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569576 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569621 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569666 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569711 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569756 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569802 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569851 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569897 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569942 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.569988 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570034 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570080 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570126 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570172 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570217 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570263 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570308 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570354 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570399 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570445 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570491 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570537 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570583 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570628 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570678 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570725 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570770 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570816 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570862 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570907 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570953 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.570999 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571045 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571102 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571155 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571202 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571249 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571295 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571341 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571388 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571434 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571480 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571526 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571577 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571624 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571669 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571715 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571760 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571806 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571852 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571898 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571943 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.571989 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572034 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572080 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572126 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572172 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572217 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572263 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572308 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572354 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572404 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572451 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572496 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572542 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572588 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572634 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572678 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572723 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572769 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572814 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572859 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572904 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572949 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.572995 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.573040 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.573085 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.573130 140265095153472 py_utils.py:1474] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:CPU:0
I1001 09:57:48.573224 140265095153472 py_utils.py:1490] ==========
I1001 09:57:51.103666 140265095153472 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 09:57:53.384854 140265095153472 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:57:55.931483 140265095153472 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:57:58.112231 140265095153472 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:58:00.843351 140265095153472 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W1001 09:58:02.791131 140265095153472 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 09:58:02.918886 140265095153472 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 09:58:05.356338 140265095153472 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 09:58:05.482343 140265095153472 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 09:58:07.381656 140265095153472 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 09:58:07.524627 140265095153472 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 09:58:10.077417 140265095153472 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 09:58:10.569185 140265095153472 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 09:58:13.171472 140265095153472 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:58:16.411727 140265095153472 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:58:19.714630 140265095153472 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:58:21.812184 140265095153472 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I1001 09:58:21.816616 140265095153472 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I1001 09:58:21.909867 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I1001 09:58:21.911928 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:58:21.920812 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I1001 09:58:21.920919 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.921025 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.921113 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.921182 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.921241 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.921320 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.921400 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.921459 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.921517 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.921589 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.921658 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.921735 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.921830 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.921907 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.921975 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.922035 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.922087 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.922158 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.922220 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.922295 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.922372 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.922446 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.922533 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.922598 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.922665 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.922728 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.922804 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.922873 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.922943 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.923006 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.923084 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.923156 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.923233 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.923293 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.923362 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.923432 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.923513 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.923584 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.923654 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.923724 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.923791 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.923860 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.923928 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.923995 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.924063 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.924123 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.924206 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.924278 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.924348 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.924407 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.924479 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.924542 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.924597 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.924670 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.924733 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.924785 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.924864 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.924922 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.924982 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.925053 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.925121 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.925202 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.925267 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.925323 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.925397 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.925458 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.925516 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.925589 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.925653 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.925725 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.925791 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.925863 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.925930 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.925999 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.926062 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.926132 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.926202 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.926279 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.926352 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.926425 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.926496 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.926558 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.926625 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.926699 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.926761 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.926837 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.926907 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.926983 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.927040 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.927125 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.927184 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.927258 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.927322 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.927398 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.927461 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.927532 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.927609 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.927678 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.927734 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.927794 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.927864 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.927931 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.928002 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.928056 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.928129 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.928190 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.928261 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.928326 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.928401 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.928467 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.928541 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.928621 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.928694 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.928770 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.928836 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.928910 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.928972 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.929027 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.929103 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.929176 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.929234 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.929303 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.929375 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.929462 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.929548 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.929627 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.929700 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.929756 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.929833 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.929898 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.929972 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.930042 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.930106 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.930179 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.930256 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.930326 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.930404 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.930475 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.930556 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.930637 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.930727 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.930814 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.930910 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.930977 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.931053 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.931161 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.931252 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.931334 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.931424 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.931513 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.931600 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.931676 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.931765 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.931857 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.931929 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.932017 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.932107 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.932186 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.932269 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.932362 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.932456 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.932544 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.932644 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.932741 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.932820 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.932917 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.932994 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.933085 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.933173 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.933264 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.933364 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.933460 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.933551 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.933645 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.933740 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.933834 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.933933 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.934032 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.934127 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.934223 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.934315 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.934404 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.934500 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.934602 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.934697 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.934786 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.934882 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.934980 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.935068 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.935182 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.935284 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.935379 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.935469 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.935568 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.935668 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.935765 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.935859 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.935957 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.936040 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.936132 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.936215 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.936302 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.936401 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.936475 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.936552 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.936643 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.936742 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.936839 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.936947 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.937047 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.937141 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.937216 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.937315 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.937417 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.937509 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.937608 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.937702 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.937796 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.937889 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.937983 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.938070 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.938170 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.938269 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.938368 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.938468 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.938558 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.938649 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.938734 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.938823 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.938913 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.938997 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.939090 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.939221 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.939308 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.939408 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.939502 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.939600 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.939696 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.939791 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.939876 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.939969 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.940058 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.940135 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.940222 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.940306 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.940406 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.940494 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.940594 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.940687 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.940783 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.940869 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.940969 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.941069 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.941166 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.941263 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.941359 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.941449 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.941552 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.941644 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.941739 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.941824 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.941923 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.942012 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.942110 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.942198 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.942298 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.942397 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.942490 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.942580 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.942665 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.942758 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.942857 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.942951 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.943038 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.943153 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.943249 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.943343 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.943434 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.943533 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.943633 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.943731 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.943823 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.943923 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.944010 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.944108 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.944195 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.944293 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.944390 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.944478 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.944577 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.944667 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.944761 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.944843 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.944939 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.945027 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.945119 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.945211 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.945301 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.945396 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.945485 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.945580 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.945667 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.945761 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.945847 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.945941 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.946038 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.946130 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.946227 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.946308 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.946405 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.946503 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.946594 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.946686 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.946777 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.946869 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.946965 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.947053 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.947162 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.947246 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.947343 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.947430 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.947525 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.947612 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.947706 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.947800 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.947887 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.947983 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.948071 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.948162 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.948255 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.948348 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.948441 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.948535 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.948622 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.948718 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.948813 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.948909 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.949004 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.949089 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.949184 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.949272 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.949364 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.949446 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.949541 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.949629 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.949715 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.949812 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.949909 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.949996 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.950089 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.950181 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.950275 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.950376 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.950477 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.950571 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.950658 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.950742 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.950827 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.950912 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.951009 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.951125 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.951226 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.951317 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.951415 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.951512 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.951610 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.951710 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.951807 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.951895 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.951988 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.952074 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.952171 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.952257 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.952348 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.952445 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.952538 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.952636 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.952728 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.952821 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.952926 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.953012 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.953111 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.953203 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.953296 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.953395 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.953482 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.953580 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.953673 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.953771 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.953862 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.953959 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.954046 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.954142 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.954230 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.954327 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.954416 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.954509 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.954607 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.954700 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.954798 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.954894 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.954988 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.955086 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.955196 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.955298 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.955398 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.955496 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.955595 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.955696 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.955788 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.955878 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.955966 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.956049 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.956127 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.956210 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.956290 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.956367 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.956450 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.956530 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.956608 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.956690 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.956770 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.956846 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.956929 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.957009 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.957086 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.957172 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.957257 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.957337 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.957412 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.957493 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.957577 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.957668 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.957746 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.957825 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.957919 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.958004 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.958090 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.958173 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.958246 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.958320 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.958402 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.958484 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.958576 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.958673 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.958759 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.958851 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.958937 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.959025 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.959135 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.959222 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.959306 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.959395 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.959486 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.959574 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.959653 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.959731 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.959825 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.959924 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.960020 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.960105 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.960183 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.960268 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.960350 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.960427 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.960510 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.960590 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.960667 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.960749 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.960830 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.960908 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.960989 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.961070 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.961147 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.961230 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.961312 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.961389 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.961473 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.961553 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.961629 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.961711 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.961793 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.961870 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.961953 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.962031 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.962108 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.962191 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.962280 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.962360 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.962441 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.962522 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.962601 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.962679 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.962760 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.962843 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.962927 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.963010 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.963086 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.963186 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.963269 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.963364 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.963457 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.963552 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.963650 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.963745 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.963841 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.963936 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.964031 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.964120 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.964207 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.964300 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.964388 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.964478 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.964564 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.964656 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.964747 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.964836 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.964927 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.965014 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.965102 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.965193 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.965281 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.965369 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.965459 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.965544 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.965637 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.965738 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.965825 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 09:58:21.965912 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 09:58:21.966001 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 09:58:21.966090 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 09:58:21.966177 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I1001 09:58:21.966268 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I1001 09:58:21.966356 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 09:58:21.966448 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 09:58:21.966540 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 09:58:21.966629 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 09:58:21.966722 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 09:58:21.966811 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 09:58:21.966898 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 09:58:21.966993 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 09:58:21.967084 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 09:58:21.967189 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I1001 09:58:21.967280 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I1001 09:58:21.967372 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I1001 09:58:21.967464 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I1001 09:58:21.967558 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I1001 09:58:21.967648 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I1001 09:58:21.967742 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I1001 09:58:21.967835 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I1001 09:58:21.967926 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I1001 09:58:21.968013 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I1001 09:58:21.968112 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I1001 09:58:21.968206 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I1001 09:58:21.968297 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I1001 09:58:21.968385 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I1001 09:58:21.968477 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I1001 09:58:21.968570 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I1001 09:58:21.968662 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I1001 09:58:21.968750 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I1001 09:58:21.968846 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I1001 09:58:21.968939 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I1001 09:58:21.969026 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I1001 09:58:21.969120 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I1001 09:58:21.969211 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I1001 09:58:21.969298 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I1001 09:58:21.969386 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I1001 09:58:21.969477 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I1001 09:58:21.969563 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I1001 09:58:21.969653 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I1001 09:58:21.969743 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I1001 09:58:21.969829 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I1001 09:58:21.969918 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I1001 09:58:21.970008 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I1001 09:58:21.970092 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I1001 09:58:21.970175 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I1001 09:58:27.918845 140265095153472 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:58:34.091720 140265095153472 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:58:40.073114 140265095153472 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:58:46.152448 140265095153472 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 09:58:57.914290 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I1001 09:58:57.914584 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.914671 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.914751 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.914819 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.914888 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.914952 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.915014 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.915077 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.915167 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.915231 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.915297 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.915360 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.915426 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.915488 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.915564 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.915627 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.915688 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.915747 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.915807 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.915870 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.915931 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.915994 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.916055 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.916115 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.916174 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.916237 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.916297 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.916361 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.916421 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.916490 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.916551 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.916615 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.916674 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.916734 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.916793 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.916853 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.916916 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.916977 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.917040 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.917101 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.917161 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.917222 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.917285 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.917350 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.917426 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.917487 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.917551 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.917611 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.917675 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.917735 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.917796 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.917856 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.917918 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.917982 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.918045 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.918108 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.918169 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.918230 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.918290 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.918360 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.918421 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.918486 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.918548 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.918612 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.918673 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.918737 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.918798 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.918860 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.918921 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.918981 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.919045 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.919142 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.919222 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.919286 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.919352 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.919414 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.919480 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.919542 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.919606 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.919667 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.919731 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.919791 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.919856 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.919916 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.919977 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.920038 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.920099 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.920161 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.920228 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.920292 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.920353 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.920412 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.920472 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.920536 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.920597 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.920660 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.920721 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.920784 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.920844 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.920907 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.920966 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.921026 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.921085 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.921151 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.921216 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.921277 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.921340 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.921401 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.921461 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.921520 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.921585 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.921646 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.921710 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.921771 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.921834 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.921894 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.921957 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.922022 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.922083 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.922142 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.922203 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.922266 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.922327 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.922389 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.922450 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.922509 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.922569 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.922633 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.922693 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.922757 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.922816 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.922880 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.922944 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.923008 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.923068 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.923154 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.923221 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.923283 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.923346 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.923407 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.923471 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.923532 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.923592 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.923654 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.923719 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.923780 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.923844 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.923909 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.923974 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.924035 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.924099 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.924159 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.924219 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.924278 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.924339 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.924403 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.924463 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.924526 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.924587 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.924646 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.924707 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.924771 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.924837 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.924902 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.924963 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.925027 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.925089 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.925153 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.925214 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.925274 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.925335 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.925396 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.925460 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.925522 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.925586 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.925647 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.925709 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.925774 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.925839 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.925901 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.925965 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.926025 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.926088 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.926149 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.926212 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.926273 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.926334 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.926394 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.926455 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.926520 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.926581 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.926650 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.926712 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.926773 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.926834 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.926898 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.926959 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.927024 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.927085 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.927169 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.927231 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.927295 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.927359 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.927421 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.927481 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.927543 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.927611 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.927673 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.927737 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.927798 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.927858 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.927920 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.927985 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.928045 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.928109 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.928169 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.928233 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.928293 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.928357 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.928417 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.928484 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.928544 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.928606 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.928670 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.928732 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.928796 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.928857 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.928917 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.928977 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.929042 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.929104 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.929168 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.929229 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.929293 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.929352 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.929422 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.929483 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.929543 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.929603 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.929665 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.929728 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.929790 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.929855 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.929917 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.929979 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.930041 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.930107 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.930168 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.930233 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.930294 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.930363 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.930424 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.930488 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.930549 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.930609 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.930670 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.930737 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.930801 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.930862 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.930925 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.930987 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.931047 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.931131 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.931202 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.931268 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.931334 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.931395 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.931457 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.931518 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.931582 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.931643 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.931704 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.931773 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.931836 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.931900 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.931968 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.932031 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.932092 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.932153 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.932218 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.932284 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.932345 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.932409 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.932470 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.932535 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.932596 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.932660 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.932720 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.932780 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.932840 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.932901 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.932964 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.933025 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.933089 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.933154 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.933215 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.933276 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.933342 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.933402 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.933467 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.933528 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.933592 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.933653 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.933717 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.933777 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.933837 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.933897 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.933956 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.934023 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.934085 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.934147 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.934213 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.934273 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.934333 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.934397 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.934457 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.934521 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.934581 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.934645 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.934706 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.934769 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.934829 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.934889 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.934953 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.935014 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.935079 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.935158 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.935222 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.935283 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.935344 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.935404 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.935468 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.935529 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.935593 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.935653 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.935718 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.935779 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.935846 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.935908 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.935969 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.936028 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.936089 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.936152 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.936213 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.936276 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.936336 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.936396 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.936456 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.936521 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.936580 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.936645 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.936705 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.936773 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.936835 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.936898 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.936959 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.937020 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.937080 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.937141 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.937205 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.937265 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.937330 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.937390 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.937454 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.937515 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.937579 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.937641 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.937708 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.937769 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.937834 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.937895 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.937960 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.938020 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.938081 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.938142 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.938203 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.938267 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.938328 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.938391 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.938453 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.938513 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.938575 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.938645 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.938706 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.938771 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.938832 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.938896 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.938957 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.939021 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.939081 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.939159 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.939221 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.939283 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.939347 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.939410 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.939473 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.939540 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.939602 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.939663 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.939728 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.939789 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.939852 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.939913 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.939982 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.940043 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.940107 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.940168 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.940228 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.940288 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.940349 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.940412 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.940478 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.940543 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.940604 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.940664 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.940724 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.940789 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.940851 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.940915 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.940975 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.941039 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.941099 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.941162 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.941224 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.941283 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.941349 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.941411 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.941474 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.941536 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.941600 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.941660 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.941721 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.941781 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.941845 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.941906 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.941969 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.942038 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.942102 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.942161 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.942224 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.942288 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.942350 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.942409 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.942470 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.942533 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.942595 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.942658 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.942718 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.942779 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.942839 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.942903 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.942964 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.943027 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.943088 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.943178 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.943244 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.943310 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.943370 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.943429 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.943490 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.943551 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.943615 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.943675 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.943738 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.943798 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.943857 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.943918 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.943981 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.944043 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.944113 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.944174 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.944238 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.944298 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.944362 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.944422 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.944482 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.944542 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.944608 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.944672 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.944733 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.944797 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.944857 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.944918 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.944977 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.945046 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.945107 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.945172 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.945232 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.945296 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.945356 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.945419 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.945479 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.945539 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.945599 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.945660 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.945724 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.945785 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.945854 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.945916 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.945981 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.946042 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.946107 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.946168 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.946233 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.946294 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.946358 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.946419 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.946481 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.946542 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.946601 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.946661 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.946721 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.946784 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.946850 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.946914 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.946974 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.947035 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.947107 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.947178 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.947240 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.947304 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.947366 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.947429 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.947493 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.947558 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.947618 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.947679 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.947739 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 09:58:57.947805 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 09:58:57.947871 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.947933 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 09:58:57.947996 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.948057 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.948117 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 09:58:57.948176 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.948241 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.948302 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.948366 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.948427 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.948491 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.948551 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 09:58:57.948616 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.948680 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.948741 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 09:58:57.948800 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.948862 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.948922 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.948982 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949041 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949100 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949160 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949221 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949281 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949341 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949400 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949460 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949520 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949580 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949640 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949699 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I1001 09:58:57.949763 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.949828 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.949892 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.949957 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950021 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950084 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950148 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950212 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950277 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950340 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950403 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950466 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950529 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950592 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950655 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:58:57.950720 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 09:59:07.808890 140265095153472 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f91882fbbe0>>
I1001 09:59:13.071347 140265095153472 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I1001 09:59:13.074391 140265095153472 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I1001 09:59:13.079638 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I1001 09:59:13.084716 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I1001 09:59:13.089775 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I1001 09:59:13.094791 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I1001 09:59:13.099764 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I1001 09:59:13.104826 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I1001 09:59:13.109740 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I1001 09:59:13.114733 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I1001 09:59:13.119661 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I1001 09:59:13.124651 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I1001 09:59:13.129569 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I1001 09:59:13.134559 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I1001 09:59:13.139520 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I1001 09:59:13.144505 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I1001 09:59:13.148306 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I1001 09:59:13.152091 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I1001 09:59:13.156929 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I1001 09:59:13.161965 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I1001 09:59:13.166908 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I1001 09:59:13.171974 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I1001 09:59:13.177228 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I1001 09:59:13.183648 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I1001 09:59:13.189157 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I1001 09:59:13.194197 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I1001 09:59:13.199554 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I1001 09:59:13.204541 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I1001 09:59:13.209843 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I1001 09:59:13.215025 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I1001 09:59:13.220380 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I1001 09:59:13.225387 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I1001 09:59:13.230489 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I1001 09:59:13.235497 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I1001 09:59:13.240549 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I1001 09:59:13.245620 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I1001 09:59:13.250535 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I1001 09:59:13.255569 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I1001 09:59:13.260527 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I1001 09:59:13.265526 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I1001 09:59:13.270480 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I1001 09:59:13.275538 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I1001 09:59:13.280498 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I1001 09:59:13.285555 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I1001 09:59:13.290504 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I1001 09:59:13.296165 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I1001 09:59:13.301204 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I1001 09:59:13.306147 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I1001 09:59:13.311239 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I1001 09:59:13.316174 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I1001 09:59:13.319936 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I1001 09:59:13.323726 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I1001 09:59:13.328605 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I1001 09:59:13.333663 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I1001 09:59:13.338789 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I1001 09:59:13.343757 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I1001 09:59:13.348804 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I1001 09:59:13.353826 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I1001 09:59:13.358847 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I1001 09:59:13.363927 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I1001 09:59:13.369333 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I1001 09:59:13.374329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I1001 09:59:13.379453 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I1001 09:59:13.384465 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I1001 09:59:13.389705 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I1001 09:59:13.394788 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I1001 09:59:13.399748 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I1001 09:59:13.405374 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I1001 09:59:13.410286 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I1001 09:59:13.415425 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I1001 09:59:13.420386 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I1001 09:59:13.425441 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I1001 09:59:13.430455 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I1001 09:59:13.435559 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I1001 09:59:13.440576 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I1001 09:59:13.445636 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I1001 09:59:13.450677 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I1001 09:59:13.455657 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I1001 09:59:13.460738 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I1001 09:59:13.465676 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I1001 09:59:13.470844 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I1001 09:59:13.475812 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I1001 09:59:13.480854 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I1001 09:59:13.485810 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I1001 09:59:13.489759 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I1001 09:59:13.493545 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I1001 09:59:13.498423 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I1001 09:59:13.503753 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I1001 09:59:13.509283 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I1001 09:59:13.514227 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I1001 09:59:13.519997 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I1001 09:59:13.525043 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I1001 09:59:13.530126 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I1001 09:59:13.535130 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I1001 09:59:13.540195 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I1001 09:59:13.545161 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I1001 09:59:13.550209 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I1001 09:59:13.555262 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I1001 09:59:13.560222 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I1001 09:59:13.565307 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I1001 09:59:13.570271 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I1001 09:59:13.575344 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I1001 09:59:13.580349 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I1001 09:59:13.585447 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I1001 09:59:13.590432 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I1001 09:59:13.595585 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I1001 09:59:13.600579 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I1001 09:59:13.605621 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I1001 09:59:13.610569 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I1001 09:59:13.615660 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I1001 09:59:13.620731 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I1001 09:59:13.625702 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I1001 09:59:13.631471 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I1001 09:59:13.636452 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I1001 09:59:13.641476 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I1001 09:59:13.646435 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I1001 09:59:13.651530 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I1001 09:59:13.656497 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I1001 09:59:13.660395 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I1001 09:59:13.664142 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I1001 09:59:13.669088 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I1001 09:59:13.674222 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I1001 09:59:13.679310 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I1001 09:59:13.684260 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I1001 09:59:13.689311 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I1001 09:59:13.694277 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I1001 09:59:13.699398 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I1001 09:59:13.704329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I1001 09:59:13.709364 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I1001 09:59:13.714342 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I1001 09:59:13.719439 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I1001 09:59:13.724546 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I1001 09:59:13.729526 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I1001 09:59:13.734582 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I1001 09:59:13.739613 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I1001 09:59:13.745279 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I1001 09:59:13.750258 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I1001 09:59:13.755379 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I1001 09:59:13.760353 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I1001 09:59:13.765389 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I1001 09:59:13.770327 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I1001 09:59:13.775404 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I1001 09:59:13.780361 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I1001 09:59:13.785444 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I1001 09:59:13.790487 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I1001 09:59:13.795480 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I1001 09:59:13.800634 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I1001 09:59:13.805680 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I1001 09:59:13.810725 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I1001 09:59:13.815751 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I1001 09:59:13.820962 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I1001 09:59:13.825988 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I1001 09:59:13.830054 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I1001 09:59:13.833802 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I1001 09:59:13.838712 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I1001 09:59:13.843882 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I1001 09:59:13.848939 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I1001 09:59:13.853882 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I1001 09:59:13.859624 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I1001 09:59:13.864591 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I1001 09:59:13.869652 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I1001 09:59:13.874618 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I1001 09:59:13.879726 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I1001 09:59:13.884678 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I1001 09:59:13.889743 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I1001 09:59:13.894813 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I1001 09:59:13.899809 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I1001 09:59:13.904928 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I1001 09:59:13.909899 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I1001 09:59:13.914978 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I1001 09:59:13.919984 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I1001 09:59:13.925116 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I1001 09:59:13.930118 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I1001 09:59:13.935207 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I1001 09:59:13.940189 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I1001 09:59:13.945229 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I1001 09:59:13.950201 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I1001 09:59:13.955328 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I1001 09:59:13.960515 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I1001 09:59:13.965536 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I1001 09:59:13.971316 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I1001 09:59:13.976329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I1001 09:59:13.981401 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I1001 09:59:13.986503 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I1001 09:59:13.991641 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I1001 09:59:13.996602 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I1001 09:59:14.000507 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I1001 09:59:14.004199 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I1001 09:59:14.009109 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I1001 09:59:14.014187 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I1001 09:59:14.019264 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I1001 09:59:14.024280 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I1001 09:59:14.029381 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I1001 09:59:14.034378 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I1001 09:59:14.039493 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I1001 09:59:14.044466 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I1001 09:59:14.049541 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I1001 09:59:14.054575 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I1001 09:59:14.059676 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I1001 09:59:14.064777 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I1001 09:59:14.069764 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I1001 09:59:14.074828 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I1001 09:59:14.079805 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I1001 09:59:14.085574 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I1001 09:59:14.090642 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I1001 09:59:14.095873 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I1001 09:59:14.100923 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I1001 09:59:14.106006 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I1001 09:59:14.111077 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I1001 09:59:14.116436 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I1001 09:59:14.121543 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I1001 09:59:14.126644 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I1001 09:59:14.131717 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I1001 09:59:14.136696 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I1001 09:59:14.141856 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I1001 09:59:14.147124 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I1001 09:59:14.152435 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I1001 09:59:14.157456 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I1001 09:59:14.162517 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I1001 09:59:14.167517 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I1001 09:59:14.171416 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I1001 09:59:14.175148 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I1001 09:59:14.180035 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I1001 09:59:14.185102 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I1001 09:59:14.190150 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I1001 09:59:14.195147 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I1001 09:59:14.200713 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I1001 09:59:14.205698 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I1001 09:59:14.210762 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I1001 09:59:14.215809 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I1001 09:59:14.220888 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I1001 09:59:14.225871 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I1001 09:59:14.231019 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I1001 09:59:14.236100 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I1001 09:59:14.241070 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I1001 09:59:14.246243 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I1001 09:59:14.251344 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I1001 09:59:14.256417 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I1001 09:59:14.261703 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I1001 09:59:14.266956 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I1001 09:59:14.272289 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I1001 09:59:14.277419 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I1001 09:59:14.282485 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I1001 09:59:14.287598 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I1001 09:59:14.292624 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I1001 09:59:14.298013 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I1001 09:59:14.303513 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I1001 09:59:14.308950 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I1001 09:59:14.314990 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I1001 09:59:14.320031 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I1001 09:59:14.325209 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I1001 09:59:14.330204 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I1001 09:59:14.335329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I1001 09:59:14.340384 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I1001 09:59:14.344293 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I1001 09:59:14.348011 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I1001 09:59:14.353028 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I1001 09:59:14.358142 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I1001 09:59:14.363289 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I1001 09:59:14.368416 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I1001 09:59:14.373579 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I1001 09:59:14.378746 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I1001 09:59:14.383990 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I1001 09:59:14.389055 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I1001 09:59:14.394243 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I1001 09:59:14.399320 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I1001 09:59:14.404430 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I1001 09:59:14.409516 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I1001 09:59:14.414647 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I1001 09:59:14.419918 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I1001 09:59:14.425075 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I1001 09:59:14.430962 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I1001 09:59:14.436329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I1001 09:59:14.441506 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I1001 09:59:14.446525 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I1001 09:59:14.451637 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I1001 09:59:14.456659 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I1001 09:59:14.461771 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I1001 09:59:14.466791 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I1001 09:59:14.471941 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I1001 09:59:14.477003 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I1001 09:59:14.481992 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I1001 09:59:14.487070 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I1001 09:59:14.492061 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I1001 09:59:14.497160 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I1001 09:59:14.502169 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I1001 09:59:14.507310 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I1001 09:59:14.512267 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I1001 09:59:14.516184 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I1001 09:59:14.519900 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I1001 09:59:14.524792 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I1001 09:59:14.529946 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I1001 09:59:14.535023 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I1001 09:59:14.540017 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I1001 09:59:14.545592 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I1001 09:59:14.550568 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I1001 09:59:14.555714 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I1001 09:59:14.560904 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I1001 09:59:14.566238 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I1001 09:59:14.571590 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I1001 09:59:14.577312 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I1001 09:59:14.582854 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I1001 09:59:14.587990 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I1001 09:59:14.593162 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I1001 09:59:14.598305 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I1001 09:59:14.603524 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I1001 09:59:14.608701 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I1001 09:59:14.614034 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I1001 09:59:14.619409 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I1001 09:59:14.624497 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I1001 09:59:14.629577 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I1001 09:59:14.634738 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I1001 09:59:14.639775 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I1001 09:59:14.644961 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I1001 09:59:14.650054 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I1001 09:59:14.655080 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I1001 09:59:14.660841 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I1001 09:59:14.665927 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I1001 09:59:14.671437 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I1001 09:59:14.676902 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I1001 09:59:14.682462 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I1001 09:59:14.687673 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I1001 09:59:14.691776 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I1001 09:59:14.695545 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I1001 09:59:14.700558 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I1001 09:59:14.705823 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I1001 09:59:14.710943 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I1001 09:59:14.716116 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I1001 09:59:14.721334 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I1001 09:59:14.726351 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I1001 09:59:14.731568 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I1001 09:59:14.736732 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I1001 09:59:14.741935 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I1001 09:59:14.746967 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I1001 09:59:14.752252 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I1001 09:59:14.757656 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I1001 09:59:14.762652 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I1001 09:59:14.767848 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I1001 09:59:14.773295 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I1001 09:59:14.779597 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I1001 09:59:14.784958 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I1001 09:59:14.790687 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I1001 09:59:14.796386 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I1001 09:59:14.801655 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I1001 09:59:14.806703 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I1001 09:59:14.811841 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I1001 09:59:14.816843 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I1001 09:59:14.821977 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I1001 09:59:14.827067 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I1001 09:59:14.832201 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I1001 09:59:14.837285 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I1001 09:59:14.842283 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I1001 09:59:14.847378 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I1001 09:59:14.852385 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I1001 09:59:14.857546 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I1001 09:59:14.862595 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I1001 09:59:14.866647 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I1001 09:59:14.870402 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I1001 09:59:14.875535 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I1001 09:59:14.880815 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I1001 09:59:14.885975 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I1001 09:59:14.890991 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I1001 09:59:14.896701 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I1001 09:59:14.901762 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I1001 09:59:14.906857 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I1001 09:59:14.911877 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I1001 09:59:14.916953 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I1001 09:59:14.921957 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I1001 09:59:14.927054 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I1001 09:59:14.932176 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I1001 09:59:14.937177 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I1001 09:59:14.942288 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I1001 09:59:14.947292 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I1001 09:59:14.952422 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I1001 09:59:14.957384 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I1001 09:59:14.962534 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I1001 09:59:14.967601 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I1001 09:59:14.972667 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I1001 09:59:14.977715 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I1001 09:59:14.982771 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I1001 09:59:14.987884 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I1001 09:59:14.993546 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I1001 09:59:14.999321 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I1001 09:59:15.004780 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I1001 09:59:15.010868 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I1001 09:59:15.016426 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I1001 09:59:15.021756 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I1001 09:59:15.026977 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I1001 09:59:15.032514 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I1001 09:59:15.037632 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I1001 09:59:15.041703 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I1001 09:59:15.045462 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I1001 09:59:15.050379 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I1001 09:59:15.055564 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I1001 09:59:15.060682 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I1001 09:59:15.065694 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I1001 09:59:15.070785 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I1001 09:59:15.075809 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I1001 09:59:15.080962 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I1001 09:59:15.086111 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I1001 09:59:15.091376 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I1001 09:59:15.096570 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I1001 09:59:15.101682 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I1001 09:59:15.106834 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I1001 09:59:15.111927 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I1001 09:59:15.117054 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I1001 09:59:15.122133 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I1001 09:59:15.128038 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I1001 09:59:15.133210 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I1001 09:59:15.138434 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I1001 09:59:15.143542 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I1001 09:59:15.148635 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I1001 09:59:15.153711 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I1001 09:59:15.159003 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I1001 09:59:15.164112 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I1001 09:59:15.169265 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I1001 09:59:15.174367 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I1001 09:59:15.179416 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I1001 09:59:15.184529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I1001 09:59:15.189575 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I1001 09:59:15.194689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I1001 09:59:15.199743 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I1001 09:59:15.204828 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I1001 09:59:15.209854 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I1001 09:59:15.213849 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I1001 09:59:15.217622 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I1001 09:59:15.222572 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I1001 09:59:15.227742 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I1001 09:59:15.232936 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I1001 09:59:15.238107 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I1001 09:59:15.243896 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I1001 09:59:15.249024 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I1001 09:59:15.254191 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I1001 09:59:15.259243 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I1001 09:59:15.264338 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I1001 09:59:15.269369 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I1001 09:59:15.274480 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I1001 09:59:15.279602 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I1001 09:59:15.284610 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I1001 09:59:15.289804 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I1001 09:59:15.294815 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I1001 09:59:15.299999 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I1001 09:59:15.305044 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I1001 09:59:15.310236 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I1001 09:59:15.315347 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I1001 09:59:15.320475 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I1001 09:59:15.325529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I1001 09:59:15.330667 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I1001 09:59:15.335711 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I1001 09:59:15.340921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I1001 09:59:15.346021 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I1001 09:59:15.351040 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I1001 09:59:15.356799 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I1001 09:59:15.361826 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I1001 09:59:15.367005 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I1001 09:59:15.372071 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I1001 09:59:15.377267 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I1001 09:59:15.382312 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I1001 09:59:15.386291 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I1001 09:59:15.390198 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I1001 09:59:15.395144 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I1001 09:59:15.400298 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I1001 09:59:15.405414 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I1001 09:59:15.410440 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I1001 09:59:15.415593 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I1001 09:59:15.420686 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I1001 09:59:15.425834 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I1001 09:59:15.430946 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I1001 09:59:15.436131 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I1001 09:59:15.441241 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I1001 09:59:15.446375 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I1001 09:59:15.451542 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I1001 09:59:15.456554 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I1001 09:59:15.461667 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I1001 09:59:15.466704 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I1001 09:59:15.472477 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I1001 09:59:15.477484 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I1001 09:59:15.482652 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I1001 09:59:15.487733 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I1001 09:59:15.492860 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I1001 09:59:15.497897 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I1001 09:59:15.503057 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I1001 09:59:15.508096 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I1001 09:59:15.513305 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I1001 09:59:15.518422 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I1001 09:59:15.523688 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I1001 09:59:15.528931 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I1001 09:59:15.534014 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I1001 09:59:15.539188 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I1001 09:59:15.544297 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I1001 09:59:15.549502 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I1001 09:59:15.554547 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I1001 09:59:15.558490 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I1001 09:59:15.562283 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I1001 09:59:15.567281 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I1001 09:59:15.572504 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I1001 09:59:15.577692 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I1001 09:59:15.582775 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I1001 09:59:15.588564 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I1001 09:59:15.593605 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I1001 09:59:15.598744 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I1001 09:59:15.603917 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I1001 09:59:15.609047 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I1001 09:59:15.614103 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I1001 09:59:15.619276 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I1001 09:59:15.624583 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I1001 09:59:15.629822 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I1001 09:59:15.635085 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I1001 09:59:15.640204 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I1001 09:59:15.645403 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I1001 09:59:15.650447 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I1001 09:59:15.655693 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I1001 09:59:15.660738 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I1001 09:59:15.665904 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I1001 09:59:15.671044 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I1001 09:59:15.676311 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I1001 09:59:15.681368 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I1001 09:59:15.686499 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I1001 09:59:15.691658 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I1001 09:59:15.696679 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I1001 09:59:15.702420 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I1001 09:59:15.707522 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I1001 09:59:15.712653 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I1001 09:59:15.717715 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I1001 09:59:15.722863 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I1001 09:59:15.727899 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I1001 09:59:15.731903 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I1001 09:59:15.735676 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I1001 09:59:15.740698 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I1001 09:59:15.745914 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I1001 09:59:15.751123 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I1001 09:59:15.756329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I1001 09:59:15.761674 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I1001 09:59:15.766805 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I1001 09:59:15.771998 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I1001 09:59:15.777039 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I1001 09:59:15.782269 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I1001 09:59:15.787375 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I1001 09:59:15.792557 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I1001 09:59:15.797733 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I1001 09:59:15.802780 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I1001 09:59:15.807938 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I1001 09:59:15.813018 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I1001 09:59:15.818718 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I1001 09:59:15.823793 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I1001 09:59:15.828991 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I1001 09:59:15.834100 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I1001 09:59:15.839293 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I1001 09:59:15.844375 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I1001 09:59:15.849602 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I1001 09:59:15.854685 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I1001 09:59:15.859866 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I1001 09:59:15.865019 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I1001 09:59:15.870048 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I1001 09:59:15.875220 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I1001 09:59:15.880275 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I1001 09:59:15.885442 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I1001 09:59:15.890497 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I1001 09:59:15.895666 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I1001 09:59:15.900738 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I1001 09:59:15.904945 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I1001 09:59:15.908777 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I1001 09:59:15.913747 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I1001 09:59:15.919037 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I1001 09:59:15.924244 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I1001 09:59:15.929284 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I1001 09:59:15.935028 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I1001 09:59:15.940138 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I1001 09:59:15.945339 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I1001 09:59:15.950387 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I1001 09:59:15.955563 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I1001 09:59:15.960670 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I1001 09:59:15.965823 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I1001 09:59:15.971042 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I1001 09:59:15.976154 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I1001 09:59:15.981311 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I1001 09:59:15.986369 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I1001 09:59:15.991568 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I1001 09:59:15.996600 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I1001 09:59:16.001795 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I1001 09:59:16.006840 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I1001 09:59:16.012100 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I1001 09:59:16.017428 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I1001 09:59:16.022697 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I1001 09:59:16.027826 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I1001 09:59:16.033107 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I1001 09:59:16.038253 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I1001 09:59:16.043345 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I1001 09:59:16.049111 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I1001 09:59:16.054172 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I1001 09:59:16.059345 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I1001 09:59:16.064419 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I1001 09:59:16.069541 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I1001 09:59:16.074651 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I1001 09:59:16.078638 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I1001 09:59:16.082484 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I1001 09:59:16.087519 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I1001 09:59:16.092733 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I1001 09:59:16.097961 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I1001 09:59:16.103213 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I1001 09:59:16.108479 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I1001 09:59:16.113716 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I1001 09:59:16.119212 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I1001 09:59:16.124544 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I1001 09:59:16.129731 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I1001 09:59:16.134874 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I1001 09:59:16.140084 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I1001 09:59:16.145242 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I1001 09:59:16.150362 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I1001 09:59:16.155564 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I1001 09:59:16.160657 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I1001 09:59:16.166578 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I1001 09:59:16.171792 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I1001 09:59:16.177150 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I1001 09:59:16.182264 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I1001 09:59:16.187433 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I1001 09:59:16.192492 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I1001 09:59:16.197637 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I1001 09:59:16.202709 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I1001 09:59:16.207880 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I1001 09:59:16.213041 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I1001 09:59:16.218150 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I1001 09:59:16.223348 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I1001 09:59:16.228486 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I1001 09:59:16.233710 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I1001 09:59:16.238841 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I1001 09:59:16.244074 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I1001 09:59:16.249171 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I1001 09:59:16.253212 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I1001 09:59:16.257022 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I1001 09:59:16.261993 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I1001 09:59:16.267194 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I1001 09:59:16.272328 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I1001 09:59:16.277374 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I1001 09:59:16.283155 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I1001 09:59:16.288230 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I1001 09:59:16.293412 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I1001 09:59:16.298513 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I1001 09:59:16.303725 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I1001 09:59:16.308876 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I1001 09:59:16.314096 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I1001 09:59:16.319325 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I1001 09:59:16.324387 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I1001 09:59:16.329532 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I1001 09:59:16.334640 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I1001 09:59:16.339893 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I1001 09:59:16.344921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I1001 09:59:16.350140 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I1001 09:59:16.355317 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I1001 09:59:16.360483 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I1001 09:59:16.365552 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I1001 09:59:16.370833 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I1001 09:59:16.375967 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I1001 09:59:16.381173 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I1001 09:59:16.386373 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I1001 09:59:16.391593 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I1001 09:59:16.397361 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I1001 09:59:16.402527 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I1001 09:59:16.407737 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I1001 09:59:16.412838 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I1001 09:59:16.418019 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I1001 09:59:16.423117 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I1001 09:59:16.427134 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I1001 09:59:16.431002 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I1001 09:59:16.436002 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I1001 09:59:16.441204 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I1001 09:59:16.446379 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I1001 09:59:16.451470 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I1001 09:59:16.456692 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I1001 09:59:16.461798 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I1001 09:59:16.467022 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I1001 09:59:16.472106 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I1001 09:59:16.477240 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I1001 09:59:16.482342 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I1001 09:59:16.487632 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I1001 09:59:16.492819 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I1001 09:59:16.497987 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I1001 09:59:16.503328 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I1001 09:59:16.508466 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I1001 09:59:16.514272 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I1001 09:59:16.519495 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I1001 09:59:16.524699 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I1001 09:59:16.529771 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I1001 09:59:16.534921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I1001 09:59:16.540013 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I1001 09:59:16.545264 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I1001 09:59:16.550384 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I1001 09:59:16.555672 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I1001 09:59:16.560976 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I1001 09:59:16.566037 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I1001 09:59:16.571290 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I1001 09:59:16.576371 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I1001 09:59:16.581530 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I1001 09:59:16.586623 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I1001 09:59:16.591792 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I1001 09:59:16.596871 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I1001 09:59:16.600872 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I1001 09:59:16.604683 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I1001 09:59:16.609697 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I1001 09:59:16.614888 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I1001 09:59:16.620134 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I1001 09:59:16.625254 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I1001 09:59:16.631127 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I1001 09:59:16.636257 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I1001 09:59:16.641443 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I1001 09:59:16.646520 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I1001 09:59:16.651918 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I1001 09:59:16.657104 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I1001 09:59:16.662457 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I1001 09:59:16.667683 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I1001 09:59:16.672769 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I1001 09:59:16.678087 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I1001 09:59:16.683233 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I1001 09:59:16.688461 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I1001 09:59:16.693575 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I1001 09:59:16.698880 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I1001 09:59:16.704093 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I1001 09:59:16.709282 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I1001 09:59:16.714376 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I1001 09:59:16.719677 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I1001 09:59:16.724799 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I1001 09:59:16.730068 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I1001 09:59:16.735266 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I1001 09:59:16.740332 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I1001 09:59:16.746073 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I1001 09:59:16.751188 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I1001 09:59:16.756403 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I1001 09:59:16.761504 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I1001 09:59:16.766669 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I1001 09:59:16.771754 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I1001 09:59:16.775782 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I1001 09:59:16.779622 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I1001 09:59:16.784602 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I1001 09:59:16.789922 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I1001 09:59:16.795193 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I1001 09:59:16.800250 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I1001 09:59:16.805422 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I1001 09:59:16.810532 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I1001 09:59:16.815794 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I1001 09:59:16.820900 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I1001 09:59:16.826074 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I1001 09:59:16.831330 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I1001 09:59:16.836514 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I1001 09:59:16.841688 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I1001 09:59:16.846792 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I1001 09:59:16.852008 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I1001 09:59:16.857086 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I1001 09:59:16.862829 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I1001 09:59:16.867930 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I1001 09:59:16.873236 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I1001 09:59:16.878399 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I1001 09:59:16.883629 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I1001 09:59:16.888818 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I1001 09:59:16.894005 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I1001 09:59:16.899109 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I1001 09:59:16.904284 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I1001 09:59:16.909449 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I1001 09:59:16.914538 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I1001 09:59:16.919795 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I1001 09:59:16.924906 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I1001 09:59:16.930156 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I1001 09:59:16.935344 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I1001 09:59:16.940632 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I1001 09:59:16.945770 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I1001 09:59:16.949843 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I1001 09:59:16.953689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I1001 09:59:16.958714 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I1001 09:59:16.963969 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I1001 09:59:16.969177 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I1001 09:59:16.974262 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I1001 09:59:16.980067 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I1001 09:59:16.985193 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I1001 09:59:16.990397 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I1001 09:59:16.995482 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I1001 09:59:17.000660 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I1001 09:59:17.005764 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I1001 09:59:17.010929 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I1001 09:59:17.016138 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I1001 09:59:17.021243 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I1001 09:59:17.026469 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I1001 09:59:17.031824 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I1001 09:59:17.037142 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I1001 09:59:17.042272 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I1001 09:59:17.047604 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I1001 09:59:17.052680 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I1001 09:59:17.057914 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I1001 09:59:17.063028 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I1001 09:59:17.068262 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I1001 09:59:17.073419 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I1001 09:59:17.078705 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I1001 09:59:17.083962 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I1001 09:59:17.089137 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I1001 09:59:17.094908 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I1001 09:59:17.100098 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I1001 09:59:17.105271 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I1001 09:59:17.110410 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I1001 09:59:17.115690 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I1001 09:59:17.120846 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I1001 09:59:17.125090 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I1001 09:59:17.128987 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I1001 09:59:17.134060 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I1001 09:59:17.139356 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I1001 09:59:17.144597 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I1001 09:59:17.149722 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I1001 09:59:17.154948 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I1001 09:59:17.160152 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I1001 09:59:17.165617 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I1001 09:59:17.171152 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I1001 09:59:17.176428 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I1001 09:59:17.181552 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I1001 09:59:17.186718 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I1001 09:59:17.191900 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I1001 09:59:17.196975 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I1001 09:59:17.202148 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I1001 09:59:17.207329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I1001 09:59:17.213064 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I1001 09:59:17.218128 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I1001 09:59:17.223426 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I1001 09:59:17.228517 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I1001 09:59:17.233753 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I1001 09:59:17.238859 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I1001 09:59:17.244070 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I1001 09:59:17.249181 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I1001 09:59:17.254365 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I1001 09:59:17.259573 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I1001 09:59:17.264719 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I1001 09:59:17.269920 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I1001 09:59:17.275045 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I1001 09:59:17.280557 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I1001 09:59:17.285708 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I1001 09:59:17.290942 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I1001 09:59:17.296413 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I1001 09:59:17.300573 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I1001 09:59:17.304439 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I1001 09:59:17.309562 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I1001 09:59:17.315111 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I1001 09:59:17.320878 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I1001 09:59:17.326520 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I1001 09:59:17.332849 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I1001 09:59:17.338334 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I1001 09:59:17.343595 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I1001 09:59:17.348812 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I1001 09:59:17.354145 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I1001 09:59:17.359366 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I1001 09:59:17.364647 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I1001 09:59:17.369887 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I1001 09:59:17.375010 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I1001 09:59:17.380228 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I1001 09:59:17.385355 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I1001 09:59:17.390790 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I1001 09:59:17.396093 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I1001 09:59:17.401421 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I1001 09:59:17.406591 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I1001 09:59:17.411838 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I1001 09:59:17.416942 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I1001 09:59:17.422168 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I1001 09:59:17.427297 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I1001 09:59:17.432582 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I1001 09:59:17.437938 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I1001 09:59:17.443430 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I1001 09:59:17.449553 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I1001 09:59:17.455137 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I1001 09:59:17.460442 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I1001 09:59:17.465586 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I1001 09:59:17.470836 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I1001 09:59:17.475977 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I1001 09:59:17.480108 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I1001 09:59:17.483975 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I1001 09:59:17.488990 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I1001 09:59:17.494195 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I1001 09:59:17.499511 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I1001 09:59:17.504681 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I1001 09:59:17.509918 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I1001 09:59:17.515074 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I1001 09:59:17.520293 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I1001 09:59:17.525456 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I1001 09:59:17.530685 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I1001 09:59:17.535828 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I1001 09:59:17.541030 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I1001 09:59:17.546201 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I1001 09:59:17.551340 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I1001 09:59:17.556517 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I1001 09:59:17.561638 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I1001 09:59:17.567377 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I1001 09:59:17.572448 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I1001 09:59:17.577766 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I1001 09:59:17.583057 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I1001 09:59:17.588355 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I1001 09:59:17.593536 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I1001 09:59:17.599241 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I1001 09:59:17.604550 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I1001 09:59:17.609829 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I1001 09:59:17.615165 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I1001 09:59:17.620645 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I1001 09:59:17.626416 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I1001 09:59:17.632070 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I1001 09:59:17.637578 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I1001 09:59:17.643233 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I1001 09:59:17.648708 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I1001 09:59:17.653840 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I1001 09:59:17.657940 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I1001 09:59:17.661799 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I1001 09:59:17.666902 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I1001 09:59:17.672159 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I1001 09:59:17.677379 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I1001 09:59:17.682513 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I1001 09:59:17.688416 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I1001 09:59:17.694020 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I1001 09:59:18.449585 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I1001 09:59:18.455405 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I1001 09:59:18.460590 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I1001 09:59:18.465782 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I1001 09:59:18.470948 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I1001 09:59:18.476414 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I1001 09:59:18.481626 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I1001 09:59:18.486940 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I1001 09:59:18.492134 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I1001 09:59:18.497457 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I1001 09:59:18.502645 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I1001 09:59:18.507962 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I1001 09:59:18.513123 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I1001 09:59:18.518386 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I1001 09:59:18.523665 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I1001 09:59:18.528793 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I1001 09:59:18.534020 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I1001 09:59:18.539280 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I1001 09:59:18.544534 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I1001 09:59:18.549668 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I1001 09:59:18.554924 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I1001 09:59:18.560243 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I1001 09:59:18.566251 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I1001 09:59:18.571515 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I1001 09:59:18.576797 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I1001 09:59:18.582002 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I1001 09:59:18.586004 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I1001 09:59:18.589908 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I1001 09:59:18.595158 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I1001 09:59:18.600398 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I1001 09:59:18.605722 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I1001 09:59:18.610878 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I1001 09:59:18.616158 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I1001 09:59:18.621471 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I1001 09:59:18.626613 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I1001 09:59:18.631869 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I1001 09:59:18.637083 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I1001 09:59:18.642393 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I1001 09:59:18.647618 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I1001 09:59:18.652942 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I1001 09:59:18.658273 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I1001 09:59:18.663536 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I1001 09:59:18.668760 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I1001 09:59:18.674023 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I1001 09:59:18.679873 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I1001 09:59:18.685101 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I1001 09:59:18.690335 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I1001 09:59:18.695496 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I1001 09:59:18.700779 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I1001 09:59:18.706060 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I1001 09:59:18.711366 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I1001 09:59:18.716523 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I1001 09:59:18.721761 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I1001 09:59:18.726915 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I1001 09:59:18.732161 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I1001 09:59:18.737285 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I1001 09:59:18.742520 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I1001 09:59:18.747775 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I1001 09:59:18.752936 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I1001 09:59:18.758206 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I1001 09:59:18.762335 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I1001 09:59:18.766237 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I1001 09:59:18.771514 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I1001 09:59:18.776778 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I1001 09:59:18.782095 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I1001 09:59:18.787359 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I1001 09:59:18.792482 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I1001 09:59:18.798316 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I1001 09:59:18.803452 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I1001 09:59:18.808707 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I1001 09:59:18.813832 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I1001 09:59:18.819133 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I1001 09:59:18.824286 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I1001 09:59:18.829540 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I1001 09:59:18.834810 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I1001 09:59:18.840095 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I1001 09:59:18.845396 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I1001 09:59:18.850647 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I1001 09:59:18.856043 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I1001 09:59:18.861402 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I1001 09:59:18.866833 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I1001 09:59:18.872094 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I1001 09:59:18.877417 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I1001 09:59:18.882559 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I1001 09:59:18.888066 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I1001 09:59:18.893292 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I1001 09:59:18.898518 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I1001 09:59:18.903753 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I1001 09:59:18.908880 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I1001 09:59:18.914651 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I1001 09:59:18.919809 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I1001 09:59:18.925059 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I1001 09:59:18.930194 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I1001 09:59:18.935417 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I1001 09:59:18.939407 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I1001 09:59:18.943264 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I1001 09:59:18.948408 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I1001 09:59:18.953646 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I1001 09:59:18.958785 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I1001 09:59:18.964011 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I1001 09:59:18.969164 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I1001 09:59:18.974369 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I1001 09:59:18.979520 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I1001 09:59:18.984802 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I1001 09:59:18.990165 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I1001 09:59:18.995577 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I1001 09:59:19.001035 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I1001 09:59:19.006632 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I1001 09:59:19.011991 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I1001 09:59:19.017209 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I1001 09:59:19.022509 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I1001 09:59:19.027710 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I1001 09:59:19.033676 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I1001 09:59:19.039294 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I1001 09:59:19.045221 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I1001 09:59:19.051214 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I1001 09:59:19.057099 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I1001 09:59:19.062303 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I1001 09:59:19.067574 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I1001 09:59:19.072766 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I1001 09:59:19.078026 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I1001 09:59:19.083360 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I1001 09:59:19.088556 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I1001 09:59:19.093862 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I1001 09:59:19.099045 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I1001 09:59:19.104450 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I1001 09:59:19.109989 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I1001 09:59:19.115618 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I1001 09:59:19.119841 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I1001 09:59:19.123777 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I1001 09:59:19.129060 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I1001 09:59:19.134407 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I1001 09:59:19.139622 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I1001 09:59:19.145012 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I1001 09:59:19.150356 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I1001 09:59:19.156491 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I1001 09:59:19.162084 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I1001 09:59:19.167900 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I1001 09:59:19.173417 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I1001 09:59:19.179149 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I1001 09:59:19.184391 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I1001 09:59:19.189663 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I1001 09:59:19.194921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I1001 09:59:19.200157 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I1001 09:59:19.205426 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I1001 09:59:19.210578 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I1001 09:59:19.215819 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I1001 09:59:19.221056 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I1001 09:59:19.226293 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I1001 09:59:19.231423 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I1001 09:59:19.236732 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I1001 09:59:19.241902 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I1001 09:59:19.247144 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I1001 09:59:19.252290 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I1001 09:59:19.257507 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I1001 09:59:19.262753 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I1001 09:59:19.267934 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I1001 09:59:19.273775 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I1001 09:59:19.278936 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I1001 09:59:19.284222 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I1001 09:59:19.289360 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I1001 09:59:19.294626 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I1001 09:59:19.298702 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I1001 09:59:19.302680 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I1001 09:59:19.307979 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I1001 09:59:19.313326 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I1001 09:59:19.318974 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I1001 09:59:19.324615 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I1001 09:59:19.329805 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I1001 09:59:19.335152 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I1001 09:59:19.340537 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I1001 09:59:19.345884 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I1001 09:59:19.351264 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I1001 09:59:19.356818 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I1001 09:59:19.362490 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I1001 09:59:19.367921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I1001 09:59:19.373243 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I1001 09:59:19.378475 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I1001 09:59:19.383845 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I1001 09:59:19.389184 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I1001 09:59:19.395449 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I1001 09:59:19.401308 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I1001 09:59:19.406815 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I1001 09:59:19.412539 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I1001 09:59:19.418567 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I1001 09:59:19.424555 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I1001 09:59:19.430329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I1001 09:59:19.435965 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I1001 09:59:19.441533 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I1001 09:59:19.447255 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I1001 09:59:19.452506 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I1001 09:59:19.457824 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I1001 09:59:19.463254 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I1001 09:59:19.468506 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I1001 09:59:19.473687 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I1001 09:59:19.479008 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I1001 09:59:19.484379 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I1001 09:59:19.489714 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I1001 09:59:19.494942 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I1001 09:59:19.500428 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I1001 09:59:19.505625 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I1001 09:59:19.510910 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I1001 09:59:19.517179 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I1001 09:59:19.523130 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I1001 09:59:19.528691 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I1001 09:59:19.533976 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I1001 09:59:19.540027 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I1001 09:59:19.545641 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I1001 09:59:19.550890 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I1001 09:59:19.556103 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I1001 09:59:19.561388 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I1001 09:59:19.566539 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I1001 09:59:19.571816 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I1001 09:59:19.576960 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I1001 09:59:19.582239 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I1001 09:59:19.587523 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I1001 09:59:19.592709 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I1001 09:59:19.598077 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I1001 09:59:19.603441 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I1001 09:59:19.608811 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I1001 09:59:19.614062 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I1001 09:59:19.619440 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I1001 09:59:19.624684 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I1001 09:59:19.630030 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I1001 09:59:19.635303 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I1001 09:59:19.641165 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I1001 09:59:19.646425 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I1001 09:59:19.651629 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I1001 09:59:19.656910 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I1001 09:59:19.662071 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I1001 09:59:19.667330 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I1001 09:59:19.672512 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I1001 09:59:19.677815 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I1001 09:59:19.682989 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I1001 09:59:19.688253 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I1001 09:59:19.693396 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I1001 09:59:19.698644 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I1001 09:59:19.703921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I1001 09:59:19.709134 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I1001 09:59:19.714414 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I1001 09:59:19.719599 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I1001 09:59:19.724868 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I1001 09:59:19.730019 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I1001 09:59:19.735782 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I1001 09:59:19.741816 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I1001 09:59:19.747863 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I1001 09:59:19.753504 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I1001 09:59:19.760009 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I1001 09:59:20.500162 140265095153472 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I1001 09:59:20.502517 140265095153472 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:23.188706 140265095153472 py_utils.py:1474] MODEL ANALYSIS: 
I1001 09:59:23.188916 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.emb.src_token_emb.wm                               (32000, 2048)          65536000 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var
I1001 09:59:23.188982 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.189037 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.189087 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.189134 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.189182 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.189228 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.189275 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.189335 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.189383 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.189430 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.189476 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.189522 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.189568 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.189614 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.189660 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.189706 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.189753 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.189798 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.189844 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.189890 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.189935 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.189981 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.190026 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.190078 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.190125 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.190171 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.190217 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.190263 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.190310 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.190356 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.190402 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.190447 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.190499 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.190547 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.190593 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.190639 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.190685 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.190730 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.190780 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.190827 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.190872 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.190918 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.190963 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.191009 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.191055 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.191122 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.191174 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.191221 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.191267 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.191312 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.191357 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.191403 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.191448 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.191498 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.191545 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.191591 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.191640 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.191686 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.191734 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.191783 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.191831 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.191878 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.191923 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.191968 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.192013 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.192058 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.192103 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.192148 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.192194 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.192245 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.192292 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.192337 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.192383 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.192428 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.192473 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.192518 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.192563 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.192609 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.192654 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.192699 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.192745 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.192791 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.192836 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.192881 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.192932 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.192978 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.193023 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.193068 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.193114 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.193158 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.193203 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.193248 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.193294 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.193339 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.193384 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.193429 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.193474 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.193520 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.193565 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.193617 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.193664 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.193709 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.193754 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.193799 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.193845 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.193890 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.193935 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.193980 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.194024 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.194069 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.194113 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.194159 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.194204 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.194248 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.194293 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.194343 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.194389 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.194435 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.194480 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.194525 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.194570 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.194615 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.194660 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.194705 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.194750 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.194794 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.194839 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.194884 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.194930 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.194975 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.195025 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.195071 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.195135 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.195184 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.195229 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.195274 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.195320 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.195364 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.195409 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.195455 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.195500 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.195546 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.195590 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.195636 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.195680 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.195725 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.195776 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.195822 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.195868 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.195913 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.195959 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.196008 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.196055 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.196101 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.196146 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.196191 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.196236 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.196281 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.196327 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.196372 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.196417 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.196468 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.196515 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.196561 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.196606 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.196651 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.196697 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.196743 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.196788 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.196834 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.196880 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.196925 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.196970 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.197016 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.197061 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.197106 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.197152 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.197202 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.197248 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.197293 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.197338 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.197384 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.197430 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.197475 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.197521 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.197566 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.197612 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.197657 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.197702 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.197748 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.197794 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.197839 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.197889 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.197935 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.197981 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.198027 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.198073 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.198119 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.198165 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.198211 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.198258 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.198304 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.198350 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.198396 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.198441 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.198487 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.198532 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.198581 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.198627 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.198672 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.198718 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.198763 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.198808 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.198854 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.198899 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.198945 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.198991 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.199036 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.199081 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.199146 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.199194 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.199239 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.199284 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.199337 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.199383 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.199428 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.199473 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.199519 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.199564 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.199609 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.199655 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.199700 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.199745 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.199790 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.199836 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.199881 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.199926 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.199972 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.200023 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.200069 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.200115 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.200160 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.200205 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.200251 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.200296 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.200341 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.200387 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.200433 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.200478 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.200523 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.200568 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.200613 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.200659 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.200709 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.200755 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.200801 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.200846 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.200891 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.200937 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.200982 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.201027 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.201071 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.201117 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.201162 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.201207 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.201253 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.201298 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.201343 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.201388 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.201438 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.201484 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.201530 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.201576 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.201621 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.201666 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.201711 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.201756 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.201801 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.201845 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.201890 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.201935 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.201980 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.202025 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.202070 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.202119 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.202165 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.202211 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.202256 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.202301 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.202346 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.202391 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.202436 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.202482 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.202527 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.202571 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.202616 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.202662 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.202707 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.202752 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.202797 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.202847 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.202892 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.202937 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.202983 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.203028 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.203073 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.203142 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.203190 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.203236 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.203282 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.203327 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.203373 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.203418 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.203463 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.203508 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.203558 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.203604 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.203649 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.203695 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.203740 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.203785 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.203830 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.203875 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.203921 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.203965 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.204010 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.204055 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.204100 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.204145 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.204190 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.204239 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.204285 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.204331 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.204376 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.204421 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.204467 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.204511 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.204556 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.204602 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.204647 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.204692 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.204736 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.204781 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.204826 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.204871 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.204917 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.204967 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.205014 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.205060 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.205106 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.205151 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.205197 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.205242 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.205288 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.205333 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.205379 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.205424 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.205469 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.205514 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.205559 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.205605 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.205654 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.205700 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.205745 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.205790 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.205836 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.205881 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.205927 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.205972 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.206018 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.206066 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.206113 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.206158 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.206203 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.206249 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.206295 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.206344 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.206391 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.206435 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.206480 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.206525 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.206570 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.206615 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.206659 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.206705 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.206749 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.206794 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.206839 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.206884 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.206929 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.206975 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.207020 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.207070 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.207132 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.207180 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.207225 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.207270 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.207315 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.207360 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.207405 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.207450 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.207495 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.207540 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.207586 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.207631 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.207677 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.207722 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.207772 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.207818 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.207864 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.207909 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.207954 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.207999 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.208044 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.208089 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.208134 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.208180 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.208225 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.208271 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.208317 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.208363 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.208408 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.208458 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.208505 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.208550 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.208596 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.208642 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.208687 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.208732 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.208777 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.208822 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.208868 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.208913 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.208959 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.209004 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.209049 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.209095 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.209140 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.209190 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.209237 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.209283 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.209328 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.209374 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.209419 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.209464 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.209509 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.209554 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.209599 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.209643 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.209687 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.209733 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.209777 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.209822 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.209873 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.209919 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.209964 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.210009 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.210054 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.210099 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.210144 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.210188 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.210232 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.210277 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.210322 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.210366 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.210411 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.210456 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.210501 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.210546 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.210596 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.210642 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.210687 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.210732 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.210777 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.210822 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.210867 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.210912 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.210958 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.211003 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.211048 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.211106 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.211158 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.211203 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.211249 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.211300 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.211346 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.211391 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.211437 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.211482 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.211528 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.211573 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.211618 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.211663 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.211708 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.211753 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.211797 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.211843 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.211889 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.211934 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.211983 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.212029 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.212075 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.212120 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.212166 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.212212 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.212257 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.212303 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.212348 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.212394 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.212439 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.212485 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.212530 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.212575 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.212621 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.212666 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.212716 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.212762 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.212808 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.212854 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.212900 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.212945 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.212991 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.213036 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.213081 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.213126 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.213171 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.213217 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.213263 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var
I1001 09:59:23.213308 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var
I1001 09:59:23.213353 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var
I1001 09:59:23.213403 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var
I1001 09:59:23.213449 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var
I1001 09:59:23.213495 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var
I1001 09:59:23.213540 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I1001 09:59:23.213585 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var
I1001 09:59:23.213631 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I1001 09:59:23.213677 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var
I1001 09:59:23.213722 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var
I1001 09:59:23.213768 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var
I1001 09:59:23.213814 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var
I1001 09:59:23.213859 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var
I1001 09:59:23.213905 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var
I1001 09:59:23.213950 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var
I1001 09:59:23.213996 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var
I1001 09:59:23.214041 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_0                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var
I1001 09:59:23.214093 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_1                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var
I1001 09:59:23.214139 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_10                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var
I1001 09:59:23.214185 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_11                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var
I1001 09:59:23.214230 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_12                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var
I1001 09:59:23.214275 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_13                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var
I1001 09:59:23.214320 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_14                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var
I1001 09:59:23.214365 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_15                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var
I1001 09:59:23.214409 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_2                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var
I1001 09:59:23.214454 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_3                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var
I1001 09:59:23.214499 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_4                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var
I1001 09:59:23.214545 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_5                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var
I1001 09:59:23.214589 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_6                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var
I1001 09:59:23.214634 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_7                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var
I1001 09:59:23.214679 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_8                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var
I1001 09:59:23.214724 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_9                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var
I1001 09:59:23.214769 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_0                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var
I1001 09:59:23.214813 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_1                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var
I1001 09:59:23.214858 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_10                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var
I1001 09:59:23.214906 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_11                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var
I1001 09:59:23.214952 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_12                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var
I1001 09:59:23.214997 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_13                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var
I1001 09:59:23.215041 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_14                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var
I1001 09:59:23.215085 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_15                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var
I1001 09:59:23.215153 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_2                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var
I1001 09:59:23.215200 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_3                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var
I1001 09:59:23.215245 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_4                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var
I1001 09:59:23.215291 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_5                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var
I1001 09:59:23.215337 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_6                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var
I1001 09:59:23.215381 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_7                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var
I1001 09:59:23.215427 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_8                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var
I1001 09:59:23.215472 140265095153472 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_9                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var
I1001 09:59:23.215517 140265095153472 py_utils.py:1474] MODEL ANALYSIS: ====================================================================================================
I1001 09:59:23.215563 140265095153472 py_utils.py:1474] MODEL ANALYSIS: total #params: 1742572800
I1001 09:59:23.215610 140265095153472 py_utils.py:1474] MODEL ANALYSIS: 
I1001 09:59:43.185965 140265095153472 trainer.py:1515] Job trainer_client start
I1001 09:59:43.199674 140265095153472 base_runner.py:57] ============================================================
I1001 09:59:43.205919 140265095153472 base_runner.py:59] allow_implicit_capture : NoneType
I1001 09:59:43.206081 140265095153472 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I1001 09:59:43.206146 140265095153472 base_runner.py:59] cluster.add_summary : NoneType
I1001 09:59:43.206202 140265095153472 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I1001 09:59:43.206254 140265095153472 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I1001 09:59:43.206307 140265095153472 base_runner.py:59] cluster.controller.devices_per_split : 1
I1001 09:59:43.206369 140265095153472 base_runner.py:59] cluster.controller.gpus_per_replica : 0
I1001 09:59:43.206421 140265095153472 base_runner.py:59] cluster.controller.name : '/job:local'
I1001 09:59:43.206470 140265095153472 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I1001 09:59:43.206518 140265095153472 base_runner.py:59] cluster.controller.replicas : 1
I1001 09:59:43.206567 140265095153472 base_runner.py:59] cluster.controller.targets : ''
I1001 09:59:43.206616 140265095153472 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I1001 09:59:43.206665 140265095153472 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I1001 09:59:43.206714 140265095153472 base_runner.py:59] cluster.decoder.devices_per_split : 1
I1001 09:59:43.206763 140265095153472 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I1001 09:59:43.206811 140265095153472 base_runner.py:59] cluster.decoder.name : '/job:local'
I1001 09:59:43.206860 140265095153472 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I1001 09:59:43.206908 140265095153472 base_runner.py:59] cluster.decoder.replicas : 1
I1001 09:59:43.206957 140265095153472 base_runner.py:59] cluster.decoder.targets : ''
I1001 09:59:43.207005 140265095153472 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I1001 09:59:43.207052 140265095153472 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I1001 09:59:43.207124 140265095153472 base_runner.py:59] cluster.evaler.devices_per_split : 1
I1001 09:59:43.207179 140265095153472 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I1001 09:59:43.207228 140265095153472 base_runner.py:59] cluster.evaler.name : '/job:local'
I1001 09:59:43.207276 140265095153472 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I1001 09:59:43.207331 140265095153472 base_runner.py:59] cluster.evaler.replicas : 1
I1001 09:59:43.207380 140265095153472 base_runner.py:59] cluster.evaler.targets : ''
I1001 09:59:43.207429 140265095153472 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I1001 09:59:43.207484 140265095153472 base_runner.py:59] cluster.input.cpus_per_replica : 1
I1001 09:59:43.207533 140265095153472 base_runner.py:59] cluster.input.devices_per_split : 1
I1001 09:59:43.207582 140265095153472 base_runner.py:59] cluster.input.gpus_per_replica : 0
I1001 09:59:43.207631 140265095153472 base_runner.py:59] cluster.input.name : '/job:local'
I1001 09:59:43.207680 140265095153472 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I1001 09:59:43.207729 140265095153472 base_runner.py:59] cluster.input.replicas : 0
I1001 09:59:43.207778 140265095153472 base_runner.py:59] cluster.input.targets : ''
I1001 09:59:43.207827 140265095153472 base_runner.py:59] cluster.input.tpus_per_replica : 0
I1001 09:59:43.207876 140265095153472 base_runner.py:59] cluster.job : 'trainer_client'
I1001 09:59:43.207925 140265095153472 base_runner.py:59] cluster.logdir : ''
I1001 09:59:43.207974 140265095153472 base_runner.py:59] cluster.mode : 'sync'
I1001 09:59:43.208023 140265095153472 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I1001 09:59:43.208073 140265095153472 base_runner.py:59] cluster.ps.devices_per_split : 1
I1001 09:59:43.208122 140265095153472 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I1001 09:59:43.208172 140265095153472 base_runner.py:59] cluster.ps.name : '/job:local'
I1001 09:59:43.208220 140265095153472 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I1001 09:59:43.208269 140265095153472 base_runner.py:59] cluster.ps.replicas : 1
I1001 09:59:43.208318 140265095153472 base_runner.py:59] cluster.ps.targets : ''
I1001 09:59:43.208367 140265095153472 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I1001 09:59:43.208415 140265095153472 base_runner.py:59] cluster.task : 0
I1001 09:59:43.208464 140265095153472 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I1001 09:59:43.208512 140265095153472 base_runner.py:59] cluster.worker.devices_per_split : 1
I1001 09:59:43.208566 140265095153472 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I1001 09:59:43.208624 140265095153472 base_runner.py:59] cluster.worker.name : '/job:local'
I1001 09:59:43.208675 140265095153472 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I1001 09:59:43.208725 140265095153472 base_runner.py:59] cluster.worker.replicas : 1
I1001 09:59:43.208775 140265095153472 base_runner.py:59] cluster.worker.targets : ''
I1001 09:59:43.208824 140265095153472 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I1001 09:59:43.208873 140265095153472 base_runner.py:59] dtype : float32
I1001 09:59:43.208922 140265095153472 base_runner.py:59] fprop_dtype : NoneType
I1001 09:59:43.208972 140265095153472 base_runner.py:59] inference_driver_name : NoneType
I1001 09:59:43.209021 140265095153472 base_runner.py:59] input.allow_implicit_capture : NoneType
I1001 09:59:43.209070 140265095153472 base_runner.py:59] input.bucket_adjust_every_n : 0
I1001 09:59:43.209120 140265095153472 base_runner.py:59] input.bucket_batch_limit : [32]
I1001 09:59:43.209169 140265095153472 base_runner.py:59] input.bucket_upper_bound : [1024]
I1001 09:59:43.209218 140265095153472 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I1001 09:59:43.209267 140265095153472 base_runner.py:59] input.dtype : float32
I1001 09:59:43.209316 140265095153472 base_runner.py:59] input.file_buffer_size : 10000000
I1001 09:59:43.209363 140265095153472 base_runner.py:59] input.file_datasource : NoneType
I1001 09:59:43.209412 140265095153472 base_runner.py:59] input.file_parallelism : 10
I1001 09:59:43.209460 140265095153472 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I1001 09:59:43.209512 140265095153472 base_runner.py:59] input.file_random_seed : 301
I1001 09:59:43.209560 140265095153472 base_runner.py:59] input.fixed_input_shape : True
I1001 09:59:43.209609 140265095153472 base_runner.py:59] input.flush_every_n : 0
I1001 09:59:43.209658 140265095153472 base_runner.py:59] input.fprop_dtype : NoneType
I1001 09:59:43.209707 140265095153472 base_runner.py:59] input.inference_driver_name : NoneType
I1001 09:59:43.209755 140265095153472 base_runner.py:59] input.is_eval : NoneType
I1001 09:59:43.209804 140265095153472 base_runner.py:59] input.is_inference : NoneType
I1001 09:59:43.209852 140265095153472 base_runner.py:59] input.name : '1bwds_train_set'
I1001 09:59:43.209901 140265095153472 base_runner.py:59] input.num_batcher_threads : 16
I1001 09:59:43.209949 140265095153472 base_runner.py:59] input.num_samples : 0
I1001 09:59:43.209998 140265095153472 base_runner.py:59] input.pad_to_max_seq_length : False
I1001 09:59:43.210046 140265095153472 base_runner.py:59] input.params_init.method : 'xavier'
I1001 09:59:43.210095 140265095153472 base_runner.py:59] input.params_init.scale : 1.000001
I1001 09:59:43.210143 140265095153472 base_runner.py:59] input.params_init.seed : NoneType
I1001 09:59:43.210191 140265095153472 base_runner.py:59] input.random_seed : NoneType
I1001 09:59:43.210240 140265095153472 base_runner.py:59] input.remote.max_inflights_per_target : 32
I1001 09:59:43.210289 140265095153472 base_runner.py:59] input.remote.shardable_batch : False
I1001 09:59:43.210338 140265095153472 base_runner.py:59] input.require_sequential_order : False
I1001 09:59:43.210386 140265095153472 base_runner.py:59] input.skip_lp_regularization : NoneType
I1001 09:59:43.210434 140265095153472 base_runner.py:59] input.source_max_length : NoneType
I1001 09:59:43.210483 140265095153472 base_runner.py:59] input.target_max_length : 1024
I1001 09:59:43.210531 140265095153472 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I1001 09:59:43.210580 140265095153472 base_runner.py:59] input.tokenizer.append_eos : True
I1001 09:59:43.210628 140265095153472 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I1001 09:59:43.210678 140265095153472 base_runner.py:59] input.tokenizer.dtype : float32
I1001 09:59:43.210726 140265095153472 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I1001 09:59:43.210780 140265095153472 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I1001 09:59:43.210829 140265095153472 base_runner.py:59] input.tokenizer.is_eval : NoneType
I1001 09:59:43.210878 140265095153472 base_runner.py:59] input.tokenizer.is_inference : NoneType
I1001 09:59:43.210927 140265095153472 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I1001 09:59:43.210976 140265095153472 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I1001 09:59:43.211024 140265095153472 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I1001 09:59:43.211073 140265095153472 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I1001 09:59:43.211141 140265095153472 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I1001 09:59:43.211191 140265095153472 base_runner.py:59] input.tokenizer.random_seed : NoneType
I1001 09:59:43.211239 140265095153472 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I1001 09:59:43.211288 140265095153472 base_runner.py:59] input.tokenizer.target_eos_id : 2
I1001 09:59:43.211337 140265095153472 base_runner.py:59] input.tokenizer.target_sos_id : 1
I1001 09:59:43.211386 140265095153472 base_runner.py:59] input.tokenizer.target_unk_id : 0
I1001 09:59:43.211434 140265095153472 base_runner.py:59] input.tokenizer.vn.global_vn : False
I1001 09:59:43.211483 140265095153472 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I1001 09:59:43.211532 140265095153472 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I1001 09:59:43.211580 140265095153472 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I1001 09:59:43.211628 140265095153472 base_runner.py:59] input.tokenizer.vocab_size : 32000
I1001 09:59:43.211676 140265095153472 base_runner.py:59] input.tokenizer_dict : {}
I1001 09:59:43.211724 140265095153472 base_runner.py:59] input.tpu_infeed_parallelism : 1
I1001 09:59:43.211773 140265095153472 base_runner.py:59] input.use_chaining : False
I1001 09:59:43.211822 140265095153472 base_runner.py:59] input.use_per_host_infeed : False
I1001 09:59:43.211870 140265095153472 base_runner.py:59] input.use_within_batch_mixing : False
I1001 09:59:43.211918 140265095153472 base_runner.py:59] input.vn.global_vn : False
I1001 09:59:43.211967 140265095153472 base_runner.py:59] input.vn.per_step_vn : False
I1001 09:59:43.212016 140265095153472 base_runner.py:59] input.vn.scale : NoneType
I1001 09:59:43.212065 140265095153472 base_runner.py:59] input.vn.seed : NoneType
I1001 09:59:43.212114 140265095153472 base_runner.py:59] is_eval : NoneType
I1001 09:59:43.212163 140265095153472 base_runner.py:59] is_inference : NoneType
I1001 09:59:43.212212 140265095153472 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:188'
I1001 09:59:43.212262 140265095153472 base_runner.py:59] name : ''
I1001 09:59:43.212311 140265095153472 base_runner.py:59] params_init.method : 'xavier'
I1001 09:59:43.212359 140265095153472 base_runner.py:59] params_init.scale : 1.000001
I1001 09:59:43.212408 140265095153472 base_runner.py:59] params_init.seed : NoneType
I1001 09:59:43.212456 140265095153472 base_runner.py:59] random_seed : NoneType
I1001 09:59:43.212504 140265095153472 base_runner.py:59] skip_lp_regularization : NoneType
I1001 09:59:43.212553 140265095153472 base_runner.py:59] task.allow_implicit_capture : NoneType
I1001 09:59:43.212601 140265095153472 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I1001 09:59:43.212650 140265095153472 base_runner.py:59] task.decoder : NoneType
I1001 09:59:43.212698 140265095153472 base_runner.py:59] task.dtype : float32
I1001 09:59:43.212747 140265095153472 base_runner.py:59] task.encoder : NoneType
I1001 09:59:43.212795 140265095153472 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I1001 09:59:43.212844 140265095153472 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I1001 09:59:43.212892 140265095153472 base_runner.py:59] task.eval.samples_per_summary : 0
I1001 09:59:43.212946 140265095153472 base_runner.py:59] task.eval.start_decoder_after : 0
I1001 09:59:43.212996 140265095153472 base_runner.py:59] task.eval.start_eval_after : 0
I1001 09:59:43.213044 140265095153472 base_runner.py:59] task.fprop_dtype : NoneType
I1001 09:59:43.213093 140265095153472 base_runner.py:59] task.inference_driver_name : NoneType
I1001 09:59:43.213141 140265095153472 base_runner.py:59] task.input : NoneType
I1001 09:59:43.213190 140265095153472 base_runner.py:59] task.is_eval : NoneType
I1001 09:59:43.213238 140265095153472 base_runner.py:59] task.is_inference : NoneType
I1001 09:59:43.213287 140265095153472 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I1001 09:59:43.213335 140265095153472 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I1001 09:59:43.213384 140265095153472 base_runner.py:59] task.lm.dtype : float32
I1001 09:59:43.213433 140265095153472 base_runner.py:59] task.lm.fprop_dtype : NoneType
I1001 09:59:43.213481 140265095153472 base_runner.py:59] task.lm.inference_driver_name : NoneType
I1001 09:59:43.213529 140265095153472 base_runner.py:59] task.lm.is_eval : NoneType
I1001 09:59:43.213577 140265095153472 base_runner.py:59] task.lm.is_inference : NoneType
I1001 09:59:43.213625 140265095153472 base_runner.py:59] task.lm.name : 'transformerlm'
I1001 09:59:43.213672 140265095153472 base_runner.py:59] task.lm.params_init.method : 'xavier'
I1001 09:59:43.213721 140265095153472 base_runner.py:59] task.lm.params_init.scale : 1.000001
I1001 09:59:43.213769 140265095153472 base_runner.py:59] task.lm.params_init.seed : NoneType
I1001 09:59:43.213817 140265095153472 base_runner.py:59] task.lm.random_seed : NoneType
I1001 09:59:43.213865 140265095153472 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I1001 09:59:43.213913 140265095153472 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I1001 09:59:43.213961 140265095153472 base_runner.py:59] task.lm.stack.batch_dim : 1
I1001 09:59:43.214009 140265095153472 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I1001 09:59:43.214058 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.214106 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1001 09:59:43.214154 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I1001 09:59:43.214203 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I1001 09:59:43.214251 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I1001 09:59:43.214299 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I1001 09:59:43.214347 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I1001 09:59:43.214396 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I1001 09:59:43.214444 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I1001 09:59:43.214493 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I1001 09:59:43.214541 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I1001 09:59:43.214590 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.214638 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:59:43.214686 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I1001 09:59:43.214734 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I1001 09:59:43.214782 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:59:43.214830 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:59:43.214883 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I1001 09:59:43.214933 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I1001 09:59:43.214981 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I1001 09:59:43.215030 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I1001 09:59:43.215079 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:59:43.215144 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:59:43.215195 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:59:43.215244 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I1001 09:59:43.215293 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.215342 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I1001 09:59:43.215390 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:59:43.215439 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I1001 09:59:43.215488 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I1001 09:59:43.215537 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I1001 09:59:43.215585 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I1001 09:59:43.215634 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I1001 09:59:43.215682 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I1001 09:59:43.215731 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I1001 09:59:43.215779 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I1001 09:59:43.215827 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I1001 09:59:43.215876 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I1001 09:59:43.215924 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I1001 09:59:43.215973 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.216021 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I1001 09:59:43.216070 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1001 09:59:43.216119 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.216167 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1001 09:59:43.216216 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1001 09:59:43.216265 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.216314 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1001 09:59:43.216362 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1001 09:59:43.216411 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1001 09:59:43.216459 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1001 09:59:43.216508 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1001 09:59:43.216556 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1001 09:59:43.216611 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1001 09:59:43.216661 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I1001 09:59:43.216710 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1001 09:59:43.216759 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1001 09:59:43.216808 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1001 09:59:43.216857 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1001 09:59:43.216906 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1001 09:59:43.216955 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1001 09:59:43.217005 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1001 09:59:43.217054 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1001 09:59:43.217104 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1001 09:59:43.217153 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1001 09:59:43.217202 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1001 09:59:43.217251 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1001 09:59:43.217300 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1001 09:59:43.217349 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1001 09:59:43.217397 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1001 09:59:43.217445 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1001 09:59:43.217494 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1001 09:59:43.217543 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1001 09:59:43.217592 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1001 09:59:43.217640 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1001 09:59:43.217690 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1001 09:59:43.217738 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1001 09:59:43.217787 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1001 09:59:43.217835 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1001 09:59:43.217883 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1001 09:59:43.217932 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1001 09:59:43.217984 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1001 09:59:43.218033 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1001 09:59:43.218082 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1001 09:59:43.218131 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1001 09:59:43.218179 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1001 09:59:43.218228 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1001 09:59:43.218276 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1001 09:59:43.218325 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1001 09:59:43.218374 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1001 09:59:43.218422 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1001 09:59:43.218471 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1001 09:59:43.218520 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1001 09:59:43.218568 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1001 09:59:43.218617 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1001 09:59:43.218665 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1001 09:59:43.218713 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1001 09:59:43.218763 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1001 09:59:43.218812 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1001 09:59:43.218861 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1001 09:59:43.218910 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.218959 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1001 09:59:43.219008 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1001 09:59:43.219057 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1001 09:59:43.219125 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1001 09:59:43.219180 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1001 09:59:43.219230 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1001 09:59:43.219279 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1001 09:59:43.219329 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I1001 09:59:43.219378 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I1001 09:59:43.219427 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1001 09:59:43.219476 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1001 09:59:43.219530 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I1001 09:59:43.219581 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I1001 09:59:43.219631 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I1001 09:59:43.219681 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.219730 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:59:43.219787 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1001 09:59:43.219835 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1001 09:59:43.219885 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:59:43.219933 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:59:43.219983 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1001 09:59:43.220032 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1001 09:59:43.220081 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1001 09:59:43.220129 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1001 09:59:43.220178 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:59:43.220227 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:59:43.220275 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:59:43.220324 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1001 09:59:43.220372 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.220420 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1001 09:59:43.220469 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:59:43.220518 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1001 09:59:43.220567 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1001 09:59:43.220616 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I1001 09:59:43.220664 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I1001 09:59:43.220713 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I1001 09:59:43.220762 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I1001 09:59:43.220811 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1001 09:59:43.220860 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1001 09:59:43.220908 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1001 09:59:43.220957 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I1001 09:59:43.221006 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1001 09:59:43.221055 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.221103 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:59:43.221159 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 09:59:43.221210 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1001 09:59:43.221260 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 09:59:43.221308 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 09:59:43.221356 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 09:59:43.221405 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 09:59:43.221454 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 09:59:43.221503 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1001 09:59:43.221551 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 09:59:43.221600 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:59:43.221648 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 09:59:43.221696 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 09:59:43.221745 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 09:59:43.221794 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 09:59:43.221842 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.221891 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 09:59:43.221940 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 09:59:43.221989 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 09:59:43.222037 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 09:59:43.222086 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.222135 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I1001 09:59:43.222183 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I1001 09:59:43.222232 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1001 09:59:43.222281 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1001 09:59:43.222329 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1001 09:59:43.222378 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I1001 09:59:43.222429 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1001 09:59:43.222487 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.222544 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1001 09:59:43.222601 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I1001 09:59:43.222656 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1001 09:59:43.222715 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.222769 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1001 09:59:43.222821 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1001 09:59:43.222874 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1001 09:59:43.222926 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1001 09:59:43.222975 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:59:43.223025 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1001 09:59:43.223074 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1001 09:59:43.223148 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1001 09:59:43.223199 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1001 09:59:43.223248 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1001 09:59:43.223296 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1001 09:59:43.223345 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1001 09:59:43.223393 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1001 09:59:43.223441 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1001 09:59:43.223489 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1001 09:59:43.223537 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1001 09:59:43.223585 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1001 09:59:43.223632 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1001 09:59:43.223680 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1001 09:59:43.223727 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1001 09:59:43.223775 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1001 09:59:43.223823 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1001 09:59:43.223872 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1001 09:59:43.223919 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1001 09:59:43.223967 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1001 09:59:43.224015 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1001 09:59:43.224064 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1001 09:59:43.224118 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1001 09:59:43.224167 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1001 09:59:43.224216 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1001 09:59:43.224264 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1001 09:59:43.224312 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1001 09:59:43.224361 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1001 09:59:43.224409 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1001 09:59:43.224457 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1001 09:59:43.224506 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1001 09:59:43.224555 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1001 09:59:43.224603 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1001 09:59:43.224651 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1001 09:59:43.224699 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1001 09:59:43.224747 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1001 09:59:43.224800 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1001 09:59:43.224849 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1001 09:59:43.224896 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1001 09:59:43.224944 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1001 09:59:43.224992 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1001 09:59:43.225040 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1001 09:59:43.225088 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1001 09:59:43.225136 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1001 09:59:43.225184 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1001 09:59:43.225232 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1001 09:59:43.225280 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1001 09:59:43.225328 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1001 09:59:43.225376 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1001 09:59:43.225424 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1001 09:59:43.225471 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1001 09:59:43.225523 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1001 09:59:43.225572 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1001 09:59:43.225620 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1001 09:59:43.225667 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1001 09:59:43.225715 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1001 09:59:43.225764 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1001 09:59:43.225811 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1001 09:59:43.225859 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1001 09:59:43.225908 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.225955 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1001 09:59:43.226004 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1001 09:59:43.226051 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1001 09:59:43.226100 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1001 09:59:43.226148 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1001 09:59:43.226197 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1001 09:59:43.226245 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I1001 09:59:43.226294 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1001 09:59:43.226342 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I1001 09:59:43.226390 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1001 09:59:43.226438 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1001 09:59:43.226487 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.226536 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:59:43.226584 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1001 09:59:43.226633 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1001 09:59:43.226680 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:59:43.226729 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:59:43.226778 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1001 09:59:43.226826 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1001 09:59:43.226875 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1001 09:59:43.226923 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1001 09:59:43.226972 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:59:43.227020 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:59:43.227073 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:59:43.227139 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1001 09:59:43.227190 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.227238 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1001 09:59:43.227287 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:59:43.227335 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1001 09:59:43.227384 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1001 09:59:43.227433 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I1001 09:59:43.227481 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I1001 09:59:43.227530 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1001 09:59:43.227578 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1001 09:59:43.227626 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1001 09:59:43.227675 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1001 09:59:43.227724 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1001 09:59:43.227772 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1001 09:59:43.227821 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1001 09:59:43.227870 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.227919 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1001 09:59:43.227967 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1001 09:59:43.228016 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1001 09:59:43.228065 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1001 09:59:43.228114 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1001 09:59:43.228162 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1001 09:59:43.228211 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1001 09:59:43.228259 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1001 09:59:43.228308 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1001 09:59:43.228356 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1001 09:59:43.228404 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1001 09:59:43.228453 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1001 09:59:43.228507 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1001 09:59:43.228555 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1001 09:59:43.228609 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1001 09:59:43.228658 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1001 09:59:43.228707 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1001 09:59:43.228755 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1001 09:59:43.228804 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.228853 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1001 09:59:43.228901 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1001 09:59:43.228950 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1001 09:59:43.228998 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1001 09:59:43.229047 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1001 09:59:43.229095 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1001 09:59:43.229144 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.229192 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:59:43.229240 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 09:59:43.229289 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1001 09:59:43.229338 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 09:59:43.229387 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 09:59:43.229435 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 09:59:43.229483 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 09:59:43.229532 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 09:59:43.229580 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1001 09:59:43.229629 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 09:59:43.229677 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:59:43.229725 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 09:59:43.229773 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 09:59:43.229821 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 09:59:43.229869 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 09:59:43.229922 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.229969 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 09:59:43.230026 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 09:59:43.230075 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 09:59:43.230123 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 09:59:43.230172 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.230221 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1001 09:59:43.230270 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1001 09:59:43.230319 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1001 09:59:43.230369 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1001 09:59:43.230418 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I1001 09:59:43.230467 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I1001 09:59:43.230515 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I1001 09:59:43.230564 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I1001 09:59:43.230612 140265095153472 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I1001 09:59:43.230661 140265095153472 base_runner.py:59] task.lm.stack.dtype : float32
I1001 09:59:43.230710 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I1001 09:59:43.230759 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.230807 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I1001 09:59:43.230856 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I1001 09:59:43.230904 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I1001 09:59:43.230953 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.231002 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:59:43.231050 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I1001 09:59:43.231113 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I1001 09:59:43.231168 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I1001 09:59:43.231222 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I1001 09:59:43.231271 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I1001 09:59:43.231320 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I1001 09:59:43.231369 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I1001 09:59:43.231418 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I1001 09:59:43.231467 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I1001 09:59:43.231515 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:59:43.231563 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I1001 09:59:43.231611 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I1001 09:59:43.231659 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I1001 09:59:43.231708 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I1001 09:59:43.231761 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.231812 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I1001 09:59:43.231860 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I1001 09:59:43.231909 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I1001 09:59:43.231957 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I1001 09:59:43.232005 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I1001 09:59:43.232054 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I1001 09:59:43.232102 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I1001 09:59:43.232150 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I1001 09:59:43.232198 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I1001 09:59:43.232247 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I1001 09:59:43.232295 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I1001 09:59:43.232343 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I1001 09:59:43.232391 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I1001 09:59:43.232439 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I1001 09:59:43.232487 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I1001 09:59:43.232535 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I1001 09:59:43.232583 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I1001 09:59:43.232631 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I1001 09:59:43.232679 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I1001 09:59:43.232728 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I1001 09:59:43.232778 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I1001 09:59:43.232827 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I1001 09:59:43.232875 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I1001 09:59:43.232924 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I1001 09:59:43.232972 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I1001 09:59:43.233021 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I1001 09:59:43.233069 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I1001 09:59:43.233118 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I1001 09:59:43.233167 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I1001 09:59:43.233216 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I1001 09:59:43.233265 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I1001 09:59:43.233314 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I1001 09:59:43.233362 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I1001 09:59:43.233411 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I1001 09:59:43.233460 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I1001 09:59:43.233508 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I1001 09:59:43.233561 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I1001 09:59:43.233611 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I1001 09:59:43.233660 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I1001 09:59:43.233708 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I1001 09:59:43.233756 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I1001 09:59:43.233804 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.233853 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I1001 09:59:43.233901 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I1001 09:59:43.233950 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I1001 09:59:43.233998 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I1001 09:59:43.234046 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I1001 09:59:43.234095 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I1001 09:59:43.234143 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I1001 09:59:43.234191 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I1001 09:59:43.234240 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I1001 09:59:43.234288 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I1001 09:59:43.234336 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I1001 09:59:43.234385 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I1001 09:59:43.234433 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I1001 09:59:43.234481 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I1001 09:59:43.234530 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I1001 09:59:43.234578 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I1001 09:59:43.234626 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I1001 09:59:43.234674 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I1001 09:59:43.234723 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I1001 09:59:43.234772 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I1001 09:59:43.234820 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I1001 09:59:43.234869 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I1001 09:59:43.234917 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I1001 09:59:43.234966 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I1001 09:59:43.235014 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I1001 09:59:43.235063 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I1001 09:59:43.235127 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I1001 09:59:43.235180 140265095153472 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I1001 09:59:43.235230 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.235279 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I1001 09:59:43.235333 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I1001 09:59:43.235384 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I1001 09:59:43.235433 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I1001 09:59:43.235482 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I1001 09:59:43.235532 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I1001 09:59:43.235581 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I1001 09:59:43.235631 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I1001 09:59:43.235680 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I1001 09:59:43.235729 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I1001 09:59:43.235778 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.235828 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:59:43.235877 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I1001 09:59:43.235927 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I1001 09:59:43.235976 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:59:43.236025 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:59:43.236074 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I1001 09:59:43.236123 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I1001 09:59:43.236172 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I1001 09:59:43.236221 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I1001 09:59:43.236270 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:59:43.236320 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:59:43.236368 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:59:43.236418 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I1001 09:59:43.236466 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.236515 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I1001 09:59:43.236564 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:59:43.236613 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I1001 09:59:43.236662 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I1001 09:59:43.236712 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I1001 09:59:43.236761 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I1001 09:59:43.236810 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I1001 09:59:43.236859 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I1001 09:59:43.236908 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I1001 09:59:43.236957 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I1001 09:59:43.237006 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I1001 09:59:43.237054 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I1001 09:59:43.237102 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I1001 09:59:43.237156 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.237206 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I1001 09:59:43.237255 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I1001 09:59:43.237303 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.237352 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I1001 09:59:43.237401 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I1001 09:59:43.237451 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.237499 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I1001 09:59:43.237547 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I1001 09:59:43.237596 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I1001 09:59:43.237645 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I1001 09:59:43.237694 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I1001 09:59:43.237743 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I1001 09:59:43.237792 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I1001 09:59:43.237842 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I1001 09:59:43.237891 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I1001 09:59:43.237940 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I1001 09:59:43.237989 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I1001 09:59:43.238038 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I1001 09:59:43.238086 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I1001 09:59:43.238135 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I1001 09:59:43.238183 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I1001 09:59:43.238232 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I1001 09:59:43.238281 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I1001 09:59:43.238330 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I1001 09:59:43.238379 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I1001 09:59:43.238427 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I1001 09:59:43.238476 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I1001 09:59:43.238525 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I1001 09:59:43.238575 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I1001 09:59:43.238634 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I1001 09:59:43.238685 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I1001 09:59:43.238735 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I1001 09:59:43.238784 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I1001 09:59:43.238833 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I1001 09:59:43.238882 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I1001 09:59:43.238931 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I1001 09:59:43.238980 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I1001 09:59:43.239029 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I1001 09:59:43.239078 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I1001 09:59:43.239144 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I1001 09:59:43.239195 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I1001 09:59:43.239243 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I1001 09:59:43.239292 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I1001 09:59:43.239341 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I1001 09:59:43.239389 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I1001 09:59:43.239438 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I1001 09:59:43.239486 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I1001 09:59:43.239535 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I1001 09:59:43.239583 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I1001 09:59:43.239632 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I1001 09:59:43.239681 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I1001 09:59:43.239730 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I1001 09:59:43.239779 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I1001 09:59:43.239828 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I1001 09:59:43.239877 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I1001 09:59:43.239926 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I1001 09:59:43.239975 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I1001 09:59:43.240024 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I1001 09:59:43.240073 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I1001 09:59:43.240126 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.240176 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I1001 09:59:43.240226 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I1001 09:59:43.240275 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I1001 09:59:43.240324 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I1001 09:59:43.240374 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I1001 09:59:43.240422 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I1001 09:59:43.240471 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I1001 09:59:43.240520 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I1001 09:59:43.240568 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I1001 09:59:43.240617 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I1001 09:59:43.240666 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I1001 09:59:43.240715 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I1001 09:59:43.240763 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I1001 09:59:43.240812 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I1001 09:59:43.240860 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.240909 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:59:43.240958 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I1001 09:59:43.241006 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I1001 09:59:43.241055 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:59:43.241103 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:59:43.241152 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I1001 09:59:43.241201 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I1001 09:59:43.241252 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I1001 09:59:43.241302 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I1001 09:59:43.241352 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:59:43.241400 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:59:43.241450 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:59:43.241498 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I1001 09:59:43.241546 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.241594 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I1001 09:59:43.241644 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:59:43.241708 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I1001 09:59:43.241764 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I1001 09:59:43.241819 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I1001 09:59:43.241873 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I1001 09:59:43.241926 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I1001 09:59:43.241978 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I1001 09:59:43.242030 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I1001 09:59:43.242081 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I1001 09:59:43.242130 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I1001 09:59:43.242179 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I1001 09:59:43.242228 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I1001 09:59:43.242276 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.242324 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:59:43.242373 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 09:59:43.242421 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I1001 09:59:43.242470 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 09:59:43.242518 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 09:59:43.242567 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 09:59:43.242615 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 09:59:43.242664 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 09:59:43.242712 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I1001 09:59:43.242761 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 09:59:43.242809 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:59:43.242857 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 09:59:43.242906 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 09:59:43.242954 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 09:59:43.243003 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 09:59:43.243052 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.243121 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 09:59:43.243177 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 09:59:43.243232 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 09:59:43.243282 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 09:59:43.243331 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.243380 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I1001 09:59:43.243429 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I1001 09:59:43.243478 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I1001 09:59:43.243527 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I1001 09:59:43.243575 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I1001 09:59:43.243624 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I1001 09:59:43.243673 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I1001 09:59:43.243721 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.243771 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I1001 09:59:43.243821 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I1001 09:59:43.243870 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I1001 09:59:43.243918 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.243967 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I1001 09:59:43.244016 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I1001 09:59:43.244065 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I1001 09:59:43.244113 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I1001 09:59:43.244162 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:59:43.244210 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I1001 09:59:43.244259 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I1001 09:59:43.244308 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I1001 09:59:43.244356 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I1001 09:59:43.244404 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I1001 09:59:43.244453 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I1001 09:59:43.244501 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I1001 09:59:43.244549 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I1001 09:59:43.244598 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I1001 09:59:43.244647 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I1001 09:59:43.244695 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I1001 09:59:43.244749 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I1001 09:59:43.244798 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I1001 09:59:43.244847 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I1001 09:59:43.244896 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I1001 09:59:43.244944 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I1001 09:59:43.244993 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I1001 09:59:43.245041 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I1001 09:59:43.245090 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I1001 09:59:43.245138 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I1001 09:59:43.245187 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I1001 09:59:43.245236 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I1001 09:59:43.245285 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I1001 09:59:43.245334 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I1001 09:59:43.245383 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I1001 09:59:43.245433 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I1001 09:59:43.245482 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I1001 09:59:43.245531 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I1001 09:59:43.245580 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I1001 09:59:43.245629 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I1001 09:59:43.245677 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I1001 09:59:43.245726 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I1001 09:59:43.245775 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I1001 09:59:43.245823 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I1001 09:59:43.245872 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I1001 09:59:43.245921 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I1001 09:59:43.245969 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I1001 09:59:43.246017 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I1001 09:59:43.246067 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I1001 09:59:43.246116 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I1001 09:59:43.246169 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I1001 09:59:43.246219 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I1001 09:59:43.246268 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I1001 09:59:43.246318 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I1001 09:59:43.246367 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I1001 09:59:43.246416 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I1001 09:59:43.246464 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I1001 09:59:43.246513 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I1001 09:59:43.246561 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I1001 09:59:43.246610 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I1001 09:59:43.246659 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I1001 09:59:43.246707 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I1001 09:59:43.246756 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I1001 09:59:43.246804 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I1001 09:59:43.246853 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I1001 09:59:43.246901 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I1001 09:59:43.246949 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I1001 09:59:43.246998 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I1001 09:59:43.247046 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I1001 09:59:43.247107 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.247162 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I1001 09:59:43.247211 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I1001 09:59:43.247261 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I1001 09:59:43.247310 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I1001 09:59:43.247359 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I1001 09:59:43.247409 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I1001 09:59:43.247457 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I1001 09:59:43.247507 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I1001 09:59:43.247555 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I1001 09:59:43.247604 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I1001 09:59:43.247658 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I1001 09:59:43.247708 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.247756 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I1001 09:59:43.247805 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I1001 09:59:43.247854 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I1001 09:59:43.247902 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I1001 09:59:43.247951 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I1001 09:59:43.248000 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I1001 09:59:43.248049 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I1001 09:59:43.248097 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I1001 09:59:43.248146 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I1001 09:59:43.248194 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I1001 09:59:43.248243 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I1001 09:59:43.248291 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I1001 09:59:43.248339 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I1001 09:59:43.248387 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.248435 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I1001 09:59:43.248484 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I1001 09:59:43.248532 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I1001 09:59:43.248580 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I1001 09:59:43.248629 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I1001 09:59:43.248678 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I1001 09:59:43.248726 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I1001 09:59:43.248774 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I1001 09:59:43.248823 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I1001 09:59:43.248871 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I1001 09:59:43.248919 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I1001 09:59:43.248968 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I1001 09:59:43.249017 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I1001 09:59:43.249066 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.249115 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I1001 09:59:43.249163 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I1001 09:59:43.249217 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I1001 09:59:43.249266 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I1001 09:59:43.249315 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I1001 09:59:43.249365 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I1001 09:59:43.249413 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I1001 09:59:43.249462 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I1001 09:59:43.249510 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I1001 09:59:43.249558 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I1001 09:59:43.249606 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I1001 09:59:43.249655 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I1001 09:59:43.249703 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I1001 09:59:43.249752 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I1001 09:59:43.249801 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I1001 09:59:43.249850 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I1001 09:59:43.249899 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I1001 09:59:43.249948 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I1001 09:59:43.249996 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.250045 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I1001 09:59:43.250094 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I1001 09:59:43.250143 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I1001 09:59:43.250191 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I1001 09:59:43.250240 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I1001 09:59:43.250289 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I1001 09:59:43.250337 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.250386 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I1001 09:59:43.250435 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I1001 09:59:43.250483 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I1001 09:59:43.250532 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I1001 09:59:43.250581 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I1001 09:59:43.250630 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I1001 09:59:43.250684 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I1001 09:59:43.250735 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I1001 09:59:43.250783 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I1001 09:59:43.250832 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I1001 09:59:43.250881 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:59:43.250929 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I1001 09:59:43.250978 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I1001 09:59:43.251027 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I1001 09:59:43.251075 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I1001 09:59:43.251141 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.251191 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I1001 09:59:43.251240 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I1001 09:59:43.251289 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I1001 09:59:43.251342 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I1001 09:59:43.251391 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.251440 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I1001 09:59:43.251489 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I1001 09:59:43.251538 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I1001 09:59:43.251587 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I1001 09:59:43.251637 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I1001 09:59:43.251686 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I1001 09:59:43.251734 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I1001 09:59:43.251783 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I1001 09:59:43.251831 140265095153472 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I1001 09:59:43.251884 140265095153472 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I1001 09:59:43.251963 140265095153472 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I1001 09:59:43.252046 140265095153472 base_runner.py:59] task.lm.stack.is_eval : NoneType
I1001 09:59:43.252125 140265095153472 base_runner.py:59] task.lm.stack.is_inference : NoneType
I1001 09:59:43.252182 140265095153472 base_runner.py:59] task.lm.stack.is_transparent : False
I1001 09:59:43.252231 140265095153472 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I1001 09:59:43.252280 140265095153472 base_runner.py:59] task.lm.stack.model_dim : 2048
I1001 09:59:43.252330 140265095153472 base_runner.py:59] task.lm.stack.name : ''
I1001 09:59:43.252379 140265095153472 base_runner.py:59] task.lm.stack.normalize_encoder : False
I1001 09:59:43.252428 140265095153472 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I1001 09:59:43.252484 140265095153472 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I1001 09:59:43.252535 140265095153472 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I1001 09:59:43.252584 140265095153472 base_runner.py:59] task.lm.stack.packed_input : False
I1001 09:59:43.252632 140265095153472 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I1001 09:59:43.252681 140265095153472 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I1001 09:59:43.252730 140265095153472 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I1001 09:59:43.252778 140265095153472 base_runner.py:59] task.lm.stack.random_seed : NoneType
I1001 09:59:43.252827 140265095153472 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I1001 09:59:43.252875 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.252923 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I1001 09:59:43.252976 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I1001 09:59:43.253025 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I1001 09:59:43.253073 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I1001 09:59:43.253122 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I1001 09:59:43.253171 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I1001 09:59:43.253220 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I1001 09:59:43.253269 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I1001 09:59:43.253318 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I1001 09:59:43.253367 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I1001 09:59:43.253415 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I1001 09:59:43.253463 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I1001 09:59:43.253512 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I1001 09:59:43.253560 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I1001 09:59:43.253609 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I1001 09:59:43.253657 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I1001 09:59:43.253705 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I1001 09:59:43.253755 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I1001 09:59:43.253803 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I1001 09:59:43.253852 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I1001 09:59:43.253900 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.253948 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I1001 09:59:43.253997 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I1001 09:59:43.254046 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I1001 09:59:43.254095 140265095153472 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I1001 09:59:43.254143 140265095153472 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I1001 09:59:43.254191 140265095153472 base_runner.py:59] task.lm.stack.state_dtype : float32
I1001 09:59:43.254239 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I1001 09:59:43.254288 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.254337 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I1001 09:59:43.254390 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I1001 09:59:43.254440 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I1001 09:59:43.254488 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I1001 09:59:43.254537 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I1001 09:59:43.254585 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I1001 09:59:43.254634 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I1001 09:59:43.254682 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I1001 09:59:43.254730 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I1001 09:59:43.254779 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I1001 09:59:43.254827 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I1001 09:59:43.254876 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I1001 09:59:43.254925 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I1001 09:59:43.254973 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I1001 09:59:43.255023 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I1001 09:59:43.255071 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I1001 09:59:43.255146 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I1001 09:59:43.255198 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.255247 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I1001 09:59:43.255296 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I1001 09:59:43.255346 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I1001 09:59:43.255395 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I1001 09:59:43.255444 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I1001 09:59:43.255493 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I1001 09:59:43.255541 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I1001 09:59:43.255590 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I1001 09:59:43.255639 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I1001 09:59:43.255687 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I1001 09:59:43.255736 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I1001 09:59:43.255784 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I1001 09:59:43.255833 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I1001 09:59:43.255882 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I1001 09:59:43.255931 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I1001 09:59:43.255984 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I1001 09:59:43.256034 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I1001 09:59:43.256083 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I1001 09:59:43.256132 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I1001 09:59:43.256180 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I1001 09:59:43.256229 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I1001 09:59:43.256278 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I1001 09:59:43.256327 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I1001 09:59:43.256375 140265095153472 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I1001 09:59:43.256424 140265095153472 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I1001 09:59:43.256473 140265095153472 base_runner.py:59] task.lm.stack.vn.global_vn : False
I1001 09:59:43.256521 140265095153472 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I1001 09:59:43.256570 140265095153472 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I1001 09:59:43.256619 140265095153472 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I1001 09:59:43.256668 140265095153472 base_runner.py:59] task.lm.vn.global_vn : False
I1001 09:59:43.256717 140265095153472 base_runner.py:59] task.lm.vn.per_step_vn : False
I1001 09:59:43.256766 140265095153472 base_runner.py:59] task.lm.vn.scale : NoneType
I1001 09:59:43.256815 140265095153472 base_runner.py:59] task.lm.vn.seed : NoneType
I1001 09:59:43.256863 140265095153472 base_runner.py:59] task.lm.vocab_size : 32000
I1001 09:59:43.256912 140265095153472 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I1001 09:59:43.256961 140265095153472 base_runner.py:59] task.online_encoder : NoneType
I1001 09:59:43.257009 140265095153472 base_runner.py:59] task.params_init.method : 'xavier'
I1001 09:59:43.257058 140265095153472 base_runner.py:59] task.params_init.scale : 1.000001
I1001 09:59:43.257107 140265095153472 base_runner.py:59] task.params_init.seed : NoneType
I1001 09:59:43.257156 140265095153472 base_runner.py:59] task.random_seed : NoneType
I1001 09:59:43.257205 140265095153472 base_runner.py:59] task.skip_lp_regularization : NoneType
I1001 09:59:43.257253 140265095153472 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I1001 09:59:43.257302 140265095153472 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I1001 09:59:43.257350 140265095153472 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I1001 09:59:43.257398 140265095153472 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I1001 09:59:43.257447 140265095153472 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I1001 09:59:43.257494 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I1001 09:59:43.257542 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I1001 09:59:43.257591 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I1001 09:59:43.257639 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I1001 09:59:43.257687 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I1001 09:59:43.257735 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I1001 09:59:43.257784 140265095153472 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I1001 09:59:43.257832 140265095153472 base_runner.py:59] task.train.early_stop.min_steps : 0
I1001 09:59:43.257885 140265095153472 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I1001 09:59:43.257936 140265095153472 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I1001 09:59:43.257984 140265095153472 base_runner.py:59] task.train.early_stop.verbose : True
I1001 09:59:43.258032 140265095153472 base_runner.py:59] task.train.early_stop.window : 0
I1001 09:59:43.258080 140265095153472 base_runner.py:59] task.train.ema_decay : 0.0
I1001 09:59:43.258128 140265095153472 base_runner.py:59] task.train.enqueue_max_steps : -1
I1001 09:59:43.258176 140265095153472 base_runner.py:59] task.train.gate_gradients : False
I1001 09:59:43.258225 140265095153472 base_runner.py:59] task.train.grad_aggregation_method : 1
I1001 09:59:43.258272 140265095153472 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I1001 09:59:43.258321 140265095153472 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I1001 09:59:43.258369 140265095153472 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I1001 09:59:43.258417 140265095153472 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I1001 09:59:43.258465 140265095153472 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I1001 09:59:43.258512 140265095153472 base_runner.py:59] task.train.learner : NoneType
I1001 09:59:43.258561 140265095153472 base_runner.py:59] task.train.learning_rate : 0.5
I1001 09:59:43.258609 140265095153472 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I1001 09:59:43.258657 140265095153472 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1001 09:59:43.258705 140265095153472 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I1001 09:59:43.258753 140265095153472 base_runner.py:59] task.train.lr_schedule.dtype : float32
I1001 09:59:43.258802 140265095153472 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I1001 09:59:43.258850 140265095153472 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I1001 09:59:43.258899 140265095153472 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I1001 09:59:43.258947 140265095153472 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I1001 09:59:43.258996 140265095153472 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I1001 09:59:43.259044 140265095153472 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I1001 09:59:43.259104 140265095153472 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I1001 09:59:43.259159 140265095153472 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I1001 09:59:43.259208 140265095153472 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I1001 09:59:43.259257 140265095153472 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I1001 09:59:43.259305 140265095153472 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I1001 09:59:43.259353 140265095153472 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I1001 09:59:43.259402 140265095153472 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I1001 09:59:43.259449 140265095153472 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I1001 09:59:43.259497 140265095153472 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I1001 09:59:43.259546 140265095153472 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I1001 09:59:43.259594 140265095153472 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I1001 09:59:43.259642 140265095153472 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I1001 09:59:43.259690 140265095153472 base_runner.py:59] task.train.max_steps : 4000000
I1001 09:59:43.259738 140265095153472 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I1001 09:59:43.259785 140265095153472 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I1001 09:59:43.259834 140265095153472 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I1001 09:59:43.259886 140265095153472 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I1001 09:59:43.259936 140265095153472 base_runner.py:59] task.train.optimizer.dtype : float32
I1001 09:59:43.259983 140265095153472 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I1001 09:59:43.260031 140265095153472 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I1001 09:59:43.260079 140265095153472 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I1001 09:59:43.260126 140265095153472 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I1001 09:59:43.260174 140265095153472 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I1001 09:59:43.260222 140265095153472 base_runner.py:59] task.train.optimizer.name : 'Adam'
I1001 09:59:43.260270 140265095153472 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I1001 09:59:43.260318 140265095153472 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I1001 09:59:43.260366 140265095153472 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I1001 09:59:43.260414 140265095153472 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I1001 09:59:43.260461 140265095153472 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I1001 09:59:43.260509 140265095153472 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I1001 09:59:43.260557 140265095153472 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I1001 09:59:43.260605 140265095153472 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I1001 09:59:43.260653 140265095153472 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I1001 09:59:43.260701 140265095153472 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I1001 09:59:43.260748 140265095153472 base_runner.py:59] task.train.save_interval_seconds : 600
I1001 09:59:43.260796 140265095153472 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I1001 09:59:43.260843 140265095153472 base_runner.py:59] task.train.save_max_to_keep : 100
I1001 09:59:43.260891 140265095153472 base_runner.py:59] task.train.start_up_delay_steps : 200
I1001 09:59:43.260939 140265095153472 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I1001 09:59:43.260987 140265095153472 base_runner.py:59] task.train.summary_interval_steps : 100
I1001 09:59:43.261035 140265095153472 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I1001 09:59:43.261083 140265095153472 base_runner.py:59] task.train.vn_start_step : 20000
I1001 09:59:43.261131 140265095153472 base_runner.py:59] task.train.vn_std : 0.0
I1001 09:59:43.261179 140265095153472 base_runner.py:59] task.vn.global_vn : False
I1001 09:59:43.261226 140265095153472 base_runner.py:59] task.vn.per_step_vn : False
I1001 09:59:43.261274 140265095153472 base_runner.py:59] task.vn.scale : NoneType
I1001 09:59:43.261321 140265095153472 base_runner.py:59] task.vn.seed : NoneType
I1001 09:59:43.261374 140265095153472 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I1001 09:59:43.261423 140265095153472 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I1001 09:59:43.261471 140265095153472 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I1001 09:59:43.261518 140265095153472 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I1001 09:59:43.261566 140265095153472 base_runner.py:59] train.early_stop.metric_history.minimize : True
I1001 09:59:43.261613 140265095153472 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I1001 09:59:43.261661 140265095153472 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I1001 09:59:43.261708 140265095153472 base_runner.py:59] train.early_stop.min_steps : 0
I1001 09:59:43.261756 140265095153472 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I1001 09:59:43.261804 140265095153472 base_runner.py:59] train.early_stop.tolerance : 0.0
I1001 09:59:43.261857 140265095153472 base_runner.py:59] train.early_stop.verbose : True
I1001 09:59:43.261906 140265095153472 base_runner.py:59] train.early_stop.window : 0
I1001 09:59:43.261953 140265095153472 base_runner.py:59] train.ema_decay : 0.0
I1001 09:59:43.262001 140265095153472 base_runner.py:59] train.enqueue_max_steps : -1
I1001 09:59:43.262048 140265095153472 base_runner.py:59] train.init_from_checkpoint_rules : {}
I1001 09:59:43.262096 140265095153472 base_runner.py:59] train.max_steps : 4000000
I1001 09:59:43.262143 140265095153472 base_runner.py:59] train.save_interval_seconds : 600
I1001 09:59:43.262191 140265095153472 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I1001 09:59:43.262238 140265095153472 base_runner.py:59] train.save_max_to_keep : 100
I1001 09:59:43.262286 140265095153472 base_runner.py:59] train.start_up_delay_steps : 200
I1001 09:59:43.262334 140265095153472 base_runner.py:59] train.summary_interval_steps : 100
I1001 09:59:43.262383 140265095153472 base_runner.py:59] train.tpu_steps_per_loop : 100
I1001 09:59:43.262430 140265095153472 base_runner.py:59] vn.global_vn : False
I1001 09:59:43.262479 140265095153472 base_runner.py:59] vn.per_step_vn : False
I1001 09:59:43.262527 140265095153472 base_runner.py:59] vn.scale : NoneType
I1001 09:59:43.262575 140265095153472 base_runner.py:59] vn.seed : NoneType
I1001 09:59:43.262622 140265095153472 base_runner.py:59] 
I1001 09:59:43.262732 140265095153472 base_runner.py:60] ============================================================
I1001 09:59:43.265200 140265095153472 base_runner.py:106] Starting ...
I1001 09:59:43.265451 140265095153472 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I1001 09:59:43.274851 140265095153472 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I1001 09:59:43.288771 140265095153472 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I1001 09:59:43.305300 140265095153472 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I1001 09:59:43.309185 140265095153472 base_input_generator.py:624] bucket_batch_limit [32]
I1001 09:59:43.359790 140265095153472 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I1001 09:59:43.359984 140265095153472 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I1001 09:59:43.360048 140265095153472 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I1001 09:59:43.360105 140265095153472 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I1001 09:59:43.360157 140265095153472 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I1001 09:59:43.360212 140265095153472 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I1001 09:59:43.360337 140265095153472 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I1001 09:59:43.360393 140265095153472 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I1001 09:59:43.360445 140265095153472 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I1001 09:59:43.360495 140265095153472 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I1001 09:59:43.360544 140265095153472 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I1001 09:59:43.360593 140265095153472 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I1001 09:59:43.360641 140265095153472 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I1001 09:59:43.360693 140265095153472 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I1001 09:59:43.360741 140265095153472 learner.py:351] Ignoring legacy param learner=None for optimization program
I1001 09:59:43.360827 140265095153472 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I1001 09:59:43.360880 140265095153472 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I1001 09:59:43.361361 140265095153472 learner.py:356] Learner params: allow_implicit_capture : NoneType
I1001 09:59:43.361443 140265095153472 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I1001 09:59:43.361504 140265095153472 learner.py:356] Learner params: bprop_variable_filter : NoneType
I1001 09:59:43.361558 140265095153472 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I1001 09:59:43.361609 140265095153472 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I1001 09:59:43.361659 140265095153472 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I1001 09:59:43.361709 140265095153472 learner.py:356] Learner params: colocate_gradients_with_ops : True
I1001 09:59:43.361758 140265095153472 learner.py:356] Learner params: dtype : float32
I1001 09:59:43.361808 140265095153472 learner.py:356] Learner params: fprop_dtype : NoneType
I1001 09:59:43.361857 140265095153472 learner.py:356] Learner params: gate_gradients : False
I1001 09:59:43.361912 140265095153472 learner.py:356] Learner params: grad_aggregation_method : 1
I1001 09:59:43.361963 140265095153472 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I1001 09:59:43.362012 140265095153472 learner.py:356] Learner params: grad_norm_tracker : NoneType
I1001 09:59:43.362061 140265095153472 learner.py:356] Learner params: inference_driver_name : NoneType
I1001 09:59:43.362120 140265095153472 learner.py:356] Learner params: is_eval : NoneType
I1001 09:59:43.362171 140265095153472 learner.py:356] Learner params: is_inference : NoneType
I1001 09:59:43.362220 140265095153472 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I1001 09:59:43.362269 140265095153472 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I1001 09:59:43.362318 140265095153472 learner.py:356] Learner params: learning_rate : 0.5
I1001 09:59:43.362367 140265095153472 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I1001 09:59:43.362416 140265095153472 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I1001 09:59:43.362465 140265095153472 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I1001 09:59:43.362514 140265095153472 learner.py:356] Learner params: lr_schedule.dtype : float32
I1001 09:59:43.362562 140265095153472 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I1001 09:59:43.362610 140265095153472 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I1001 09:59:43.362658 140265095153472 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I1001 09:59:43.362707 140265095153472 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I1001 09:59:43.362755 140265095153472 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I1001 09:59:43.362803 140265095153472 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I1001 09:59:43.362852 140265095153472 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I1001 09:59:43.362900 140265095153472 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I1001 09:59:43.362949 140265095153472 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I1001 09:59:43.362998 140265095153472 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I1001 09:59:43.363046 140265095153472 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I1001 09:59:43.363118 140265095153472 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I1001 09:59:43.363175 140265095153472 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I1001 09:59:43.363226 140265095153472 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I1001 09:59:43.363274 140265095153472 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I1001 09:59:43.363323 140265095153472 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I1001 09:59:43.363372 140265095153472 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I1001 09:59:43.363421 140265095153472 learner.py:356] Learner params: name : 'loss'
I1001 09:59:43.363470 140265095153472 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I1001 09:59:43.363518 140265095153472 learner.py:356] Learner params: optimizer.beta1 : 0.9
I1001 09:59:43.363567 140265095153472 learner.py:356] Learner params: optimizer.beta2 : 0.997
I1001 09:59:43.363615 140265095153472 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I1001 09:59:43.363663 140265095153472 learner.py:356] Learner params: optimizer.dtype : float32
I1001 09:59:43.363712 140265095153472 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I1001 09:59:43.363761 140265095153472 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I1001 09:59:43.363809 140265095153472 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I1001 09:59:43.363857 140265095153472 learner.py:356] Learner params: optimizer.is_eval : NoneType
I1001 09:59:43.363905 140265095153472 learner.py:356] Learner params: optimizer.is_inference : NoneType
I1001 09:59:43.363953 140265095153472 learner.py:356] Learner params: optimizer.name : 'Adam'
I1001 09:59:43.364002 140265095153472 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I1001 09:59:43.364050 140265095153472 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I1001 09:59:43.364105 140265095153472 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I1001 09:59:43.364155 140265095153472 learner.py:356] Learner params: optimizer.random_seed : NoneType
I1001 09:59:43.364203 140265095153472 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I1001 09:59:43.364252 140265095153472 learner.py:356] Learner params: optimizer.vn.global_vn : False
I1001 09:59:43.364300 140265095153472 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I1001 09:59:43.364349 140265095153472 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I1001 09:59:43.364397 140265095153472 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I1001 09:59:43.364445 140265095153472 learner.py:356] Learner params: params_init.method : 'xavier'
I1001 09:59:43.364494 140265095153472 learner.py:356] Learner params: params_init.scale : 1.000001
I1001 09:59:43.364542 140265095153472 learner.py:356] Learner params: params_init.seed : NoneType
I1001 09:59:43.364590 140265095153472 learner.py:356] Learner params: random_seed : NoneType
I1001 09:59:43.364639 140265095153472 learner.py:356] Learner params: skip_lp_regularization : NoneType
I1001 09:59:43.364687 140265095153472 learner.py:356] Learner params: vn.global_vn : False
I1001 09:59:43.364742 140265095153472 learner.py:356] Learner params: vn.per_step_vn : False
I1001 09:59:43.364791 140265095153472 learner.py:356] Learner params: vn.scale : NoneType
I1001 09:59:43.364839 140265095153472 learner.py:356] Learner params: vn.seed : NoneType
I1001 09:59:43.364887 140265095153472 learner.py:356] Learner params: 
I1001 09:59:43.615030 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I1001 09:59:43.617060 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.637809 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I1001 09:59:43.639991 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.642679 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I1001 09:59:43.644447 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.651450 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I1001 09:59:43.653545 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.656254 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I1001 09:59:43.657863 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.664851 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I1001 09:59:43.666957 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.669711 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I1001 09:59:43.671396 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.678639 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I1001 09:59:43.680825 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.683733 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I1001 09:59:43.685490 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.689318 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I1001 09:59:43.690947 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.694897 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I1001 09:59:43.696536 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.699782 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I1001 09:59:43.701403 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:43.711135 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:43.717605 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I1001 09:59:43.719686 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.722272 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I1001 09:59:43.723932 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:43.725928 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:43.732214 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I1001 09:59:43.734130 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.736826 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I1001 09:59:43.738450 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.743344 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I1001 09:59:43.744981 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.747704 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I1001 09:59:43.749324 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.771393 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I1001 09:59:43.773519 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.776194 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I1001 09:59:43.777919 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.784915 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I1001 09:59:43.786892 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.789745 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I1001 09:59:43.791405 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.798464 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I1001 09:59:43.800562 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.803309 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I1001 09:59:43.804970 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.812744 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I1001 09:59:43.814683 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.817340 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I1001 09:59:43.819057 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.822813 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I1001 09:59:43.824463 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.828421 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I1001 09:59:43.830041 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.832731 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I1001 09:59:43.834356 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:43.844241 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:43.850560 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I1001 09:59:43.852628 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.855242 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I1001 09:59:43.856852 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:43.858796 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:43.865559 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I1001 09:59:43.867510 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.870150 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I1001 09:59:43.871798 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.876425 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I1001 09:59:43.878036 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.880753 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I1001 09:59:43.882364 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.902711 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I1001 09:59:43.904694 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.907280 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I1001 09:59:43.909008 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.916510 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I1001 09:59:43.918414 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.921089 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I1001 09:59:43.922706 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.929861 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I1001 09:59:43.931968 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.934749 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I1001 09:59:43.936435 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.943683 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I1001 09:59:43.945796 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.948701 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I1001 09:59:43.950525 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.954442 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I1001 09:59:43.956115 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.960119 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I1001 09:59:43.961750 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.964465 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I1001 09:59:43.966095 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:43.976792 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:43.983868 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I1001 09:59:43.986052 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:43.988871 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I1001 09:59:43.990521 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:43.992668 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:43.999144 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I1001 09:59:44.001207 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.004078 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I1001 09:59:44.005738 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.010645 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I1001 09:59:44.012316 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.015067 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I1001 09:59:44.016764 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.038913 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I1001 09:59:44.041187 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.044096 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I1001 09:59:44.045939 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.053365 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I1001 09:59:44.055534 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.058428 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I1001 09:59:44.060102 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.067201 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I1001 09:59:44.069344 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.072188 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I1001 09:59:44.073853 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.081204 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I1001 09:59:44.083268 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.085998 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I1001 09:59:44.087779 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.091741 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I1001 09:59:44.093392 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.097430 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I1001 09:59:44.099065 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.102327 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I1001 09:59:44.103988 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.114177 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.121361 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I1001 09:59:44.123589 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.126367 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I1001 09:59:44.128044 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.130204 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.136695 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I1001 09:59:44.138814 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.141650 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I1001 09:59:44.143322 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.148743 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I1001 09:59:44.150625 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.153807 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I1001 09:59:44.155614 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.178204 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I1001 09:59:44.180361 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.183017 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I1001 09:59:44.184802 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.191810 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I1001 09:59:44.193744 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.196451 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I1001 09:59:44.198098 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.205085 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I1001 09:59:44.207140 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.209756 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I1001 09:59:44.211400 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.219001 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I1001 09:59:44.220957 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.223581 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I1001 09:59:44.225350 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.229231 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I1001 09:59:44.230917 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.234970 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I1001 09:59:44.236638 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.239392 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I1001 09:59:44.241034 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.251081 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.258084 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I1001 09:59:44.260231 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.262922 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I1001 09:59:44.264599 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.266679 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.273747 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I1001 09:59:44.275712 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.278408 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I1001 09:59:44.280072 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.284792 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I1001 09:59:44.286421 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.289143 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I1001 09:59:44.290784 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.310876 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I1001 09:59:44.313096 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.315708 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I1001 09:59:44.317456 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.325032 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I1001 09:59:44.326970 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.329661 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I1001 09:59:44.331331 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.338215 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I1001 09:59:44.340217 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.342851 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I1001 09:59:44.344513 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.351552 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I1001 09:59:44.353498 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.356200 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I1001 09:59:44.357970 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.362096 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I1001 09:59:44.363991 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.368671 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I1001 09:59:44.370577 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.373630 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I1001 09:59:44.375405 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.386356 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.393534 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I1001 09:59:44.395880 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.398620 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I1001 09:59:44.400312 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.402476 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.408936 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I1001 09:59:44.411015 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.413826 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I1001 09:59:44.415510 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.420453 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I1001 09:59:44.422098 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.424822 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I1001 09:59:44.426474 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.448189 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I1001 09:59:44.450333 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.453116 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I1001 09:59:44.454931 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.462040 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I1001 09:59:44.464096 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.467037 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I1001 09:59:44.468778 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.475911 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I1001 09:59:44.478049 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.480806 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I1001 09:59:44.482461 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.489696 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I1001 09:59:44.491949 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.494929 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I1001 09:59:44.496824 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.501074 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I1001 09:59:44.502836 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.507163 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I1001 09:59:44.508874 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.512274 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I1001 09:59:44.513942 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.524135 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.531313 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I1001 09:59:44.533655 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.536550 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I1001 09:59:44.538313 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.540560 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.546943 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I1001 09:59:44.548953 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.551734 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I1001 09:59:44.553415 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.558420 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I1001 09:59:44.560126 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.562891 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I1001 09:59:44.564587 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.586920 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I1001 09:59:44.589107 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.591803 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I1001 09:59:44.593572 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.600604 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I1001 09:59:44.602654 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.605510 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I1001 09:59:44.607208 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.614321 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I1001 09:59:44.616464 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.619118 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I1001 09:59:44.620778 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.628555 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I1001 09:59:44.630584 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.633299 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I1001 09:59:44.635120 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.638942 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I1001 09:59:44.640631 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.644728 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I1001 09:59:44.646385 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.649122 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I1001 09:59:44.650779 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.661041 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.667880 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I1001 09:59:44.670034 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.672746 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I1001 09:59:44.674448 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.676692 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.684015 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I1001 09:59:44.686088 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.688956 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I1001 09:59:44.690636 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.695621 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I1001 09:59:44.697297 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.700062 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I1001 09:59:44.701722 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.778494 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I1001 09:59:44.780678 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.783354 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I1001 09:59:44.785114 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.792139 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I1001 09:59:44.794135 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.797491 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I1001 09:59:44.799206 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.806255 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I1001 09:59:44.808418 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.811211 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I1001 09:59:44.812941 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.820119 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I1001 09:59:44.822165 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.824901 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I1001 09:59:44.826662 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.830554 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I1001 09:59:44.832259 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.836364 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I1001 09:59:44.838020 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.840784 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I1001 09:59:44.842476 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.852772 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.860347 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I1001 09:59:44.862466 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.865200 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I1001 09:59:44.866891 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.869021 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.875391 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I1001 09:59:44.877375 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.880144 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I1001 09:59:44.881855 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.886826 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I1001 09:59:44.888524 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.891294 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I1001 09:59:44.892967 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.914575 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I1001 09:59:44.916716 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.919401 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I1001 09:59:44.921183 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.928407 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I1001 09:59:44.930530 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.933435 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I1001 09:59:44.935151 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.942281 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I1001 09:59:44.944425 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.947179 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I1001 09:59:44.948867 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.956081 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I1001 09:59:44.958274 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.961145 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I1001 09:59:44.962942 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.966884 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I1001 09:59:44.968615 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.972809 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I1001 09:59:44.974491 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:44.977246 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I1001 09:59:44.978929 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:44.989744 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:44.996559 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I1001 09:59:44.998673 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.001369 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I1001 09:59:45.003059 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.005208 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.011622 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I1001 09:59:45.013610 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.016342 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I1001 09:59:45.018013 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.023013 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I1001 09:59:45.024719 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.027518 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I1001 09:59:45.029193 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.051377 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I1001 09:59:45.053679 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.056524 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I1001 09:59:45.058336 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.065504 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I1001 09:59:45.067629 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.070469 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I1001 09:59:45.072201 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.079278 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I1001 09:59:45.081328 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.084011 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I1001 09:59:45.085741 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.093713 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I1001 09:59:45.095805 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.098561 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I1001 09:59:45.100378 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.104292 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I1001 09:59:45.105968 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.110169 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I1001 09:59:45.111896 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.114632 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I1001 09:59:45.116350 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.126516 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.133436 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I1001 09:59:45.135624 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.138381 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I1001 09:59:45.140101 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.142292 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.148814 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I1001 09:59:45.150999 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.154885 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I1001 09:59:45.156624 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.161791 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I1001 09:59:45.163574 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.166464 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I1001 09:59:45.168744 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.190438 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I1001 09:59:45.192650 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.195372 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I1001 09:59:45.197189 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.204351 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I1001 09:59:45.206498 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.210299 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I1001 09:59:45.212037 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.219142 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I1001 09:59:45.221274 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.224079 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I1001 09:59:45.225799 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.232942 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I1001 09:59:45.234954 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.237664 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I1001 09:59:45.239485 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.243436 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I1001 09:59:45.245129 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.249309 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I1001 09:59:45.250982 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.253811 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I1001 09:59:45.255543 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.265642 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.272911 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I1001 09:59:45.275017 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.277735 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I1001 09:59:45.279460 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.281633 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.288068 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I1001 09:59:45.290152 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.293042 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I1001 09:59:45.294861 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.300302 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I1001 09:59:45.302053 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.304919 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I1001 09:59:45.306629 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.328869 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I1001 09:59:45.331034 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.333785 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I1001 09:59:45.335597 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.342736 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I1001 09:59:45.344764 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.347562 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I1001 09:59:45.349250 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.356343 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I1001 09:59:45.358420 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.361150 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I1001 09:59:45.362841 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.369970 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I1001 09:59:45.371993 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.374701 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I1001 09:59:45.376530 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.380464 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I1001 09:59:45.382150 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.386349 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I1001 09:59:45.388067 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.390826 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I1001 09:59:45.392555 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.403563 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.410322 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I1001 09:59:45.412594 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.415423 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I1001 09:59:45.417118 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.419393 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.425843 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I1001 09:59:45.427911 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.430671 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I1001 09:59:45.432381 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.437531 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I1001 09:59:45.439289 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.442135 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I1001 09:59:45.443870 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.465959 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I1001 09:59:45.468105 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.470795 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I1001 09:59:45.472628 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.479687 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I1001 09:59:45.481686 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.484472 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I1001 09:59:45.486180 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.493219 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I1001 09:59:45.495291 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.497940 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I1001 09:59:45.499680 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.507497 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I1001 09:59:45.509505 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.512231 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I1001 09:59:45.514041 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.518057 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I1001 09:59:45.519784 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.524354 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I1001 09:59:45.526108 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.529004 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I1001 09:59:45.530792 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.541325 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.548705 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I1001 09:59:45.550987 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.553888 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I1001 09:59:45.555633 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.557892 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.564342 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I1001 09:59:45.566341 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.569934 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I1001 09:59:45.571677 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.576679 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I1001 09:59:45.578425 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.581247 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I1001 09:59:45.582962 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.604030 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I1001 09:59:45.606178 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.608925 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I1001 09:59:45.610738 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.617846 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I1001 09:59:45.619976 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.623729 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I1001 09:59:45.625440 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.632550 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I1001 09:59:45.634714 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.637552 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I1001 09:59:45.639281 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.646476 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I1001 09:59:45.648543 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.651271 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I1001 09:59:45.653086 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.657205 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I1001 09:59:45.658917 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.663210 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I1001 09:59:45.664916 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.667711 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I1001 09:59:45.669425 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.679826 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.687393 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I1001 09:59:45.689624 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.692492 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I1001 09:59:45.694240 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.696629 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.703305 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I1001 09:59:45.705466 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.708434 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I1001 09:59:45.710163 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.715371 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I1001 09:59:45.717081 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.719897 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I1001 09:59:45.721610 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.744040 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I1001 09:59:45.746258 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.749065 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I1001 09:59:45.750883 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.757995 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I1001 09:59:45.760039 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.762805 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I1001 09:59:45.764554 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.771606 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I1001 09:59:45.773697 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.776465 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I1001 09:59:45.778181 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.785391 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I1001 09:59:45.787509 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.790306 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I1001 09:59:45.792146 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.796167 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I1001 09:59:45.797902 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.802195 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I1001 09:59:45.803949 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.806724 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I1001 09:59:45.808487 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.819386 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.826247 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I1001 09:59:45.828590 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.831516 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I1001 09:59:45.833250 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:45.835580 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:45.842089 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I1001 09:59:45.844211 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.847056 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I1001 09:59:45.848826 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.853983 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I1001 09:59:45.855804 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.858600 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I1001 09:59:45.860345 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.936462 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I1001 09:59:45.938744 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.941650 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I1001 09:59:45.943530 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.950713 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I1001 09:59:45.952875 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.955761 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I1001 09:59:45.957479 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.964747 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I1001 09:59:45.966871 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.969630 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I1001 09:59:45.971385 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.978525 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I1001 09:59:45.980592 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.983340 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I1001 09:59:45.985808 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.989784 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I1001 09:59:45.991535 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:45.995772 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I1001 09:59:45.997486 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.000263 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I1001 09:59:46.002001 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.012273 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.018903 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I1001 09:59:46.021121 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.023954 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I1001 09:59:46.025689 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.028018 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.034506 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I1001 09:59:46.036644 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.039525 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I1001 09:59:46.041254 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.046449 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I1001 09:59:46.048197 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.051726 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I1001 09:59:46.053444 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.075233 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I1001 09:59:46.077532 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.080377 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I1001 09:59:46.082235 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.089412 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I1001 09:59:46.091470 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.094264 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I1001 09:59:46.096010 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.103825 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I1001 09:59:46.105992 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.108764 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I1001 09:59:46.110489 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.117761 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I1001 09:59:46.119876 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.122696 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I1001 09:59:46.124567 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.128568 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I1001 09:59:46.130290 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.134696 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I1001 09:59:46.136499 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.139300 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I1001 09:59:46.141043 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.151389 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.158303 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I1001 09:59:46.161527 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.164497 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I1001 09:59:46.166246 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.168582 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.175163 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I1001 09:59:46.177274 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.180203 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I1001 09:59:46.181966 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.187349 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I1001 09:59:46.189107 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.192014 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I1001 09:59:46.193747 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.216587 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I1001 09:59:46.218808 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.221600 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I1001 09:59:46.223469 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.230570 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I1001 09:59:46.232630 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.235472 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I1001 09:59:46.237205 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.244316 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I1001 09:59:46.246403 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.249133 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I1001 09:59:46.250947 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.258264 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I1001 09:59:46.260436 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.263319 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I1001 09:59:46.265177 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.269219 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I1001 09:59:46.270966 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.275313 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I1001 09:59:46.277060 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.279901 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I1001 09:59:46.281646 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.292696 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.299537 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I1001 09:59:46.301764 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.304584 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I1001 09:59:46.306350 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.308689 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.315175 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I1001 09:59:46.317213 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.320040 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I1001 09:59:46.321779 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.326936 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I1001 09:59:46.328705 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.331541 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I1001 09:59:46.333270 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.355984 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I1001 09:59:46.358289 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.361121 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I1001 09:59:46.362973 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.370239 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I1001 09:59:46.372361 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.375227 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I1001 09:59:46.376965 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.384109 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I1001 09:59:46.386255 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.388995 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I1001 09:59:46.390727 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.398012 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I1001 09:59:46.400105 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.402996 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I1001 09:59:46.405516 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.409551 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I1001 09:59:46.411329 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.415641 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I1001 09:59:46.417371 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.420160 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I1001 09:59:46.421895 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.432138 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.438609 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I1001 09:59:46.440740 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.443459 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I1001 09:59:46.445209 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.447478 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.453862 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I1001 09:59:46.456044 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.458853 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I1001 09:59:46.460614 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.465771 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I1001 09:59:46.467519 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.471009 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I1001 09:59:46.472780 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.493499 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I1001 09:59:46.495564 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.498291 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I1001 09:59:46.500161 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.507329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I1001 09:59:46.509452 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.512330 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I1001 09:59:46.514091 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.522315 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I1001 09:59:46.524598 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.527450 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I1001 09:59:46.529216 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.536549 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I1001 09:59:46.538669 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.541491 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I1001 09:59:46.543364 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.547415 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I1001 09:59:46.549178 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.553709 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I1001 09:59:46.555574 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.558577 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I1001 09:59:46.560431 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.571165 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.578539 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I1001 09:59:46.581780 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.584824 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I1001 09:59:46.586626 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.589018 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.595659 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I1001 09:59:46.597851 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.600837 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I1001 09:59:46.602621 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.607928 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I1001 09:59:46.609682 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.612617 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I1001 09:59:46.614365 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.637071 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I1001 09:59:46.639361 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.642185 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I1001 09:59:46.644059 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.651264 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I1001 09:59:46.653368 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.656341 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I1001 09:59:46.658084 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.665294 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I1001 09:59:46.667453 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.670210 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I1001 09:59:46.672016 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.679455 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I1001 09:59:46.681710 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.684689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I1001 09:59:46.686582 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.690814 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I1001 09:59:46.692633 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.697192 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I1001 09:59:46.698979 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.701869 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I1001 09:59:46.703655 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.715074 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.722338 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I1001 09:59:46.724594 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.727402 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I1001 09:59:46.729156 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.731425 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.737739 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I1001 09:59:46.739820 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.742608 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I1001 09:59:46.744395 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.749527 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I1001 09:59:46.751299 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.754176 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I1001 09:59:46.755965 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.777297 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I1001 09:59:46.779357 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.782047 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I1001 09:59:46.783929 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.790980 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I1001 09:59:46.793043 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.795870 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I1001 09:59:46.797630 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.804744 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I1001 09:59:46.806877 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.809627 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I1001 09:59:46.811420 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.818819 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I1001 09:59:46.821255 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.824285 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I1001 09:59:46.826845 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.831206 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I1001 09:59:46.833104 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.838015 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I1001 09:59:46.839896 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.842743 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I1001 09:59:46.844554 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.855163 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.862328 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I1001 09:59:46.864757 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.867713 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I1001 09:59:46.869498 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:46.871920 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:46.878548 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I1001 09:59:46.880876 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.883916 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I1001 09:59:46.885730 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.891328 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I1001 09:59:46.893182 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.896826 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I1001 09:59:46.898607 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.920553 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I1001 09:59:46.922851 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.925817 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I1001 09:59:46.927800 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.935292 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I1001 09:59:46.937698 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.940815 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I1001 09:59:46.942590 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.950777 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I1001 09:59:46.953245 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.956254 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I1001 09:59:46.958068 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.965738 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I1001 09:59:46.968103 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.971069 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I1001 09:59:46.972966 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.977212 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I1001 09:59:46.979048 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.983608 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I1001 09:59:46.985377 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:46.988223 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I1001 09:59:46.989991 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.000687 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.008126 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I1001 09:59:47.011450 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.014447 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I1001 09:59:47.016266 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.018629 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.025277 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I1001 09:59:47.027477 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.031573 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I1001 09:59:47.033406 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.039183 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I1001 09:59:47.041107 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.044106 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I1001 09:59:47.045867 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.122140 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I1001 09:59:47.124577 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.128328 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I1001 09:59:47.130165 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.137492 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I1001 09:59:47.139852 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.142758 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I1001 09:59:47.144582 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.151904 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I1001 09:59:47.154006 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.156795 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I1001 09:59:47.158670 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.165789 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I1001 09:59:47.167905 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.170783 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I1001 09:59:47.172590 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.176655 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I1001 09:59:47.178424 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.182862 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I1001 09:59:47.184672 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.187517 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I1001 09:59:47.189291 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.200223 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.206681 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I1001 09:59:47.208856 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.211598 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I1001 09:59:47.213391 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.215674 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.222128 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I1001 09:59:47.224248 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.227129 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I1001 09:59:47.228914 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.234229 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I1001 09:59:47.236040 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.238938 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I1001 09:59:47.240760 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.264376 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I1001 09:59:47.266807 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.269892 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I1001 09:59:47.271918 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.279334 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I1001 09:59:47.281630 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.284747 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I1001 09:59:47.286581 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.294127 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I1001 09:59:47.296703 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.299806 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I1001 09:59:47.301612 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.308947 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I1001 09:59:47.311080 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.313962 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I1001 09:59:47.315858 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.320583 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I1001 09:59:47.322377 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.326831 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I1001 09:59:47.328627 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.331478 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I1001 09:59:47.333263 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.344059 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.351310 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I1001 09:59:47.353834 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.356961 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I1001 09:59:47.358807 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.361323 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.367917 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I1001 09:59:47.370114 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.373146 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I1001 09:59:47.375000 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.380537 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I1001 09:59:47.382355 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.385369 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I1001 09:59:47.387192 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.409731 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I1001 09:59:47.412100 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.415045 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I1001 09:59:47.416993 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.424279 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I1001 09:59:47.426409 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.429320 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I1001 09:59:47.431126 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.438289 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I1001 09:59:47.441185 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.444040 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I1001 09:59:47.445801 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.453046 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I1001 09:59:47.455156 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.458057 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I1001 09:59:47.459989 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.464155 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I1001 09:59:47.465993 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.470586 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I1001 09:59:47.472411 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.475278 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I1001 09:59:47.477067 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.487639 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.494689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I1001 09:59:47.497071 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.500030 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I1001 09:59:47.501865 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.505158 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.511924 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I1001 09:59:47.514149 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.517127 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I1001 09:59:47.518916 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.524304 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I1001 09:59:47.526102 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.529044 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I1001 09:59:47.530833 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.552529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I1001 09:59:47.554798 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.557654 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I1001 09:59:47.560375 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.567651 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I1001 09:59:47.569790 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.572871 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I1001 09:59:47.574668 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.581988 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I1001 09:59:47.584216 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.587035 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I1001 09:59:47.588851 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.596107 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I1001 09:59:47.598206 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.601072 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I1001 09:59:47.602984 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.607209 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I1001 09:59:47.609033 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.613802 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I1001 09:59:47.615637 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.618541 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I1001 09:59:47.620364 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.631695 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.638502 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I1001 09:59:47.640745 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.643608 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I1001 09:59:47.645424 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.647866 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.654360 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I1001 09:59:47.656594 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.659489 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I1001 09:59:47.661287 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.666671 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I1001 09:59:47.668492 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.671477 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I1001 09:59:47.673278 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.695478 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I1001 09:59:47.697685 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.700498 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I1001 09:59:47.702394 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.709692 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I1001 09:59:47.711958 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.715026 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I1001 09:59:47.716873 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.724173 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I1001 09:59:47.726491 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.729496 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I1001 09:59:47.731326 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.738597 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I1001 09:59:47.740780 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.743701 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I1001 09:59:47.745622 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.750391 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I1001 09:59:47.752228 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.756735 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I1001 09:59:47.758524 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.761429 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I1001 09:59:47.763258 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.773801 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.780463 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I1001 09:59:47.782675 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.785531 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I1001 09:59:47.787372 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.789765 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.796276 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I1001 09:59:47.798393 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.801272 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I1001 09:59:47.803090 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.808456 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I1001 09:59:47.810263 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.813187 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I1001 09:59:47.814992 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.837573 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I1001 09:59:47.839886 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.842706 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I1001 09:59:47.844625 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.851997 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I1001 09:59:47.854351 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.857606 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I1001 09:59:47.859443 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.866820 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I1001 09:59:47.869895 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.872874 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I1001 09:59:47.874665 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.881927 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I1001 09:59:47.884126 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.886998 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I1001 09:59:47.888912 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.893098 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I1001 09:59:47.894915 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.899514 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I1001 09:59:47.901324 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.904209 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I1001 09:59:47.906031 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.916634 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.923323 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I1001 09:59:47.925666 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.928647 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I1001 09:59:47.930476 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:47.933764 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:47.940434 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I1001 09:59:47.942601 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.945525 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I1001 09:59:47.947371 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.952727 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I1001 09:59:47.954529 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.957452 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I1001 09:59:47.959288 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.981249 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I1001 09:59:47.983510 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.986454 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I1001 09:59:47.989242 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:47.996630 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I1001 09:59:47.998925 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:48.001986 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I1001 09:59:48.003843 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:48.011186 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I1001 09:59:48.013408 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:48.016316 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I1001 09:59:48.018126 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:48.025382 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I1001 09:59:48.027521 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:48.030333 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I1001 09:59:48.032260 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:48.036521 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I1001 09:59:48.038370 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:48.043115 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I1001 09:59:48.044947 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:48.047915 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I1001 09:59:48.049750 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:48.972016 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:48.979712 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I1001 09:59:48.982200 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:48.985245 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I1001 09:59:48.987076 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:48.989592 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:48.996130 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I1001 09:59:48.998374 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.001294 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I1001 09:59:49.003191 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.008862 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I1001 09:59:49.010982 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.013881 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I1001 09:59:49.015826 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.040589 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I1001 09:59:49.043023 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.046145 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I1001 09:59:49.048016 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.055500 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I1001 09:59:49.057981 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.061016 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I1001 09:59:49.062891 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.070513 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I1001 09:59:49.072832 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.075799 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I1001 09:59:49.077735 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.085138 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I1001 09:59:49.087510 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.090654 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I1001 09:59:49.092500 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.096761 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I1001 09:59:49.098616 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.103379 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I1001 09:59:49.105254 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.109250 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I1001 09:59:49.111296 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.122396 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.129985 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I1001 09:59:49.132548 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.135596 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I1001 09:59:49.137524 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.140199 140265095153472 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.147230 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I1001 09:59:49.149648 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.152742 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I1001 09:59:49.154565 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.160179 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I1001 09:59:49.162061 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.165047 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I1001 09:59:49.166884 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.172805 140265095153472 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.179553 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I1001 09:59:49.181727 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.182598 140265095153472 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.189856 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I1001 09:59:49.192131 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.193031 140265095153472 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.199532 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I1001 09:59:49.201853 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.202834 140265095153472 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.209716 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I1001 09:59:49.212095 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.213043 140265095153472 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.219649 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I1001 09:59:49.221899 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.222841 140265095153472 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.229492 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I1001 09:59:49.231775 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.232682 140265095153472 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.239186 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I1001 09:59:49.241326 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.242170 140265095153472 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.248542 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I1001 09:59:49.250690 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.251546 140265095153472 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.257915 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I1001 09:59:49.260047 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.260880 140265095153472 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.268044 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I1001 09:59:49.270164 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.271025 140265095153472 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.277307 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I1001 09:59:49.279441 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.280279 140265095153472 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.286602 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I1001 09:59:49.288714 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.289549 140265095153472 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.295863 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I1001 09:59:49.297934 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.298756 140265095153472 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.305099 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I1001 09:59:49.307198 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.308036 140265095153472 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.314330 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I1001 09:59:49.316444 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W1001 09:59:49.317279 140265095153472 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I1001 09:59:49.323626 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I1001 09:59:49.325716 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.328596 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I1001 09:59:49.330523 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.333324 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I1001 09:59:49.335206 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.339039 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I1001 09:59:49.341171 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.344429 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I1001 09:59:49.346379 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.349607 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I1001 09:59:49.351572 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.354533 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I1001 09:59:49.356392 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.359529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I1001 09:59:49.361372 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.364209 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I1001 09:59:49.366136 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.369101 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I1001 09:59:49.370962 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.373909 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I1001 09:59:49.375770 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.378590 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I1001 09:59:49.380436 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.383398 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I1001 09:59:49.385238 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.388025 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I1001 09:59:49.389829 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.392755 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I1001 09:59:49.394567 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.397452 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I1001 09:59:49.399391 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:49.402171 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I1001 09:59:49.404016 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I1001 09:59:50.080394 140265095153472 py_utils.py:1484] === worker 0 ===
I1001 09:59:50.097151 140265095153472 py_utils.py:1474] worker 0: global_step                                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097352 140265095153472 py_utils.py:1474] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097411 140265095153472 py_utils.py:1474] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097464 140265095153472 py_utils.py:1474] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097513 140265095153472 py_utils.py:1474] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097560 140265095153472 py_utils.py:1474] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097617 140265095153472 py_utils.py:1474] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097665 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097711 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097758 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097803 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097849 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097894 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097940 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.097985 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098030 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098076 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098121 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098166 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098212 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098257 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098302 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098346 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098391 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098436 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098484 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098531 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098576 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098621 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098666 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098711 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098756 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098801 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098845 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098890 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098935 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.098981 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099026 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099071 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099136 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099184 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099230 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099275 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099320 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099369 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099415 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099460 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099505 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099550 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099596 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099641 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099686 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099732 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099777 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099822 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099868 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099913 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.099959 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100004 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100049 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100094 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100139 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100188 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100235 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100280 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100326 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100371 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100416 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100460 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100505 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100550 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100595 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100640 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100685 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100730 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100775 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100821 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100865 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100910 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100954 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.100999 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101047 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101093 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101138 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101183 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101227 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101273 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101318 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101363 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101408 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101453 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101498 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101543 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101587 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101632 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101677 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101721 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101767 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101811 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101859 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101905 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101949 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.101995 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102039 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102084 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102129 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102174 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102219 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102263 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102308 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102352 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102397 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102441 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102486 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102530 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102575 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102620 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102664 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102713 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102759 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102804 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102849 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102893 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102938 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.102983 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103027 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103072 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103134 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103182 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103227 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103272 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103317 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103362 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103406 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103451 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103496 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103545 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103592 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103637 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103682 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103728 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103772 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103817 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103861 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103905 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103950 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.103994 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104038 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104083 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104127 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104171 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104216 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104261 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104306 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104351 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104401 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104447 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104492 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104537 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104583 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104627 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104671 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104716 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104762 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104806 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104851 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104895 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104940 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.104985 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105030 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105074 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105119 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105164 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105213 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105259 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105304 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105349 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105393 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105438 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105483 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105528 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105573 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105618 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105663 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105708 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105752 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105797 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105841 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105886 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105931 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.105976 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106020 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106069 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106115 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106160 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106205 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106250 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106295 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106340 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106384 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106429 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106474 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106518 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106563 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106607 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106651 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106696 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106740 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106784 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106829 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106876 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106922 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.106970 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107016 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107061 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107126 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107176 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107221 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107266 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107311 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107356 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107400 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107445 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107489 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107533 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107578 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107622 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107667 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107712 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107761 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107806 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107851 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107896 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107940 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.107984 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108028 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108072 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108117 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108161 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108205 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108250 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108294 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108338 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108383 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108427 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108472 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108515 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108563 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108608 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108654 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108698 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108743 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108787 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108831 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108876 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108920 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.108965 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109010 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109056 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109101 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109146 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109191 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109236 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109281 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109325 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109369 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109418 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109464 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109509 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109555 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109599 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109644 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109689 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109734 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109778 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109823 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109868 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109913 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.109957 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110002 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110047 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110091 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110136 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110180 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110228 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110273 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110318 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110362 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110407 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110451 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110496 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110540 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110585 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110630 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110674 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110719 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110764 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110809 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110854 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110899 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110944 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.110989 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111033 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111081 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111143 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111189 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111233 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111279 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111324 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111369 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111414 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111459 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111504 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111548 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111593 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111638 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111683 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111728 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111773 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111817 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111861 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111906 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.111955 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112002 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112046 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112092 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112136 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112180 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112226 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112270 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112315 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112360 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112404 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112449 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112494 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112539 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112584 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112628 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112673 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112719 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112767 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112813 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112858 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112904 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112948 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.112993 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113038 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113082 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113127 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113172 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113217 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113262 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113307 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113352 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113396 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113441 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113485 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113530 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113574 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113623 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113668 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113713 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113758 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113803 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113847 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113892 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113937 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.113982 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114027 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114071 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114116 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114161 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114205 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114250 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114295 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114339 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114384 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114433 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114478 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114523 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114568 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114613 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114657 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114701 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114747 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114792 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114837 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114881 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114926 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.114970 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115015 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115060 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115118 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115168 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115213 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115259 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115308 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115354 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115399 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115445 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115490 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115534 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115579 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115624 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115669 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115714 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115759 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115803 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115849 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115894 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115938 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.115982 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116027 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116072 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116122 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116168 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116213 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116259 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116304 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116349 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116394 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116440 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116485 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116531 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116576 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116621 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116666 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116711 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116756 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116801 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116846 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116891 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116936 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.116986 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117036 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117081 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117126 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117171 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117217 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117262 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117307 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117352 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117397 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117441 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117486 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117532 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117577 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117621 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117666 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117711 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117756 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117806 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117852 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117897 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117942 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.117986 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118031 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118076 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118120 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118165 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118210 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118254 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118299 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118343 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118387 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118432 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118476 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118520 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118566 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118611 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118661 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118707 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118752 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118797 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118841 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118886 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118931 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.118976 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119021 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119065 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119130 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119179 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119225 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119269 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119314 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119359 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119403 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119448 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119498 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119544 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119589 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119634 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119679 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119724 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119769 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119814 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119859 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119904 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119949 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.119994 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120039 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120084 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120128 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120173 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120217 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120262 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120306 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120356 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120401 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120446 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120490 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120535 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120580 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120624 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120668 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120720 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120765 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120809 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120854 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120898 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120943 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.120987 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121032 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121076 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121121 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121173 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121219 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121264 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121309 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121354 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121398 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121443 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121488 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121533 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121578 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121623 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121668 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121713 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121758 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121803 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121848 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121893 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121938 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.121983 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122031 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122077 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122122 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122167 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122212 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122256 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122301 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122346 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122390 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122435 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122480 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122525 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122570 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122615 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122660 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122705 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122749 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122794 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122843 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122888 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122933 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.122977 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123022 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123067 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123127 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123176 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123221 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123265 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123310 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123354 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123399 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123443 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123487 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123532 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123577 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123622 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123667 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123717 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123762 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123807 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123853 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123898 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123943 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.123988 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124033 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124078 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124123 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124168 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124213 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124258 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124303 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124348 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124393 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124438 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124483 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124528 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124577 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124623 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124667 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124712 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124757 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124801 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124845 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124891 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124935 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.124979 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125024 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125069 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125114 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125159 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125204 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125248 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125293 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125338 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125386 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125432 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125477 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125522 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125567 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125611 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125656 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125701 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125745 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125790 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125834 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125879 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125924 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.125968 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126013 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126057 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126101 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126146 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126190 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126239 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126284 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126328 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126373 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126418 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126463 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126508 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126553 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126597 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126642 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126687 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126731 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126776 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126821 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126866 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126910 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.126955 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127000 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127056 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127115 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127165 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127212 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127257 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127302 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127346 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127390 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127434 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127479 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127523 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127568 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127612 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127657 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127701 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127746 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127791 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127836 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127880 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127931 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.127976 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128021 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128066 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128110 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128155 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128200 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128245 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128289 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128334 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128378 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128423 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128468 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128513 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128558 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128602 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128647 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128691 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128741 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128787 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128832 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128877 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128921 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.128966 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129010 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129054 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129099 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129145 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129190 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129235 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129280 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129325 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129370 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129416 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129461 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129506 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129550 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129599 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129645 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129690 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129735 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129779 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129824 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129869 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129914 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.129958 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130003 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130048 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130093 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130138 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130182 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130227 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130272 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130317 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130362 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130412 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130458 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130503 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130548 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130593 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130638 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130684 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130729 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130774 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130819 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130864 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130910 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.130955 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131000 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131046 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131105 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131159 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131205 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131251 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131301 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131347 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131393 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131438 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131483 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131527 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131572 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131617 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131662 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131707 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131752 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131797 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131842 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131886 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131931 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.131976 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132021 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132066 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132114 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132160 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132205 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132250 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132295 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132339 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132385 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132429 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132474 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132519 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132564 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132609 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132654 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132699 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132745 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132790 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132834 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132879 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132924 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.132972 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133018 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133063 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133107 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133152 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133196 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133241 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133286 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133330 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133375 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133419 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133464 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133509 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133554 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133599 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133644 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133688 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133733 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133782 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133829 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133874 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133919 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.133965 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134010 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134055 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134100 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134145 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134190 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134235 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134280 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134325 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134370 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134415 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134459 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134505 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134549 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134593 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134642 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134688 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134732 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134778 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134823 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134867 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134912 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.134957 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135002 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135046 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135101 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135153 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135200 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135244 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135290 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135334 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135380 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135425 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135474 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135520 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135565 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135610 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135655 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135700 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135745 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135790 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135834 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135879 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135924 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.135969 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136013 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136058 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136103 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136148 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136193 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136238 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136283 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136331 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136378 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136423 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136468 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136513 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136558 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136603 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136647 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136692 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136736 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136780 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136825 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136870 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136915 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.136960 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137005 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137050 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137098 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137144 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137193 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137240 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137284 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137329 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137374 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137418 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137463 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137507 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137551 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137596 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137640 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137684 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137729 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137773 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137817 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137862 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137906 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.137950 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138000 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138046 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138090 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138134 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138179 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138223 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138268 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138312 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138356 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138401 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138446 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138490 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138535 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138578 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138623 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138668 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138713 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138757 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138801 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138850 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138895 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138940 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.138985 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139029 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139074 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139136 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139184 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139229 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139275 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139320 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139366 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139411 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139456 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139500 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139545 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139590 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139634 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139684 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139730 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139776 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139821 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139866 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139912 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.139956 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140002 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140047 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140092 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140136 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140181 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140226 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140271 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140316 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140360 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140405 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140450 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140495 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140545 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140590 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140635 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140679 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140723 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140768 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140813 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140857 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140902 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140946 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.140991 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141036 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141080 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141124 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141168 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141213 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141258 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141302 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141350 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141395 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141439 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141484 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141529 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141573 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141618 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141662 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141706 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141751 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141797 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141841 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141887 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141932 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.141976 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142021 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142065 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142109 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142154 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142203 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142249 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142293 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142338 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142383 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142427 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142472 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142517 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142562 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142607 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142651 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142696 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142740 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142785 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142830 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142875 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142920 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.142964 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143013 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143059 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143121 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143172 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143218 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143264 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143309 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143355 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143400 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143445 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143490 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143536 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143581 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143626 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143671 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143717 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143762 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143807 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143852 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143902 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143948 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.143993 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144038 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144083 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144127 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144171 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144216 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144260 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144304 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144349 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144394 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144438 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144484 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144529 140265095153472 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144573 140265095153472 py_utils.py:1474] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I1001 09:59:50.144704 140265095153472 py_utils.py:1490] ==========
I1001 09:59:52.371649 140265095153472 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 09:59:54.587584 140265095153472 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:59:56.761447 140265095153472 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 09:59:58.936623 140265095153472 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 10:00:01.252970 140265095153472 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W1001 10:00:04.301432 140265095153472 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 10:00:04.785035 140265095153472 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 10:00:06.658026 140265095153472 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 10:00:07.176977 140265095153472 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 10:00:09.051285 140265095153472 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 10:00:09.625161 140265095153472 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W1001 10:00:11.526375 140265095153472 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I1001 10:00:12.499653 140265095153472 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 10:00:15.080533 140265095153472 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 10:00:17.630314 140265095153472 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 10:00:22.077523 140265095153472 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 10:00:24.178498 140265095153472 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I1001 10:00:24.182907 140265095153472 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I1001 10:00:24.274249 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I1001 10:00:24.276243 140265095153472 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1001 10:00:24.284166 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I1001 10:00:24.284273 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.284341 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.284398 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.284453 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.284505 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.284557 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.284620 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.284701 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.284783 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.284860 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.284914 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.284965 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.285027 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.285084 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.285135 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.285186 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.285236 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.285286 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.285336 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.285386 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.285436 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.285486 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.285536 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.285586 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.285637 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.285691 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.285741 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.285791 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.285841 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.285891 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.285940 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.285989 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.286038 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.286087 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.286137 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.286186 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.286236 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.286290 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.286341 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.286391 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.286442 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.286492 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.286542 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.286593 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.286643 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.286693 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.286743 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.286793 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.286842 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.286892 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.286942 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.286992 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.287042 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.287106 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.287163 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.287214 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.287265 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.287314 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.287364 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.287414 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.287463 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.287519 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.287571 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.287621 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.287671 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.287721 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.287771 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.287821 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.287870 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.287920 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.287970 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.288019 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.288069 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.288118 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.288169 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.288218 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.288268 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.288317 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.288367 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.288417 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.288466 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.288516 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.288566 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.288615 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.288665 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.288715 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.288770 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.288821 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.288870 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.288920 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.288969 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.289018 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.289068 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.289119 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.289169 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.289219 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.289268 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.289318 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.289368 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.289417 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.289466 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.289516 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.289566 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.289615 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.289665 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.289715 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.289765 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.289814 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.289865 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.289914 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.289964 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.290018 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.290070 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.290120 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.290169 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.290219 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.290268 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.290318 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.290368 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.290425 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.290475 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.290524 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.290574 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.290623 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.290673 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.290722 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.290772 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.290822 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.290873 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.290923 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.290972 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.291022 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.291071 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.291138 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.291190 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.291247 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.291298 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.291348 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.291397 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.291446 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.291496 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.291546 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.291595 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.291646 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.291695 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.291745 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.291794 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.291843 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.291892 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.291941 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.291990 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.292039 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.292089 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.292138 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.292187 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.292237 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.292286 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.292335 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.292384 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.292433 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.292488 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.292539 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.292588 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.292638 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.292687 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.292736 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.292785 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.292835 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.292884 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.292933 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.292983 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.293032 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.293081 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.293131 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.293180 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.293229 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.293279 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.293328 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.293378 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.293427 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.293477 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.293526 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.293576 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.293624 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.293678 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.293728 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.293777 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.293828 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.293878 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.293927 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.293977 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.294027 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.294076 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.294125 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.294176 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.294225 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.294275 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.294326 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.294376 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.294426 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.294475 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.294524 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.294574 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.294623 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.294673 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.294722 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.294772 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.294821 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.294871 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.294930 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.294981 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.295031 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.295080 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.295154 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.295206 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.295256 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.295306 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.295355 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.295405 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.295454 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.295502 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.295552 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.295601 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.295650 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.295698 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.295748 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.295796 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.295845 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.295895 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.295944 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.295994 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.296043 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.296092 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.296147 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.296197 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.296246 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.296296 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.296345 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.296395 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.296444 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.296494 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.296543 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.296592 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.296641 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.296691 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.296740 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.296790 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.296839 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.296888 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.296937 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.296986 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.297035 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.297085 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.297134 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.297183 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.297232 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.297281 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.297330 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.297383 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.297432 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.297481 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.297530 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.297580 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.297629 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.297678 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.297728 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.297778 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.297827 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.297876 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.297925 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.297974 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.298023 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.298074 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.298124 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.298174 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.298224 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.298273 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.298323 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.298372 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.298422 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.298472 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.298523 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.298577 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.298628 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.298677 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.298727 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.298777 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.298827 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.298877 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.298926 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.298977 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.299027 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.299075 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.299142 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.299193 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.299242 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.299292 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.299341 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.299391 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.299440 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.299490 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.299539 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.299588 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.299637 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.299687 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.299737 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.299791 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.299842 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.299892 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.299942 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.299991 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.300041 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.300090 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.300140 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.300189 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.300239 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.300288 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.300338 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.300388 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.300436 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.300486 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.300535 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.300585 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.300635 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.300684 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.300734 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.300782 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.300831 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.300880 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.300929 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.300979 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.301033 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.301084 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.301133 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.301182 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.301232 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.301281 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.301330 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.301379 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.301429 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.301478 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.301527 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.301576 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.301626 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.301675 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.301724 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.301773 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.301823 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.301872 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.301922 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.301971 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.302020 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.302070 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.302119 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.302168 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.302221 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.302271 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.302321 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.302370 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.302420 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.302469 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.302518 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.302567 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.302616 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.302667 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.302716 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.302766 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.302815 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.302864 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.302913 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.302961 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.303010 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.303059 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.303122 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.303176 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.303225 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.303275 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.303325 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.303374 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.303423 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.303478 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.303529 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.303579 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.303629 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.303678 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.303728 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.303777 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.303827 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.303876 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.303925 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.303974 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.304023 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.304072 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.304121 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.304171 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.304220 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.304269 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.304318 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.304367 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.304417 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.304467 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.304517 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.304566 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.304615 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.304672 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.304722 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.304771 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.304820 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.304869 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.304919 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.304969 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.305018 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.305067 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.305116 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.305170 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.305221 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.305271 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.305322 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.305372 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.305422 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.305471 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.305521 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.305570 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.305619 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.305669 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.305719 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.305768 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.305818 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.305867 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.305923 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.305974 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.306023 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.306074 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.306124 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.306174 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.306223 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.306273 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.306322 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.306372 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.306421 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.306470 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.306519 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.306569 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.306618 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.306669 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.306718 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.306768 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.306817 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.306866 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.306916 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.306965 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.307014 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.307064 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.307137 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.307192 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.307242 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.307291 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.307341 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.307390 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.307440 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.307489 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.307539 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.307588 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.307636 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.307685 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.307734 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.307783 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.307832 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.307882 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.307930 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.307979 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.308028 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.308077 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.308126 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.308175 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.308224 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.308274 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.308329 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.308380 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.308429 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.308479 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.308528 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.308577 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.308626 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.308675 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.308724 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.308774 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.308823 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.308871 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.308921 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.308969 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.309019 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.309068 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.309116 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.309166 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.309215 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.309263 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.309313 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.309362 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.309411 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.309462 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.309511 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.309565 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.309615 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.309664 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.309714 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.309763 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.309812 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.309861 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.309910 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.309960 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.310009 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.310059 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.310109 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.310159 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.310209 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.310258 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.310308 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.310357 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.310406 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.310455 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.310504 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.310553 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.310602 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.310651 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.310700 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.310754 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.310804 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I1001 10:00:24.310853 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I1001 10:00:24.310903 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I1001 10:00:24.310952 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I1001 10:00:24.311001 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I1001 10:00:24.311050 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I1001 10:00:24.311112 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I1001 10:00:24.311168 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I1001 10:00:24.311218 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I1001 10:00:24.311268 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I1001 10:00:24.311317 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I1001 10:00:24.311367 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I1001 10:00:24.311416 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I1001 10:00:24.311465 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I1001 10:00:24.311514 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I1001 10:00:24.311563 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I1001 10:00:24.311612 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I1001 10:00:24.311661 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I1001 10:00:24.311712 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I1001 10:00:24.311761 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I1001 10:00:24.311810 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I1001 10:00:24.311860 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I1001 10:00:24.311910 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I1001 10:00:24.311959 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I1001 10:00:24.312008 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I1001 10:00:24.312057 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I1001 10:00:24.312112 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I1001 10:00:24.312163 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I1001 10:00:24.312212 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I1001 10:00:24.312262 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I1001 10:00:24.312311 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I1001 10:00:24.312361 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I1001 10:00:24.312410 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I1001 10:00:24.312459 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I1001 10:00:24.312509 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I1001 10:00:24.312558 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I1001 10:00:24.312608 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I1001 10:00:24.312657 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I1001 10:00:24.312707 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I1001 10:00:24.312757 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I1001 10:00:24.312806 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I1001 10:00:24.312856 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I1001 10:00:24.312905 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I1001 10:00:24.312955 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I1001 10:00:24.313004 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I1001 10:00:24.313053 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I1001 10:00:24.313102 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I1001 10:00:24.313152 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I1001 10:00:24.313201 140265095153472 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I1001 10:00:28.930187 140265095153472 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 10:00:36.156698 140265095153472 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 10:00:41.126882 140265095153472 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I1001 10:00:48.070958 140265095153472 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I1001 10:00:58.207993 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I1001 10:00:58.208321 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.208416 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.208499 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.208571 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.208644 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.208711 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.208777 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.208842 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.208909 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.208972 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.209038 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.209100 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.209165 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.209225 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.209290 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.209357 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.209419 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.209480 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.209540 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.209604 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.209699 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.209770 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.209832 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.209892 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.209973 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.210082 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.210191 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.210308 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.210418 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.210546 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.210662 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.210784 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.210931 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.211076 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.211218 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.211323 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.211436 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.211535 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.211644 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.211745 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.211862 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.211977 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.212101 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.212216 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.212332 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.212440 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.212563 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.212676 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.212798 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.212908 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.213023 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.213135 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.213246 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.213359 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.213469 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.213585 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.213689 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.213792 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.213905 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.214034 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.214143 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.214260 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.214370 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.214483 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.214591 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.214710 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.214826 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.214933 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.215049 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.215177 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.215294 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.215408 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.215529 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.215636 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.215750 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.215866 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.215973 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.216089 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.216203 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.216278 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.216379 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.216487 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.216607 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.216715 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.216794 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.216892 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.217006 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.217125 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.217237 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.217365 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.217475 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.217576 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.217681 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.217800 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.217911 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.218022 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.218120 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.218220 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.218320 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.218416 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.218517 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.218611 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.218682 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.218793 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.218897 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.218993 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.219115 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.219220 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.219290 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.219379 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.219468 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.219564 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.219666 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.219764 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.219847 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.219948 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.220043 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.220129 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.220210 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.220283 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.220350 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.220424 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.220520 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.220605 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.220707 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.220774 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.220838 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.220908 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.221012 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.221114 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.221209 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.221309 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.221410 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.221501 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.221596 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.221688 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.221784 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.221868 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.221955 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.222059 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.222164 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.222240 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.222332 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.222414 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.222519 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.222619 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.222703 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.222817 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.222922 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.223019 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.223133 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.223217 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.223319 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.223403 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.223495 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.223589 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.223686 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.223784 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.223857 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.223957 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.224048 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.224141 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.224260 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.224356 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.224425 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.224524 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.224616 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.224716 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.224787 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.224851 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.224915 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.224979 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.225048 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.225126 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.225194 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.225260 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.225324 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.225395 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.225473 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.225538 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.225603 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.225663 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.225735 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.225800 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.225871 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.225932 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.225998 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.226061 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.226131 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.226199 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.226268 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.226346 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.226412 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.226478 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.226546 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.226612 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.226681 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.226752 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.226814 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.226878 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.226942 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.227010 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.227077 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.227162 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.227228 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.227293 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.227365 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.227433 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.227500 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.227570 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.227635 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.227704 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.227780 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.227851 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.227918 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.227993 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.228067 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.228130 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.228193 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.228255 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.228321 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.228391 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.228461 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.228530 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.228595 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.228659 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.228722 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.228783 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.228851 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.228918 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.228987 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.229061 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.229132 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.229202 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.229264 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.229339 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.229402 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.229463 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.229526 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.229588 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.229652 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.229717 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.229782 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.229847 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.229917 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.229985 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.230054 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.230124 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.230202 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.230269 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.230341 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.230406 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.230474 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.230551 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.230621 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.230685 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.230750 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.230822 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.230888 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.230959 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.231022 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.231087 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.231179 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.231255 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.231321 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.231399 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.231464 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.231531 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.231594 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.231664 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.231734 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.231805 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.231868 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.231934 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.232000 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.232064 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.232131 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.232200 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.232270 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.232341 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.232411 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.232478 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.232545 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.232608 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.232675 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.232739 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.232809 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.232879 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.232942 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.233010 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.233077 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.233143 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.233207 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.233273 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.233342 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.233407 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.233469 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.233543 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.233614 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.233681 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.233741 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.233808 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.233870 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.233940 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.234007 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.234077 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.234154 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.234216 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.234288 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.234359 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.234424 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.234488 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.234549 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.234610 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.234679 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.234744 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.234817 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.234884 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.234956 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.235028 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.235115 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.235189 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.235258 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.235333 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.235405 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.235474 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.235541 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.235609 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.235673 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.235737 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.235800 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.235876 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.235944 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.236011 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.236079 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.236150 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.236211 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.236277 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.236346 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.236409 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.236473 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.236543 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.236622 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.236688 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.236760 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.236825 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.236888 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.236955 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.237023 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.237091 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.237160 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.237228 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.237312 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.237378 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.237445 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.237513 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.237582 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.237644 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.237706 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.237773 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.237838 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.237903 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.237973 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.238035 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.238096 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.238170 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.238233 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.238306 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.238373 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.238440 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.238505 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.238579 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.238650 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.238716 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.238780 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.238850 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.238921 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.238983 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.239051 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.239136 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.239201 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.239266 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.239341 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.239412 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.239487 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.239552 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.239618 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.239680 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.239757 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.239851 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.239931 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.240017 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.240085 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.240162 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.240251 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.240324 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.240417 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.240492 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.240574 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.240668 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.240740 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.240828 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.240916 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.240988 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.241075 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.241163 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.241249 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.241317 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.241401 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.241483 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.241563 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.241652 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.241724 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.241796 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.241867 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.241933 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.242009 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.242082 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.242152 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.242217 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.242286 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.242348 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.242420 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.242486 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.242551 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.242614 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.242690 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.242760 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.242824 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.242892 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.242956 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.243026 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.243113 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.243191 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.243262 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.243338 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.243404 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.243473 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.243542 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.243615 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.243690 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.243761 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.243832 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.243898 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.243965 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.244029 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.244101 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.244166 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.244236 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.244301 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.244370 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.244435 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.244502 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.244572 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.244640 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.244714 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.244786 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.244851 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.244914 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.244975 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.245046 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.245121 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.245188 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.245253 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.245321 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.245391 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.245462 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.245532 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.245602 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.245672 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.245746 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.245816 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.245880 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.245951 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.246021 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.246090 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.246159 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.246223 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.246297 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.246365 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.246431 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.246501 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.246566 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.246631 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.246711 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.246783 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.246856 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.246927 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.247001 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.247071 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.247159 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.247236 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.247305 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.247368 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.247436 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.247505 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.247569 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.247636 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.247704 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.247779 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.247849 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.247923 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.247989 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.248063 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.248132 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.248198 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.248261 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.248331 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.248394 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.248456 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.248524 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.248591 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.248658 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.248722 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.248800 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.248871 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.248938 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.249002 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.249076 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.249146 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.249214 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.249279 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.249350 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.249414 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.249488 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.249552 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.249623 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.249693 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I1001 10:00:58.249767 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I1001 10:00:58.249842 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.249914 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I1001 10:00:58.249981 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.250046 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.250110 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I1001 10:00:58.250180 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.250254 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.250325 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.250394 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.250459 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.250533 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.250599 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I1001 10:00:58.250668 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.250737 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.250808 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I1001 10:00:58.250880 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.250948 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251011 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251077 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251164 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251235 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251304 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251370 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251434 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251498 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251569 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251634 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251703 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251769 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251837 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251901 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I1001 10:00:58.251965 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252040 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252110 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252179 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252248 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252321 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252390 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252463 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252531 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252605 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252674 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252743 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252811 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252879 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.252951 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:00:58.253019 140265095153472 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I1001 10:01:08.065779 140265095153472 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f9094de46a0>>
I1001 10:01:11.627351 140265095153472 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I1001 10:01:11.630538 140265095153472 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I1001 10:01:11.635469 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I1001 10:01:11.640738 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I1001 10:01:11.645846 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I1001 10:01:11.650889 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I1001 10:01:11.656022 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I1001 10:01:11.661054 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I1001 10:01:11.666155 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I1001 10:01:11.671196 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I1001 10:01:11.676305 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I1001 10:01:11.681318 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I1001 10:01:11.687162 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I1001 10:01:11.692211 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I1001 10:01:11.697345 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I1001 10:01:11.702423 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I1001 10:01:11.706269 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I1001 10:01:11.710032 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I1001 10:01:11.715073 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I1001 10:01:11.720113 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I1001 10:01:11.747450 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I1001 10:01:11.752541 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I1001 10:01:11.757697 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I1001 10:01:11.762794 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I1001 10:01:11.767860 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I1001 10:01:11.773005 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I1001 10:01:11.778007 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I1001 10:01:11.783177 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I1001 10:01:11.788177 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I1001 10:01:11.793318 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I1001 10:01:11.798342 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I1001 10:01:11.803534 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I1001 10:01:11.808551 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I1001 10:01:11.813661 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I1001 10:01:11.819364 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I1001 10:01:11.824435 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I1001 10:01:11.829535 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I1001 10:01:11.834554 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I1001 10:01:11.839703 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I1001 10:01:11.844736 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I1001 10:01:11.849824 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I1001 10:01:11.854863 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I1001 10:01:11.860001 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I1001 10:01:11.865017 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I1001 10:01:11.870093 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I1001 10:01:11.875195 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I1001 10:01:11.880278 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I1001 10:01:11.885425 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I1001 10:01:11.890420 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I1001 10:01:11.895589 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I1001 10:01:11.899460 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I1001 10:01:11.903257 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I1001 10:01:11.908271 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I1001 10:01:11.913307 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I1001 10:01:11.918416 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I1001 10:01:11.923667 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I1001 10:01:11.928723 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I1001 10:01:11.934440 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I1001 10:01:11.939468 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I1001 10:01:11.944624 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I1001 10:01:11.949645 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I1001 10:01:11.954824 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I1001 10:01:11.959866 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I1001 10:01:11.964974 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I1001 10:01:11.970010 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I1001 10:01:11.975205 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I1001 10:01:11.980335 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I1001 10:01:11.985367 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I1001 10:01:11.990492 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I1001 10:01:11.995600 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I1001 10:01:12.000712 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I1001 10:01:12.005738 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I1001 10:01:12.010862 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I1001 10:01:12.015936 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I1001 10:01:12.021074 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I1001 10:01:12.026100 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I1001 10:01:12.031266 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I1001 10:01:12.036390 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I1001 10:01:12.041414 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I1001 10:01:12.047138 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I1001 10:01:12.052165 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I1001 10:01:12.057326 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I1001 10:01:12.062425 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I1001 10:01:12.067594 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I1001 10:01:12.071477 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I1001 10:01:12.075253 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I1001 10:01:12.080291 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I1001 10:01:12.085495 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I1001 10:01:12.090561 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I1001 10:01:12.095774 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I1001 10:01:12.100803 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I1001 10:01:12.105975 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I1001 10:01:12.111002 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I1001 10:01:12.116184 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I1001 10:01:12.121302 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I1001 10:01:12.126434 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I1001 10:01:12.131506 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I1001 10:01:12.136651 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I1001 10:01:12.141821 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I1001 10:01:12.146871 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I1001 10:01:12.152046 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I1001 10:01:12.157085 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I1001 10:01:12.162739 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I1001 10:01:12.167864 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I1001 10:01:12.172984 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I1001 10:01:12.178037 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I1001 10:01:12.183198 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I1001 10:01:12.188249 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I1001 10:01:12.193364 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I1001 10:01:12.198413 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I1001 10:01:12.203536 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I1001 10:01:12.208657 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I1001 10:01:12.213673 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I1001 10:01:12.218893 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I1001 10:01:12.223929 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I1001 10:01:12.229067 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I1001 10:01:12.234104 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I1001 10:01:12.239276 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I1001 10:01:12.243162 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I1001 10:01:12.246929 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I1001 10:01:12.252045 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I1001 10:01:12.257205 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I1001 10:01:12.262232 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I1001 10:01:12.267400 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I1001 10:01:12.272470 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I1001 10:01:12.278253 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I1001 10:01:12.283339 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I1001 10:01:12.288516 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I1001 10:01:12.293591 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I1001 10:01:12.298724 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I1001 10:01:12.303793 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I1001 10:01:12.308937 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I1001 10:01:12.314106 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I1001 10:01:12.319212 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I1001 10:01:12.324476 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I1001 10:01:12.329522 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I1001 10:01:12.334623 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I1001 10:01:12.339703 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I1001 10:01:12.344820 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I1001 10:01:12.349849 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I1001 10:01:12.355039 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I1001 10:01:12.360097 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I1001 10:01:12.365201 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I1001 10:01:12.370284 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I1001 10:01:12.375489 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I1001 10:01:12.380651 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I1001 10:01:12.385685 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I1001 10:01:12.391371 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I1001 10:01:12.396384 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I1001 10:01:12.401494 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I1001 10:01:12.406549 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I1001 10:01:12.411760 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I1001 10:01:12.415637 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I1001 10:01:12.419412 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I1001 10:01:12.424496 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I1001 10:01:12.429630 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I1001 10:01:12.434680 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I1001 10:01:12.439840 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I1001 10:01:12.444869 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I1001 10:01:12.450026 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I1001 10:01:12.455059 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I1001 10:01:12.460219 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I1001 10:01:12.465239 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I1001 10:01:12.470350 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I1001 10:01:12.475389 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I1001 10:01:12.480657 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I1001 10:01:12.485786 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I1001 10:01:12.490813 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I1001 10:01:12.495954 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I1001 10:01:12.501002 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I1001 10:01:12.506689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I1001 10:01:12.511731 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I1001 10:01:12.516833 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I1001 10:01:12.521895 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I1001 10:01:12.527011 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I1001 10:01:12.532149 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I1001 10:01:12.537219 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I1001 10:01:12.542217 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I1001 10:01:12.547309 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I1001 10:01:12.552412 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I1001 10:01:12.557483 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I1001 10:01:12.562599 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I1001 10:01:12.567597 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I1001 10:01:12.572692 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I1001 10:01:12.577666 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I1001 10:01:12.582806 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I1001 10:01:12.586663 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I1001 10:01:12.590394 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I1001 10:01:12.595421 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I1001 10:01:12.600508 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I1001 10:01:12.605487 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I1001 10:01:12.610584 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I1001 10:01:12.615617 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I1001 10:01:12.621274 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I1001 10:01:12.626261 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I1001 10:01:12.631367 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I1001 10:01:12.636333 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I1001 10:01:12.641409 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I1001 10:01:12.646381 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I1001 10:01:12.651468 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I1001 10:01:12.656565 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I1001 10:01:12.661696 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I1001 10:01:12.666756 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I1001 10:01:12.671777 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I1001 10:01:12.676831 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I1001 10:01:12.681840 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I1001 10:01:12.686934 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I1001 10:01:12.691950 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I1001 10:01:12.697053 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I1001 10:01:12.702045 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I1001 10:01:12.707158 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I1001 10:01:12.712230 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I1001 10:01:12.717315 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I1001 10:01:12.722450 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I1001 10:01:12.727457 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I1001 10:01:12.733116 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I1001 10:01:12.738117 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I1001 10:01:12.743245 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I1001 10:01:12.748252 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I1001 10:01:12.753333 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I1001 10:01:12.757171 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I1001 10:01:12.760929 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I1001 10:01:12.765975 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I1001 10:01:12.771070 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I1001 10:01:12.776097 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I1001 10:01:12.781216 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I1001 10:01:12.786322 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I1001 10:01:12.791434 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I1001 10:01:12.796455 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I1001 10:01:12.801548 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I1001 10:01:12.806549 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I1001 10:01:12.811692 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I1001 10:01:12.816704 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I1001 10:01:12.821789 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I1001 10:01:12.826887 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I1001 10:01:12.831927 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I1001 10:01:12.837012 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I1001 10:01:12.842027 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I1001 10:01:12.847652 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I1001 10:01:12.852696 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I1001 10:01:12.857827 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I1001 10:01:12.862841 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I1001 10:01:12.868014 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I1001 10:01:12.873059 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I1001 10:01:12.878190 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I1001 10:01:12.883228 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I1001 10:01:12.888339 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I1001 10:01:12.893479 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I1001 10:01:12.898470 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I1001 10:01:12.903600 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I1001 10:01:12.908592 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I1001 10:01:12.913689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I1001 10:01:12.918684 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I1001 10:01:12.923860 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I1001 10:01:12.927697 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I1001 10:01:12.931439 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I1001 10:01:12.936452 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I1001 10:01:12.941608 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I1001 10:01:12.946642 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I1001 10:01:12.951756 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I1001 10:01:12.956755 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I1001 10:01:12.962404 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I1001 10:01:12.967474 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I1001 10:01:12.972588 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I1001 10:01:12.977601 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I1001 10:01:12.982721 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I1001 10:01:12.987749 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I1001 10:01:12.992872 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I1001 10:01:12.998002 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I1001 10:01:13.003119 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I1001 10:01:13.008296 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I1001 10:01:13.013337 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I1001 10:01:13.018504 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I1001 10:01:13.023616 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I1001 10:01:13.028729 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I1001 10:01:13.033749 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I1001 10:01:13.038864 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I1001 10:01:13.043916 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I1001 10:01:13.048979 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I1001 10:01:13.053996 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I1001 10:01:13.059114 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I1001 10:01:13.064227 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I1001 10:01:13.069223 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I1001 10:01:13.074887 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I1001 10:01:13.079919 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I1001 10:01:13.085000 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I1001 10:01:13.089985 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I1001 10:01:13.095113 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I1001 10:01:13.098951 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I1001 10:01:13.102746 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I1001 10:01:13.107795 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I1001 10:01:13.112918 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I1001 10:01:13.117925 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I1001 10:01:13.123156 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I1001 10:01:13.128185 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I1001 10:01:13.133317 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I1001 10:01:13.138322 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I1001 10:01:13.143444 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I1001 10:01:13.148565 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I1001 10:01:13.153709 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I1001 10:01:13.158847 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I1001 10:01:13.163946 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I1001 10:01:13.169065 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I1001 10:01:13.174204 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I1001 10:01:13.179341 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I1001 10:01:13.184379 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I1001 10:01:13.190029 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I1001 10:01:13.195061 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I1001 10:01:13.200246 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I1001 10:01:13.205266 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I1001 10:01:13.210398 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I1001 10:01:13.215434 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I1001 10:01:13.220542 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I1001 10:01:13.225566 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I1001 10:01:13.230707 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I1001 10:01:13.235847 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I1001 10:01:13.240874 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I1001 10:01:13.246020 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I1001 10:01:13.251023 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I1001 10:01:13.256190 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I1001 10:01:13.261181 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I1001 10:01:13.266281 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I1001 10:01:13.270152 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I1001 10:01:13.273979 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I1001 10:01:13.279022 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I1001 10:01:13.284177 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I1001 10:01:13.289205 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I1001 10:01:13.294331 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I1001 10:01:13.299425 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I1001 10:01:13.305164 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I1001 10:01:13.310206 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I1001 10:01:13.315347 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I1001 10:01:13.320360 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I1001 10:01:13.325567 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I1001 10:01:13.330592 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I1001 10:01:13.335737 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I1001 10:01:13.340869 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I1001 10:01:13.345911 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I1001 10:01:13.351013 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I1001 10:01:13.356100 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I1001 10:01:13.361228 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I1001 10:01:13.366313 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I1001 10:01:13.371500 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I1001 10:01:13.376533 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I1001 10:01:13.381728 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I1001 10:01:13.386759 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I1001 10:01:13.391874 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I1001 10:01:13.396888 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I1001 10:01:13.402029 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I1001 10:01:13.407268 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I1001 10:01:13.412286 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I1001 10:01:13.417951 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I1001 10:01:13.422949 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I1001 10:01:13.428113 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I1001 10:01:13.433117 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I1001 10:01:13.438254 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I1001 10:01:13.442121 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I1001 10:01:13.445902 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I1001 10:01:13.450966 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I1001 10:01:13.456186 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I1001 10:01:13.461214 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I1001 10:01:13.466368 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I1001 10:01:13.471426 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I1001 10:01:13.476589 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I1001 10:01:13.481588 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I1001 10:01:13.486722 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I1001 10:01:13.491783 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I1001 10:01:13.496905 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I1001 10:01:13.501957 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I1001 10:01:13.507149 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I1001 10:01:13.512254 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I1001 10:01:13.517285 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I1001 10:01:13.522480 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I1001 10:01:13.527540 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I1001 10:01:13.533186 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I1001 10:01:13.538256 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I1001 10:01:13.543403 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I1001 10:01:13.548420 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I1001 10:01:13.553538 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I1001 10:01:13.558585 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I1001 10:01:13.563730 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I1001 10:01:13.568770 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I1001 10:01:13.573864 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I1001 10:01:13.578991 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I1001 10:01:13.584047 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I1001 10:01:13.589168 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I1001 10:01:13.594210 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I1001 10:01:13.599420 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I1001 10:01:13.604454 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I1001 10:01:13.609602 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I1001 10:01:13.613471 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I1001 10:01:13.617249 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I1001 10:01:13.622309 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I1001 10:01:13.627466 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I1001 10:01:13.632508 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I1001 10:01:13.637683 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I1001 10:01:13.642709 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I1001 10:01:13.648414 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I1001 10:01:13.653437 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I1001 10:01:13.658553 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I1001 10:01:13.663597 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I1001 10:01:13.668724 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I1001 10:01:13.673757 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I1001 10:01:13.678875 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I1001 10:01:13.684000 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I1001 10:01:13.689065 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I1001 10:01:13.694161 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I1001 10:01:13.699244 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I1001 10:01:13.704365 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I1001 10:01:13.709441 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I1001 10:01:13.714587 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I1001 10:01:13.719666 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I1001 10:01:13.724896 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I1001 10:01:13.729931 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I1001 10:01:13.735069 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I1001 10:01:13.740235 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I1001 10:01:13.745353 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I1001 10:01:13.750492 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I1001 10:01:13.755563 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I1001 10:01:13.761343 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I1001 10:01:13.766374 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I1001 10:01:13.771538 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I1001 10:01:13.776566 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I1001 10:01:13.781712 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I1001 10:01:13.785573 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I1001 10:01:13.789342 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I1001 10:01:13.794381 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I1001 10:01:13.799557 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I1001 10:01:13.804572 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I1001 10:01:13.809723 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I1001 10:01:13.814739 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I1001 10:01:13.819905 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I1001 10:01:13.824921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I1001 10:01:13.830060 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I1001 10:01:13.835063 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I1001 10:01:13.840223 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I1001 10:01:13.845262 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I1001 10:01:13.850396 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I1001 10:01:13.855552 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I1001 10:01:13.860593 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I1001 10:01:13.865751 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I1001 10:01:13.870773 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I1001 10:01:13.876466 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I1001 10:01:13.881559 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I1001 10:01:13.886667 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I1001 10:01:13.891757 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I1001 10:01:13.896902 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I1001 10:01:13.901946 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I1001 10:01:13.907076 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I1001 10:01:13.912137 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I1001 10:01:13.917368 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I1001 10:01:13.922595 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I1001 10:01:13.927650 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I1001 10:01:13.932805 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I1001 10:01:13.937865 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I1001 10:01:13.943115 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I1001 10:01:13.948205 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I1001 10:01:13.953341 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I1001 10:01:13.957231 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I1001 10:01:13.961013 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I1001 10:01:13.966078 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I1001 10:01:13.971280 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I1001 10:01:13.976316 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I1001 10:01:13.981434 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I1001 10:01:13.986451 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I1001 10:01:13.992168 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I1001 10:01:13.997225 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I1001 10:01:14.002347 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I1001 10:01:14.007402 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I1001 10:01:14.012541 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I1001 10:01:14.017582 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I1001 10:01:14.022727 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I1001 10:01:14.027913 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I1001 10:01:14.032987 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I1001 10:01:14.038113 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I1001 10:01:14.043200 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I1001 10:01:14.048330 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I1001 10:01:14.053417 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I1001 10:01:14.058562 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I1001 10:01:14.063638 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I1001 10:01:14.068772 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I1001 10:01:14.073846 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I1001 10:01:14.078989 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I1001 10:01:14.084095 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I1001 10:01:14.089218 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I1001 10:01:14.094413 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I1001 10:01:14.099517 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I1001 10:01:14.105263 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I1001 10:01:14.110311 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I1001 10:01:14.115518 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I1001 10:01:14.120577 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I1001 10:01:14.125855 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I1001 10:01:14.129740 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I1001 10:01:14.133554 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I1001 10:01:14.138593 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I1001 10:01:14.143797 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I1001 10:01:14.148854 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I1001 10:01:14.154026 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I1001 10:01:14.159073 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I1001 10:01:14.164274 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I1001 10:01:14.169311 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I1001 10:01:14.174476 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I1001 10:01:14.179593 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I1001 10:01:14.184741 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I1001 10:01:14.189773 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I1001 10:01:14.194956 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I1001 10:01:14.200102 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I1001 10:01:14.205313 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I1001 10:01:14.210454 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I1001 10:01:14.215529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I1001 10:01:14.221266 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I1001 10:01:14.226422 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I1001 10:01:14.231793 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I1001 10:01:14.236903 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I1001 10:01:14.242129 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I1001 10:01:14.247252 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I1001 10:01:14.252450 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I1001 10:01:14.257571 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I1001 10:01:14.262725 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I1001 10:01:14.267942 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I1001 10:01:14.272984 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I1001 10:01:14.278159 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I1001 10:01:14.283236 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I1001 10:01:14.288396 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I1001 10:01:14.293460 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I1001 10:01:14.298600 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I1001 10:01:14.302502 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I1001 10:01:14.306303 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I1001 10:01:14.311390 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I1001 10:01:14.316572 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I1001 10:01:14.321620 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I1001 10:01:14.326880 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I1001 10:01:14.331975 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I1001 10:01:14.337714 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I1001 10:01:14.342754 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I1001 10:01:14.347951 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I1001 10:01:14.353017 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I1001 10:01:14.358250 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I1001 10:01:14.363360 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I1001 10:01:14.368549 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I1001 10:01:14.373749 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I1001 10:01:14.378822 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I1001 10:01:14.384042 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I1001 10:01:14.389093 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I1001 10:01:14.394287 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I1001 10:01:14.399477 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I1001 10:01:14.404643 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I1001 10:01:14.409896 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I1001 10:01:14.415070 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I1001 10:01:14.420156 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I1001 10:01:14.425325 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I1001 10:01:14.430377 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I1001 10:01:14.435570 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I1001 10:01:14.440692 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I1001 10:01:14.445754 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I1001 10:01:14.451489 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I1001 10:01:14.456531 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I1001 10:01:14.461705 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I1001 10:01:14.466762 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I1001 10:01:14.471960 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I1001 10:01:14.475865 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I1001 10:01:14.479697 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I1001 10:01:14.484751 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I1001 10:01:14.489993 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I1001 10:01:14.495038 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I1001 10:01:14.500223 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I1001 10:01:14.505270 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I1001 10:01:14.510473 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I1001 10:01:14.515572 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I1001 10:01:14.520726 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I1001 10:01:14.525915 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I1001 10:01:14.531084 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I1001 10:01:14.536145 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I1001 10:01:14.541319 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I1001 10:01:14.546529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I1001 10:01:14.551649 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I1001 10:01:14.556809 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I1001 10:01:14.561883 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I1001 10:01:14.567642 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I1001 10:01:14.572754 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I1001 10:01:14.577894 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I1001 10:01:14.582964 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I1001 10:01:14.588150 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I1001 10:01:14.593254 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I1001 10:01:14.598427 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I1001 10:01:14.603537 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I1001 10:01:14.608680 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I1001 10:01:14.613855 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I1001 10:01:14.618961 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I1001 10:01:14.624173 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I1001 10:01:14.629255 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I1001 10:01:14.634430 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I1001 10:01:14.639509 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I1001 10:01:14.644684 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I1001 10:01:14.648591 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I1001 10:01:14.652405 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I1001 10:01:14.657480 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I1001 10:01:14.662703 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I1001 10:01:14.667810 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I1001 10:01:14.673036 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I1001 10:01:14.678123 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I1001 10:01:14.683937 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I1001 10:01:14.689004 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I1001 10:01:14.694174 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I1001 10:01:14.699281 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I1001 10:01:14.704424 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I1001 10:01:14.709501 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I1001 10:01:14.714663 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I1001 10:01:14.719860 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I1001 10:01:14.725136 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I1001 10:01:14.730298 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I1001 10:01:14.735390 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I1001 10:01:14.740548 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I1001 10:01:14.745660 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I1001 10:01:14.750849 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I1001 10:01:14.755939 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I1001 10:01:14.761132 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I1001 10:01:14.766211 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I1001 10:01:14.771369 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I1001 10:01:14.776510 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I1001 10:01:14.781674 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I1001 10:01:14.786836 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I1001 10:01:14.791924 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I1001 10:01:14.797652 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I1001 10:01:14.802719 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I1001 10:01:14.807936 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I1001 10:01:14.813011 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I1001 10:01:14.818168 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I1001 10:01:14.822087 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I1001 10:01:14.825937 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I1001 10:01:14.831065 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I1001 10:01:14.836280 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I1001 10:01:14.841364 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I1001 10:01:14.846570 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I1001 10:01:14.851689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I1001 10:01:14.856902 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I1001 10:01:14.861971 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I1001 10:01:14.867205 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I1001 10:01:14.872282 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I1001 10:01:14.877506 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I1001 10:01:14.882607 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I1001 10:01:14.887832 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I1001 10:01:14.893024 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I1001 10:01:14.898117 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I1001 10:01:14.903355 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I1001 10:01:14.908427 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I1001 10:01:14.914106 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I1001 10:01:14.919284 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I1001 10:01:14.924527 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I1001 10:01:14.929650 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I1001 10:01:14.934804 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I1001 10:01:14.939933 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I1001 10:01:14.945080 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I1001 10:01:14.950194 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I1001 10:01:14.955386 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I1001 10:01:14.960556 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I1001 10:01:14.965621 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I1001 10:01:14.970786 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I1001 10:01:14.975893 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I1001 10:01:14.981082 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I1001 10:01:14.986148 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I1001 10:01:14.991374 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I1001 10:01:14.995292 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I1001 10:01:14.999090 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I1001 10:01:15.004195 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I1001 10:01:15.009413 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I1001 10:01:15.014475 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I1001 10:01:15.019689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I1001 10:01:15.024777 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I1001 10:01:15.030537 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I1001 10:01:15.035636 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I1001 10:01:15.040815 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I1001 10:01:15.045907 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I1001 10:01:15.051087 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I1001 10:01:15.056195 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I1001 10:01:15.061376 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I1001 10:01:15.066528 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I1001 10:01:15.071670 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I1001 10:01:15.076851 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I1001 10:01:15.081933 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I1001 10:01:15.087152 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I1001 10:01:15.092305 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I1001 10:01:15.097488 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I1001 10:01:15.102563 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I1001 10:01:15.107779 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I1001 10:01:15.112932 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I1001 10:01:15.118140 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I1001 10:01:15.123331 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I1001 10:01:15.128529 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I1001 10:01:15.133720 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I1001 10:01:15.138811 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I1001 10:01:15.144574 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I1001 10:01:15.149676 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I1001 10:01:15.154903 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I1001 10:01:15.160010 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I1001 10:01:15.165210 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I1001 10:01:15.169129 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I1001 10:01:15.172975 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I1001 10:01:15.178081 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I1001 10:01:15.183318 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I1001 10:01:15.188418 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I1001 10:01:15.193689 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I1001 10:01:15.198783 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I1001 10:01:15.204023 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I1001 10:01:15.209089 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I1001 10:01:15.214259 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I1001 10:01:15.219388 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I1001 10:01:15.224607 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I1001 10:01:15.229732 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I1001 10:01:15.234994 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I1001 10:01:15.240231 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I1001 10:01:15.245396 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I1001 10:01:15.250585 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I1001 10:01:15.255700 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I1001 10:01:15.261437 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I1001 10:01:15.266589 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I1001 10:01:15.271821 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I1001 10:01:15.276901 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I1001 10:01:15.282070 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I1001 10:01:15.287230 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I1001 10:01:15.292411 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I1001 10:01:15.297569 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I1001 10:01:15.302750 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I1001 10:01:15.307959 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I1001 10:01:15.313049 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I1001 10:01:15.318280 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I1001 10:01:15.323452 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I1001 10:01:15.328742 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I1001 10:01:15.333880 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I1001 10:01:15.339178 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I1001 10:01:15.343129 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I1001 10:01:15.346961 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I1001 10:01:15.352094 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I1001 10:01:15.357384 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I1001 10:01:15.362482 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I1001 10:01:15.367734 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I1001 10:01:15.372939 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I1001 10:01:15.378690 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I1001 10:01:15.383836 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I1001 10:01:15.389052 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I1001 10:01:15.394138 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I1001 10:01:15.399410 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I1001 10:01:15.404495 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I1001 10:01:15.409776 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I1001 10:01:15.414965 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I1001 10:01:15.420110 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I1001 10:01:15.425339 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I1001 10:01:15.430420 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I1001 10:01:15.435623 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I1001 10:01:15.440761 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I1001 10:01:15.445953 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I1001 10:01:15.451073 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I1001 10:01:15.456324 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I1001 10:01:15.461452 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I1001 10:01:15.466642 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I1001 10:01:15.471785 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I1001 10:01:15.476990 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I1001 10:01:15.482183 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I1001 10:01:15.487289 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I1001 10:01:15.493030 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I1001 10:01:15.498128 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I1001 10:01:15.503399 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I1001 10:01:15.508457 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I1001 10:01:15.513672 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I1001 10:01:15.517610 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I1001 10:01:15.521465 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I1001 10:01:15.526668 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I1001 10:01:15.531970 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I1001 10:01:15.537086 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I1001 10:01:15.542297 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I1001 10:01:15.547422 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I1001 10:01:15.552683 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I1001 10:01:15.557802 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I1001 10:01:15.563019 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I1001 10:01:15.568114 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I1001 10:01:15.573327 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I1001 10:01:15.578411 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I1001 10:01:15.583708 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I1001 10:01:15.588917 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I1001 10:01:15.594094 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I1001 10:01:15.599318 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I1001 10:01:15.604432 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I1001 10:01:15.610197 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I1001 10:01:15.615351 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I1001 10:01:15.620554 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I1001 10:01:15.625663 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I1001 10:01:15.630875 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I1001 10:01:15.636025 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I1001 10:01:15.641196 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I1001 10:01:15.646391 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I1001 10:01:15.651582 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I1001 10:01:15.656771 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I1001 10:01:15.661925 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I1001 10:01:15.667144 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I1001 10:01:15.672282 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I1001 10:01:15.677559 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I1001 10:01:15.682683 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I1001 10:01:15.687960 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I1001 10:01:15.691921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I1001 10:01:15.695800 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I1001 10:01:15.700931 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I1001 10:01:15.706168 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I1001 10:01:15.711266 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I1001 10:01:15.716495 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I1001 10:01:15.721618 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I1001 10:01:15.727580 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I1001 10:01:15.732688 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I1001 10:01:15.737879 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I1001 10:01:15.743028 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I1001 10:01:15.748252 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I1001 10:01:15.753364 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I1001 10:01:15.758563 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I1001 10:01:15.763784 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I1001 10:01:15.768908 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I1001 10:01:15.774135 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I1001 10:01:15.779253 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I1001 10:01:15.784457 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I1001 10:01:15.789610 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I1001 10:01:15.794815 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I1001 10:01:15.799947 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I1001 10:01:15.805186 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I1001 10:01:15.810340 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I1001 10:01:15.815587 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I1001 10:01:15.820752 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I1001 10:01:15.825986 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I1001 10:01:15.831207 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I1001 10:01:15.836319 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I1001 10:01:15.842090 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I1001 10:01:15.847302 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I1001 10:01:15.852535 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I1001 10:01:15.857626 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I1001 10:01:15.862868 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I1001 10:01:15.866819 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I1001 10:01:15.870726 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I1001 10:01:15.875866 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I1001 10:01:15.881090 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I1001 10:01:15.886182 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I1001 10:01:15.891430 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I1001 10:01:15.896668 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I1001 10:01:15.901885 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I1001 10:01:15.907020 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I1001 10:01:15.912239 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I1001 10:01:15.917393 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I1001 10:01:15.922606 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I1001 10:01:15.927870 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I1001 10:01:15.933082 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I1001 10:01:15.938293 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I1001 10:01:15.943458 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I1001 10:01:15.948714 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I1001 10:01:15.953852 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I1001 10:01:15.959646 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I1001 10:01:15.964828 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I1001 10:01:15.970063 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I1001 10:01:15.975231 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I1001 10:01:15.980479 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I1001 10:01:15.985615 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I1001 10:01:15.990812 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I1001 10:01:15.995993 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I1001 10:01:16.001183 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I1001 10:01:16.006410 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I1001 10:01:16.011561 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I1001 10:01:16.016794 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I1001 10:01:16.021904 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I1001 10:01:16.027159 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I1001 10:01:16.032313 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I1001 10:01:16.037559 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I1001 10:01:16.041545 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I1001 10:01:16.045439 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I1001 10:01:16.050545 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I1001 10:01:16.055859 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I1001 10:01:16.060972 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I1001 10:01:16.066204 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I1001 10:01:16.071373 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I1001 10:01:16.077207 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I1001 10:01:16.082347 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I1001 10:01:16.087575 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I1001 10:01:16.092687 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I1001 10:01:16.097918 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I1001 10:01:16.103060 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I1001 10:01:16.108318 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I1001 10:01:16.113530 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I1001 10:01:16.118678 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I1001 10:01:16.123954 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I1001 10:01:16.129105 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I1001 10:01:16.134331 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I1001 10:01:16.139516 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I1001 10:01:16.144727 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I1001 10:01:16.149863 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I1001 10:01:16.155120 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I1001 10:01:16.160291 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I1001 10:01:16.165511 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I1001 10:01:16.170635 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I1001 10:01:16.175886 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I1001 10:01:16.181071 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I1001 10:01:16.186190 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I1001 10:01:16.191984 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I1001 10:01:16.197130 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I1001 10:01:16.202400 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I1001 10:01:16.207555 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I1001 10:01:16.212824 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I1001 10:01:16.216806 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I1001 10:01:16.220712 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I1001 10:01:16.225845 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I1001 10:01:16.231154 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I1001 10:01:16.236315 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I1001 10:01:16.241600 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I1001 10:01:16.246776 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I1001 10:01:16.252077 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I1001 10:01:16.257216 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I1001 10:01:16.262420 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I1001 10:01:16.267713 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I1001 10:01:16.272927 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I1001 10:01:16.278042 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I1001 10:01:16.283310 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I1001 10:01:16.288505 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I1001 10:01:16.293742 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I1001 10:01:16.298990 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I1001 10:01:16.304151 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I1001 10:01:16.309904 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I1001 10:01:16.315116 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I1001 10:01:16.320367 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I1001 10:01:16.325554 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I1001 10:01:16.330797 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I1001 10:01:16.335990 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I1001 10:01:16.341261 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I1001 10:01:16.346504 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I1001 10:01:16.351815 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I1001 10:01:16.357113 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I1001 10:01:16.362265 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I1001 10:01:16.367564 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I1001 10:01:16.372752 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I1001 10:01:16.378000 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I1001 10:01:16.383149 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I1001 10:01:16.388381 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I1001 10:01:16.392348 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I1001 10:01:16.396255 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I1001 10:01:16.401396 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I1001 10:01:16.406648 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I1001 10:01:16.411884 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I1001 10:01:16.417112 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I1001 10:01:16.422233 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I1001 10:01:16.428056 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I1001 10:01:16.433190 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I1001 10:01:16.438445 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I1001 10:01:16.443594 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I1001 10:01:16.448852 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I1001 10:01:16.453971 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I1001 10:01:16.459235 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I1001 10:01:16.464478 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I1001 10:01:16.469624 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I1001 10:01:16.474884 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I1001 10:01:16.480041 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I1001 10:01:16.485305 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I1001 10:01:16.490515 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I1001 10:01:16.495781 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I1001 10:01:16.500934 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I1001 10:01:16.506202 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I1001 10:01:16.511372 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I1001 10:01:16.516606 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I1001 10:01:16.521756 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I1001 10:01:16.527058 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I1001 10:01:16.532358 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I1001 10:01:16.537516 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I1001 10:01:16.543326 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I1001 10:01:16.548485 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I1001 10:01:16.553710 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I1001 10:01:16.558941 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I1001 10:01:16.564214 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I1001 10:01:16.568205 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I1001 10:01:16.572124 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I1001 10:01:16.577259 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I1001 10:01:16.582535 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I1001 10:01:16.587719 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I1001 10:01:16.592969 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I1001 10:01:16.598107 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I1001 10:01:16.603405 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I1001 10:01:16.608569 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I1001 10:01:16.613831 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I1001 10:01:16.618984 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I1001 10:01:16.624261 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I1001 10:01:16.629401 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I1001 10:01:16.634685 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I1001 10:01:16.640005 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I1001 10:01:16.645203 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I1001 10:01:16.650429 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I1001 10:01:16.655591 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I1001 10:01:16.661446 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I1001 10:01:16.666662 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I1001 10:01:16.671943 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I1001 10:01:16.677115 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I1001 10:01:16.682357 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I1001 10:01:16.687552 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I1001 10:01:16.692780 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I1001 10:01:16.697941 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I1001 10:01:16.703215 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I1001 10:01:16.708472 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I1001 10:01:16.713597 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I1001 10:01:16.718844 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I1001 10:01:16.723992 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I1001 10:01:16.729377 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I1001 10:01:16.734520 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I1001 10:01:16.739831 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I1001 10:01:16.743851 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I1001 10:01:16.747798 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I1001 10:01:16.752965 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I1001 10:01:16.758289 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I1001 10:01:16.763471 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I1001 10:01:16.768749 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I1001 10:01:16.773916 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I1001 10:01:16.779785 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I1001 10:01:16.784911 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I1001 10:01:16.790190 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I1001 10:01:16.795400 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I1001 10:01:16.800655 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I1001 10:01:16.805788 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I1001 10:01:16.811031 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I1001 10:01:16.816286 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I1001 10:01:16.821466 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I1001 10:01:16.826745 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I1001 10:01:16.831921 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I1001 10:01:16.837182 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I1001 10:01:16.842396 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I1001 10:01:16.847656 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I1001 10:01:16.852808 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I1001 10:01:16.858069 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I1001 10:01:16.863283 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I1001 10:01:16.868525 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I1001 10:01:16.873697 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I1001 10:01:16.878958 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I1001 10:01:16.884230 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I1001 10:01:16.889385 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I1001 10:01:16.895229 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I1001 10:01:16.900418 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I1001 10:01:16.905705 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I1001 10:01:16.910877 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I1001 10:01:16.916150 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I1001 10:01:16.920143 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I1001 10:01:16.924062 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I1001 10:01:16.929316 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I1001 10:01:16.934586 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I1001 10:01:16.939798 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I1001 10:01:16.945061 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I1001 10:01:16.950238 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I1001 10:01:16.955534 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I1001 10:01:16.960699 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I1001 10:01:16.965944 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I1001 10:01:16.971122 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I1001 10:01:16.976389 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I1001 10:01:16.981574 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I1001 10:01:16.986842 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I1001 10:01:16.992146 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I1001 10:01:16.997329 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I1001 10:01:17.002587 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I1001 10:01:17.007773 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I1001 10:01:17.013571 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I1001 10:01:17.018772 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I1001 10:01:17.024061 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I1001 10:01:17.029213 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I1001 10:01:17.034483 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I1001 10:01:17.039688 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I1001 10:01:17.044960 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I1001 10:01:17.050145 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I1001 10:01:17.055403 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I1001 10:01:17.060764 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I1001 10:01:17.065966 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I1001 10:01:17.071284 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I1001 10:01:17.076454 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I1001 10:01:17.081754 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I1001 10:01:17.086881 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I1001 10:01:17.092164 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I1001 10:01:17.096170 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I1001 10:01:17.100115 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I1001 10:01:17.105302 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I1001 10:01:17.110626 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I1001 10:01:17.115815 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I1001 10:01:17.121095 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I1001 10:01:17.126348 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I1001 10:01:17.132187 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I1001 10:01:17.137399 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I1001 10:01:17.142646 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I1001 10:01:17.147830 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I1001 10:01:17.153115 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I1001 10:01:17.158277 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I1001 10:01:17.163590 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I1001 10:01:17.168862 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I1001 10:01:17.174041 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I1001 10:01:17.179345 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I1001 10:01:17.184515 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I1001 10:01:17.189829 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I1001 10:01:17.195084 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I1001 10:01:17.200369 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I1001 10:01:17.205549 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I1001 10:01:17.210817 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I1001 10:01:17.216083 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I1001 10:01:17.221331 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I1001 10:01:17.226478 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I1001 10:01:17.231779 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I1001 10:01:17.237031 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I1001 10:01:17.242249 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I1001 10:01:17.248165 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I1001 10:01:17.253341 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I1001 10:01:17.258618 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I1001 10:01:17.263856 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I1001 10:01:17.269120 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I1001 10:01:17.274316 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I1001 10:01:17.279592 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I1001 10:01:17.284747 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I1001 10:01:17.290007 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I1001 10:01:17.295159 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I1001 10:01:17.300436 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I1001 10:01:17.305696 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I1001 10:01:17.310870 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I1001 10:01:17.316201 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I1001 10:01:17.321392 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I1001 10:01:17.326768 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I1001 10:01:17.331946 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I1001 10:01:17.337213 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I1001 10:01:17.342381 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I1001 10:01:17.347648 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I1001 10:01:17.352851 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I1001 10:01:17.358148 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I1001 10:01:17.363362 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I1001 10:01:17.369291 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I1001 10:01:17.374592 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I1001 10:01:17.379812 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I1001 10:01:17.385198 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I1001 10:01:17.390396 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I1001 10:01:17.395699 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I1001 10:01:17.400866 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I1001 10:01:17.406109 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I1001 10:01:17.411454 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I1001 10:01:17.416720 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I1001 10:01:17.421915 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I1001 10:01:17.427222 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I1001 10:01:17.432511 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I1001 10:01:17.437768 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I1001 10:01:17.443041 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I1001 10:01:17.448248 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I1001 10:01:17.453533 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I1001 10:01:17.458696 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I1001 10:01:17.464063 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I1001 10:01:17.469269 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I1001 10:01:17.474563 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I1001 10:01:17.479784 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I1001 10:01:17.485639 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I1001 10:01:17.490901 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I1001 10:01:17.496128 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I1001 10:01:17.501419 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I1001 10:01:17.506606 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I1001 10:01:17.511885 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I1001 10:01:17.517084 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I1001 10:01:17.522350 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I1001 10:01:17.527648 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I1001 10:01:17.532906 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I1001 10:01:17.538083 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I1001 10:01:17.543388 140265095153472 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I1001 10:01:18.287602 140265095153472 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I1001 10:01:18.289844 140265095153472 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I1001 10:01:18.495880 140265095153472 trainer.py:401] Trainer number of enqueue ops: 0
I1001 10:01:18.496108 140265095153472 trainer.py:410] AttributeError. Expected for single task models.
I1001 10:01:38.776100 140265095153472 trainer.py:1590] Starting runners
I1001 10:01:38.776768 140258454193920 base_runner.py:167] controller started.
I1001 10:01:38.777118 140258445801216 base_runner.py:167] trainer started.
I1001 10:01:38.777222 140265095153472 trainer.py:1609] Waiting for runners to finish...
I1001 10:02:02.032956 140258454193920 checkpointer.py:133] Uninitialized var list: [b'global_step', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var', b'1bwds_wpm_level_lm/total_samples/var', b'beta1_power', b'beta2_power', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1', b'total_nan_gradients/var'] 
I1001 10:02:02.034442 140258454193920 checkpointer.py:140] Initialize ALL variables: [b'global_step', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var', b'1bwds_wpm_level_lm/total_samples/var', b'beta1_power', b'beta2_power', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1', b'total_nan_gradients/var']
I1001 10:03:25.074203 140258445801216 base_runner.py:106] step:     0
I1001 10:03:35.273411 140258454193920 checkpointer.py:142] Initialize variables done.
I1001 10:03:40.931801 140258454193920 trainer.py:380] Steps/second: 0.000000, Examples/second: 0.000000
I1001 10:03:40.932534 140258454193920 checkpointer.py:111] Save checkpoint
2019-10-01 10:05:59.810454: I ./lingvo/core/ops/input_common.h:71] Create RecordProcessor
2019-10-01 10:05:59.811682: I lingvo/core/ops/input_common.cc:33] Input source weights are empty, fall back to legacy behavior.
2019-10-01 10:05:59.812058: I lingvo/core/ops/record_yielder.cc:324] 0x7f82e0657050 Record yielder start
2019-10-01 10:05:59.812097: I ./lingvo/core/ops/input_common.h:76] Create batcher
2019-10-01 10:05:59.812131: I lingvo/core/ops/record_yielder.cc:383] Epoch 1 /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*
2019-10-01 10:05:59.825692: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825759: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825777: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825783: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825820: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825833: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825845: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825857: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825868: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825879: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825891: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825902: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825921: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825934: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825946: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825957: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825969: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825979: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825991: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826003: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826014: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826025: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826037: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826049: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826060: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826072: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826083: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826093: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826104: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826116: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826127: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826144: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826155: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826166: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826177: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826187: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826234: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826246: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826258: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826269: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826278: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826289: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826300: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826311: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826321: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826332: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826343: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826354: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826365: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826375: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826386: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826401: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826412: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826422: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826433: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826444: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826455: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826466: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826476: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826487: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826498: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826509: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826519: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826531: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826542: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826554: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826564: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826575: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826586: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826596: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826612: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826623: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826633: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826644: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826656: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826666: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826676: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826687: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826697: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826708: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826719: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826730: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826741: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826752: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826762: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826773: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826784: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826795: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826806: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826817: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826832: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826843: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826853: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826864: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826874: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826886: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826897: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826907: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826919: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826930: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826941: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826951: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826961: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826971: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826982: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.826994: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827004: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827015: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827026: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827042: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827054: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827065: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827076: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827086: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827129: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827142: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827152: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827163: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827173: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827184: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827195: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827206: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827217: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827228: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827238: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827249: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827260: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827270: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827281: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827298: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827309: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827320: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827331: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827343: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827353: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827364: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827374: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827385: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827396: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827406: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827417: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827428: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827439: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827450: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827461: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827471: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827482: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827492: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827508: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827519: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827530: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827541: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827552: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827563: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827573: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827584: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827595: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827606: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827618: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827628: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827638: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827649: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827660: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827670: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827681: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827692: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827703: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827713: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827762: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827776: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827787: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827799: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827810: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827821: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827832: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827843: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827854: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827864: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827875: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827886: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827896: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827907: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827918: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827928: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827940: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827950: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827961: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827978: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.827988: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828000: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828011: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828021: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828032: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828043: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828053: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828063: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828074: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828085: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828095: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828106: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828117: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828128: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828140: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828150: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828161: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828172: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828183: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828200: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828212: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828223: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828234: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828245: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828255: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828266: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828278: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828288: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828299: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828310: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828320: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828331: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828341: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828352: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828363: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828374: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828386: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828397: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828413: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828424: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828435: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828446: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828457: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828468: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828479: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828491: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828502: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828513: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828524: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828534: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828545: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828556: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828566: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828577: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828588: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828599: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828610: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828621: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828637: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828648: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828659: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828669: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828680: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828691: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828702: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828713: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828723: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828733: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828744: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828754: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828765: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828777: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828788: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828799: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828811: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828821: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828832: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828848: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828860: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828871: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828882: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828892: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828904: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828915: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828925: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828937: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828948: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828959: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828970: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828980: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.828991: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829002: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829012: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829023: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829033: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829044: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829055: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829071: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829083: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829093: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829104: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829146: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829160: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829171: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829183: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829194: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829205: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829216: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829227: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829238: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829249: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829260: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829270: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829282: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829292: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829303: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829320: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829330: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829341: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829352: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829363: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829374: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829385: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829396: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829407: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829418: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829429: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829440: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829451: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829462: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829473: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829484: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829494: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829506: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829517: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829528: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829544: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829556: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829567: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829577: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829587: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829598: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829610: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829621: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829631: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829641: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829652: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829663: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829673: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829684: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829695: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829706: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829717: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829728: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829739: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829755: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829766: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829777: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829788: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829799: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829809: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829819: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829830: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829841: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829852: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829863: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829873: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829883: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829894: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829905: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829916: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829926: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829938: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829949: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829960: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829975: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829986: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.829997: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830008: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830019: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830031: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830041: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830052: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830062: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830073: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830084: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830095: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830105: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830116: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830126: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830138: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830149: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830160: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830170: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830186: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830198: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830208: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830219: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830229: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830240: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830251: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830261: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830273: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830284: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830295: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830305: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830316: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830359: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830373: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830384: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830395: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830407: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830417: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830429: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830445: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830456: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830466: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830477: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830489: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830500: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830511: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830523: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830534: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830545: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830555: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830566: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830576: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830587: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830598: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830608: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830620: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830632: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830643: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830659: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830670: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830682: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830693: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830705: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830718: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830729: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830740: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830750: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830760: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830771: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830782: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830792: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830802: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830814: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830825: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830836: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830846: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830856: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830866: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830882: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830893: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830904: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830915: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830925: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830936: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830947: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830958: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830969: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830979: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.830990: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831002: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831012: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831022: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831033: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831044: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831055: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831068: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831079: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825762: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831202: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831215: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831226: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831237: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831247: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831257: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831270: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831280: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831153: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831303: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831317: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825716: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.825787: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831351: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831323: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831364: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831378: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831388: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831407: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831341: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831400: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831447: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831452: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831461: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831465: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831473: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831477: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831484: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831490: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831496: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831503: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831506: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831528: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831328: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831538: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831589: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831607: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831548: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831627: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831609: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831680: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831530: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831738: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831693: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831691: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831672: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831997: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832011: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832022: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832033: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832044: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832054: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832065: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832076: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832086: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832096: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832108: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832118: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832135: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832146: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832162: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831770: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832148: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832188: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832202: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832212: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832223: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832234: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832244: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832255: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832265: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831592: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831836: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.831563: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832297: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832314: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832320: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832343: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832354: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832364: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832374: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832384: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832396: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832407: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832414: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832419: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832425: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832430: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832436: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832441: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832447: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832452: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832457: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832463: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832468: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832474: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832479: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832489: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832495: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832501: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832506: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832511: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832517: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832522: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832528: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832533: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832539: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832546: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832556: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832565: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832575: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832583: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832589: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832594: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832601: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832607: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832616: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832622: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832627: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832633: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832639: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832644: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832650: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832655: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832661: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832667: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832672: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832677: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832683: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832688: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832694: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832700: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832705: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832711: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832716: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832724: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832739: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832749: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832757: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832763: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832768: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832774: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832779: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832785: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832790: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832796: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832803: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832808: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832814: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832819: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832825: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832830: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832835: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832841: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832846: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832855: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832861: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832866: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832872: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832877: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832882: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832888: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832896: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832907: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832916: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832926: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832932: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832938: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832943: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832949: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832955: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832960: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832966: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832971: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832977: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832986: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832992: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.832997: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833004: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833010: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833015: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833020: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833026: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833031: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833037: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833042: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833047: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833053: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833058: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833065: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833075: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833085: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833095: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833102: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833111: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833117: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833123: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833129: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833134: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833140: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833145: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833151: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833156: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833161: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833167: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833172: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833178: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833183: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833189: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833194: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833201: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833206: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833212: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833217: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833226: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833231: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833239: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833250: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833260: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833269: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833276: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833281: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833287: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833292: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833298: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833303: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833309: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833314: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833320: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833325: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833331: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833336: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833341: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833351: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833357: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833362: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833368: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833373: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833379: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833384: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833390: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833395: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833402: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833409: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833418: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833428: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833438: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833446: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833452: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833457: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833463: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833468: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833474: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833483: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833488: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833494: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833499: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833505: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833511: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833516: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833522: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833527: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833533: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833538: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833544: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833549: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833554: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833560: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833565: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833571: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833576: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833584: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833599: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
2019-10-01 10:05:59.833609: W lingvo/core/ops/record_batcher.cc:274] Not found: /tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled; No such file or directory
