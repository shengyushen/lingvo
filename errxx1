WARNING:tensorflow:

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U "tensorflow==1.*"`

  Otherwise your code may be broken by the change.

  
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0930 03:58:54.286957 140278725625664 model_imports.py:47] Importing lingvo.tasks.asr.params
I0930 03:58:54.318839 140278725625664 model_registry.py:124] Registering models from module: lingvo.tasks.asr.params.librispeech
I0930 03:58:54.325049 140278725625664 model_imports.py:47] Importing lingvo.tasks.car.params
I0930 03:58:54.355368 140278725625664 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.kitti
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

W0930 03:58:54.366403 140278725625664 module_wrapper.py:137] From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

I0930 03:58:54.384610 140278725625664 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.waymo
I0930 03:58:54.390201 140278725625664 model_imports.py:47] Importing lingvo.tasks.image.params
I0930 03:58:54.393038 140278725625664 model_registry.py:124] Registering models from module: lingvo.tasks.image.params.mnist
I0930 03:58:54.393170 140278725625664 model_imports.py:47] Importing lingvo.tasks.lm.params
I0930 03:58:54.395517 140278725625664 model_registry.py:124] Registering models from module: lingvo.tasks.lm.params.one_billion_wds
I0930 03:58:54.398813 140278725625664 model_imports.py:47] Importing lingvo.tasks.mt.params
I0930 03:58:54.405258 140278725625664 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmt14_en_de
I0930 03:58:54.412672 140278725625664 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmtm16_en_de
I0930 03:58:54.413838 140278725625664 model_imports.py:47] Importing lingvo.tasks.punctuator.params
I0930 03:58:54.416202 140278725625664 model_registry.py:124] Registering models from module: lingvo.tasks.punctuator.params.codelab
2019-09-30 03:58:54.416724: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-30 03:58:54.424306: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300050000 Hz
2019-09-30 03:58:54.424883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x51faf00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-09-30 03:58:54.424903: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-09-30 03:58:54.427351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-09-30 03:58:54.568735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:58:54.569806: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5894db0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-09-30 03:58:54.569834: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2019-09-30 03:58:54.570051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:58:54.570952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:1e.0
2019-09-30 03:58:54.571328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-30 03:58:54.573037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-30 03:58:54.574469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-30 03:58:54.574801: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-30 03:58:54.576608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-30 03:58:54.577987: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-30 03:58:54.582165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-30 03:58:54.582273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:58:54.583221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:58:54.584091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-30 03:58:54.584145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-30 03:58:54.585846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-30 03:58:54.585868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-30 03:58:54.585875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-30 03:58:54.586008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:58:54.586964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 03:58:54.587870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:local/replica:0/task:0/device:GPU:0 with 15024 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
2019-09-30 03:58:54.590200: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job local -> {0 -> localhost:34068}
2019-09-30 03:58:54.591856: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:34068
I0930 03:58:54.592300 140278725625664 trainer.py:1501] Job controller start
I0930 03:58:54.694845 140278725625664 base_runner.py:57] ============================================================
I0930 03:58:54.713188 140278725625664 base_runner.py:59] allow_implicit_capture : NoneType
I0930 03:58:54.713403 140278725625664 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I0930 03:58:54.713495 140278725625664 base_runner.py:59] cluster.add_summary : NoneType
I0930 03:58:54.713604 140278725625664 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I0930 03:58:54.713699 140278725625664 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I0930 03:58:54.713778 140278725625664 base_runner.py:59] cluster.controller.devices_per_split : 1
I0930 03:58:54.713853 140278725625664 base_runner.py:59] cluster.controller.gpus_per_replica : 1
I0930 03:58:54.713931 140278725625664 base_runner.py:59] cluster.controller.name : '/job:local'
I0930 03:58:54.714007 140278725625664 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I0930 03:58:54.714081 140278725625664 base_runner.py:59] cluster.controller.replicas : 1
I0930 03:58:54.714159 140278725625664 base_runner.py:59] cluster.controller.targets : ''
I0930 03:58:54.714233 140278725625664 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I0930 03:58:54.714307 140278725625664 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I0930 03:58:54.714385 140278725625664 base_runner.py:59] cluster.decoder.devices_per_split : 1
I0930 03:58:54.714460 140278725625664 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I0930 03:58:54.714533 140278725625664 base_runner.py:59] cluster.decoder.name : '/job:local'
I0930 03:58:54.714607 140278725625664 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I0930 03:58:54.714685 140278725625664 base_runner.py:59] cluster.decoder.replicas : 1
I0930 03:58:54.714756 140278725625664 base_runner.py:59] cluster.decoder.targets : ''
I0930 03:58:54.714837 140278725625664 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I0930 03:58:54.714910 140278725625664 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I0930 03:58:54.714979 140278725625664 base_runner.py:59] cluster.evaler.devices_per_split : 1
I0930 03:58:54.715056 140278725625664 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I0930 03:58:54.715130 140278725625664 base_runner.py:59] cluster.evaler.name : '/job:local'
I0930 03:58:54.715205 140278725625664 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I0930 03:58:54.715277 140278725625664 base_runner.py:59] cluster.evaler.replicas : 1
I0930 03:58:54.715352 140278725625664 base_runner.py:59] cluster.evaler.targets : ''
I0930 03:58:54.715429 140278725625664 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I0930 03:58:54.715502 140278725625664 base_runner.py:59] cluster.input.cpus_per_replica : 1
I0930 03:58:54.715576 140278725625664 base_runner.py:59] cluster.input.devices_per_split : 1
I0930 03:58:54.715648 140278725625664 base_runner.py:59] cluster.input.gpus_per_replica : 0
I0930 03:58:54.715725 140278725625664 base_runner.py:59] cluster.input.name : '/job:local'
I0930 03:58:54.715798 140278725625664 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I0930 03:58:54.715871 140278725625664 base_runner.py:59] cluster.input.replicas : 0
I0930 03:58:54.715948 140278725625664 base_runner.py:59] cluster.input.targets : ''
I0930 03:58:54.716022 140278725625664 base_runner.py:59] cluster.input.tpus_per_replica : 0
I0930 03:58:54.716095 140278725625664 base_runner.py:59] cluster.job : 'controller'
I0930 03:58:54.716170 140278725625664 base_runner.py:59] cluster.logdir : ''
I0930 03:58:54.716246 140278725625664 base_runner.py:59] cluster.mode : 'sync'
I0930 03:58:54.716318 140278725625664 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I0930 03:58:54.716393 140278725625664 base_runner.py:59] cluster.ps.devices_per_split : 1
I0930 03:58:54.716460 140278725625664 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I0930 03:58:54.716528 140278725625664 base_runner.py:59] cluster.ps.name : '/job:local'
I0930 03:58:54.716604 140278725625664 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I0930 03:58:54.716681 140278725625664 base_runner.py:59] cluster.ps.replicas : 1
I0930 03:58:54.716758 140278725625664 base_runner.py:59] cluster.ps.targets : ''
I0930 03:58:54.716833 140278725625664 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I0930 03:58:54.716901 140278725625664 base_runner.py:59] cluster.task : 0
I0930 03:58:54.716972 140278725625664 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I0930 03:58:54.717048 140278725625664 base_runner.py:59] cluster.worker.devices_per_split : 1
I0930 03:58:54.717130 140278725625664 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I0930 03:58:54.717204 140278725625664 base_runner.py:59] cluster.worker.name : '/job:local'
I0930 03:58:54.717276 140278725625664 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I0930 03:58:54.717355 140278725625664 base_runner.py:59] cluster.worker.replicas : 1
I0930 03:58:54.717428 140278725625664 base_runner.py:59] cluster.worker.targets : ''
I0930 03:58:54.717500 140278725625664 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I0930 03:58:54.717600 140278725625664 base_runner.py:59] dtype : float32
I0930 03:58:54.717675 140278725625664 base_runner.py:59] fprop_dtype : NoneType
I0930 03:58:54.717738 140278725625664 base_runner.py:59] inference_driver_name : NoneType
I0930 03:58:54.717814 140278725625664 base_runner.py:59] input.allow_implicit_capture : NoneType
I0930 03:58:54.717890 140278725625664 base_runner.py:59] input.bucket_adjust_every_n : 0
I0930 03:58:54.717966 140278725625664 base_runner.py:59] input.bucket_batch_limit : [32]
I0930 03:58:54.718038 140278725625664 base_runner.py:59] input.bucket_upper_bound : [1024]
I0930 03:58:54.718111 140278725625664 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I0930 03:58:54.718188 140278725625664 base_runner.py:59] input.dtype : float32
I0930 03:58:54.718258 140278725625664 base_runner.py:59] input.file_buffer_size : 10000000
I0930 03:58:54.718331 140278725625664 base_runner.py:59] input.file_datasource : NoneType
I0930 03:58:54.718403 140278725625664 base_runner.py:59] input.file_parallelism : 10
I0930 03:58:54.718467 140278725625664 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I0930 03:58:54.718540 140278725625664 base_runner.py:59] input.file_random_seed : 301
I0930 03:58:54.718621 140278725625664 base_runner.py:59] input.fixed_input_shape : True
I0930 03:58:54.718695 140278725625664 base_runner.py:59] input.flush_every_n : 0
I0930 03:58:54.718775 140278725625664 base_runner.py:59] input.fprop_dtype : NoneType
I0930 03:58:54.718844 140278725625664 base_runner.py:59] input.inference_driver_name : NoneType
I0930 03:58:54.718907 140278725625664 base_runner.py:59] input.is_eval : NoneType
I0930 03:58:54.718986 140278725625664 base_runner.py:59] input.is_inference : NoneType
I0930 03:58:54.719059 140278725625664 base_runner.py:59] input.name : '1bwds_train_set'
I0930 03:58:54.719139 140278725625664 base_runner.py:59] input.num_batcher_threads : 16
I0930 03:58:54.719210 140278725625664 base_runner.py:59] input.num_samples : 0
I0930 03:58:54.719276 140278725625664 base_runner.py:59] input.pad_to_max_seq_length : False
I0930 03:58:54.719354 140278725625664 base_runner.py:59] input.params_init.method : 'xavier'
I0930 03:58:54.719419 140278725625664 base_runner.py:59] input.params_init.scale : 1.000001
I0930 03:58:54.719501 140278725625664 base_runner.py:59] input.params_init.seed : NoneType
I0930 03:58:54.719576 140278725625664 base_runner.py:59] input.random_seed : NoneType
I0930 03:58:54.719643 140278725625664 base_runner.py:59] input.remote.max_inflights_per_target : 32
I0930 03:58:54.719715 140278725625664 base_runner.py:59] input.remote.shardable_batch : False
I0930 03:58:54.719794 140278725625664 base_runner.py:59] input.require_sequential_order : False
I0930 03:58:54.719871 140278725625664 base_runner.py:59] input.skip_lp_regularization : NoneType
I0930 03:58:54.719944 140278725625664 base_runner.py:59] input.source_max_length : NoneType
I0930 03:58:54.720017 140278725625664 base_runner.py:59] input.target_max_length : 1024
I0930 03:58:54.720096 140278725625664 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I0930 03:58:54.720175 140278725625664 base_runner.py:59] input.tokenizer.append_eos : True
I0930 03:58:54.720243 140278725625664 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I0930 03:58:54.720313 140278725625664 base_runner.py:59] input.tokenizer.dtype : float32
I0930 03:58:54.720398 140278725625664 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I0930 03:58:54.720461 140278725625664 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I0930 03:58:54.720534 140278725625664 base_runner.py:59] input.tokenizer.is_eval : NoneType
I0930 03:58:54.720613 140278725625664 base_runner.py:59] input.tokenizer.is_inference : NoneType
I0930 03:58:54.720686 140278725625664 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I0930 03:58:54.720765 140278725625664 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I0930 03:58:54.720831 140278725625664 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I0930 03:58:54.720903 140278725625664 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I0930 03:58:54.720975 140278725625664 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I0930 03:58:54.721053 140278725625664 base_runner.py:59] input.tokenizer.random_seed : NoneType
I0930 03:58:54.721129 140278725625664 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I0930 03:58:54.721201 140278725625664 base_runner.py:59] input.tokenizer.target_eos_id : 2
I0930 03:58:54.721273 140278725625664 base_runner.py:59] input.tokenizer.target_sos_id : 1
I0930 03:58:54.721349 140278725625664 base_runner.py:59] input.tokenizer.target_unk_id : 0
I0930 03:58:54.721415 140278725625664 base_runner.py:59] input.tokenizer.vn.global_vn : False
I0930 03:58:54.721488 140278725625664 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I0930 03:58:54.721586 140278725625664 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I0930 03:58:54.721655 140278725625664 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I0930 03:58:54.721717 140278725625664 base_runner.py:59] input.tokenizer.vocab_size : 32000
I0930 03:58:54.721797 140278725625664 base_runner.py:59] input.tokenizer_dict : {}
I0930 03:58:54.721870 140278725625664 base_runner.py:59] input.tpu_infeed_parallelism : 1
I0930 03:58:54.721947 140278725625664 base_runner.py:59] input.use_chaining : False
I0930 03:58:54.722017 140278725625664 base_runner.py:59] input.use_per_host_infeed : False
I0930 03:58:54.722092 140278725625664 base_runner.py:59] input.use_within_batch_mixing : False
I0930 03:58:54.722167 140278725625664 base_runner.py:59] input.vn.global_vn : False
I0930 03:58:54.722234 140278725625664 base_runner.py:59] input.vn.per_step_vn : False
I0930 03:58:54.722308 140278725625664 base_runner.py:59] input.vn.scale : NoneType
I0930 03:58:54.722382 140278725625664 base_runner.py:59] input.vn.seed : NoneType
I0930 03:58:54.722448 140278725625664 base_runner.py:59] is_eval : NoneType
I0930 03:58:54.722520 140278725625664 base_runner.py:59] is_inference : NoneType
I0930 03:58:54.722592 140278725625664 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:187'
I0930 03:58:54.722665 140278725625664 base_runner.py:59] name : ''
I0930 03:58:54.722740 140278725625664 base_runner.py:59] params_init.method : 'xavier'
I0930 03:58:54.722812 140278725625664 base_runner.py:59] params_init.scale : 1.000001
I0930 03:58:54.722884 140278725625664 base_runner.py:59] params_init.seed : NoneType
I0930 03:58:54.722961 140278725625664 base_runner.py:59] random_seed : NoneType
I0930 03:58:54.723033 140278725625664 base_runner.py:59] skip_lp_regularization : NoneType
I0930 03:58:54.723105 140278725625664 base_runner.py:59] task.allow_implicit_capture : NoneType
I0930 03:58:54.723178 140278725625664 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I0930 03:58:54.723245 140278725625664 base_runner.py:59] task.decoder : NoneType
I0930 03:58:54.723313 140278725625664 base_runner.py:59] task.dtype : float32
I0930 03:58:54.723395 140278725625664 base_runner.py:59] task.encoder : NoneType
I0930 03:58:54.723468 140278725625664 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I0930 03:58:54.723541 140278725625664 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I0930 03:58:54.723625 140278725625664 base_runner.py:59] task.eval.samples_per_summary : 0
I0930 03:58:54.723694 140278725625664 base_runner.py:59] task.eval.start_decoder_after : 0
I0930 03:58:54.723776 140278725625664 base_runner.py:59] task.eval.start_eval_after : 0
I0930 03:58:54.723843 140278725625664 base_runner.py:59] task.fprop_dtype : NoneType
I0930 03:58:54.723910 140278725625664 base_runner.py:59] task.inference_driver_name : NoneType
I0930 03:58:54.723988 140278725625664 base_runner.py:59] task.input : NoneType
I0930 03:58:54.724064 140278725625664 base_runner.py:59] task.is_eval : NoneType
I0930 03:58:54.724138 140278725625664 base_runner.py:59] task.is_inference : NoneType
I0930 03:58:54.724205 140278725625664 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I0930 03:58:54.724276 140278725625664 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I0930 03:58:54.724358 140278725625664 base_runner.py:59] task.lm.dtype : float32
I0930 03:58:54.724425 140278725625664 base_runner.py:59] task.lm.fprop_dtype : NoneType
I0930 03:58:54.724497 140278725625664 base_runner.py:59] task.lm.inference_driver_name : NoneType
I0930 03:58:54.724569 140278725625664 base_runner.py:59] task.lm.is_eval : NoneType
I0930 03:58:54.724645 140278725625664 base_runner.py:59] task.lm.is_inference : NoneType
I0930 03:58:54.724717 140278725625664 base_runner.py:59] task.lm.name : 'transformerlm'
I0930 03:58:54.724788 140278725625664 base_runner.py:59] task.lm.params_init.method : 'xavier'
I0930 03:58:54.724864 140278725625664 base_runner.py:59] task.lm.params_init.scale : 1.000001
I0930 03:58:54.724929 140278725625664 base_runner.py:59] task.lm.params_init.seed : NoneType
I0930 03:58:54.725008 140278725625664 base_runner.py:59] task.lm.random_seed : NoneType
I0930 03:58:54.725082 140278725625664 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I0930 03:58:54.725151 140278725625664 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I0930 03:58:54.725215 140278725625664 base_runner.py:59] task.lm.stack.batch_dim : 1
I0930 03:58:54.725294 140278725625664 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I0930 03:58:54.725359 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.725433 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 03:58:54.725511 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I0930 03:58:54.725602 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I0930 03:58:54.725679 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I0930 03:58:54.725756 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I0930 03:58:54.725821 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I0930 03:58:54.725902 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I0930 03:58:54.725976 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I0930 03:58:54.726043 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I0930 03:58:54.726120 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I0930 03:58:54.726189 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.726271 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:58:54.726345 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I0930 03:58:54.726425 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I0930 03:58:54.726488 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:58:54.726577 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:58:54.726647 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I0930 03:58:54.726710 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I0930 03:58:54.726789 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I0930 03:58:54.726864 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I0930 03:58:54.726940 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:58:54.727011 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:58:54.727077 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:58:54.727154 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I0930 03:58:54.727217 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.727290 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I0930 03:58:54.727364 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:58:54.727435 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I0930 03:58:54.727508 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I0930 03:58:54.727581 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I0930 03:58:54.727655 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I0930 03:58:54.727727 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I0930 03:58:54.727800 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I0930 03:58:54.727874 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I0930 03:58:54.727942 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I0930 03:58:54.728012 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I0930 03:58:54.728087 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I0930 03:58:54.728152 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I0930 03:58:54.728221 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.728301 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I0930 03:58:54.728374 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 03:58:54.728454 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.728521 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 03:58:54.728595 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 03:58:54.728669 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.728741 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 03:58:54.728808 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 03:58:54.728883 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 03:58:54.728957 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 03:58:54.729031 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 03:58:54.729104 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 03:58:54.729188 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 03:58:54.729262 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I0930 03:58:54.729335 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 03:58:54.729413 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 03:58:54.729482 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 03:58:54.729583 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 03:58:54.729648 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 03:58:54.729723 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 03:58:54.729800 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 03:58:54.729869 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 03:58:54.729942 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 03:58:54.730015 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 03:58:54.730087 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 03:58:54.730162 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 03:58:54.730234 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 03:58:54.730306 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 03:58:54.730383 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 03:58:54.730454 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 03:58:54.730528 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 03:58:54.730603 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 03:58:54.730672 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 03:58:54.730748 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 03:58:54.730822 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 03:58:54.730894 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 03:58:54.730966 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 03:58:54.731039 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 03:58:54.731103 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 03:58:54.731182 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 03:58:54.731259 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 03:58:54.731327 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 03:58:54.731400 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 03:58:54.731476 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 03:58:54.731554 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 03:58:54.731629 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 03:58:54.731697 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 03:58:54.731761 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 03:58:54.731838 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 03:58:54.731907 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 03:58:54.731979 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 03:58:54.732050 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 03:58:54.732127 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 03:58:54.732198 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 03:58:54.732272 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 03:58:54.732348 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 03:58:54.732414 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 03:58:54.732488 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 03:58:54.732559 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 03:58:54.732626 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.732707 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 03:58:54.732788 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 03:58:54.732866 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 03:58:54.732953 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 03:58:54.733033 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 03:58:54.733116 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 03:58:54.733196 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 03:58:54.733279 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I0930 03:58:54.733356 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I0930 03:58:54.733441 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 03:58:54.733550 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 03:58:54.733645 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 03:58:54.733734 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 03:58:54.733822 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I0930 03:58:54.733889 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.733941 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:58:54.733992 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 03:58:54.734043 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 03:58:54.734094 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:58:54.734144 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:58:54.734195 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 03:58:54.734246 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 03:58:54.734296 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 03:58:54.734347 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 03:58:54.734397 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:58:54.734448 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:58:54.734498 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:58:54.734548 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 03:58:54.734598 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.734647 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 03:58:54.734697 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:58:54.734747 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 03:58:54.734797 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 03:58:54.734847 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 03:58:54.734897 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I0930 03:58:54.734948 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I0930 03:58:54.734998 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I0930 03:58:54.735048 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 03:58:54.735098 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 03:58:54.735148 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 03:58:54.735198 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 03:58:54.735247 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 03:58:54.735298 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.735355 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:58:54.735407 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 03:58:54.735458 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 03:58:54.735508 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 03:58:54.735560 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 03:58:54.735611 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 03:58:54.735661 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 03:58:54.735712 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 03:58:54.735763 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 03:58:54.735814 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 03:58:54.735864 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:58:54.735915 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 03:58:54.735966 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 03:58:54.736017 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 03:58:54.736067 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 03:58:54.736118 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.736169 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 03:58:54.736220 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 03:58:54.736270 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 03:58:54.736326 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 03:58:54.736377 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.736428 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I0930 03:58:54.736479 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 03:58:54.736530 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 03:58:54.736580 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 03:58:54.736632 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 03:58:54.736683 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I0930 03:58:54.736734 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 03:58:54.736785 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.736835 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 03:58:54.736891 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 03:58:54.736943 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 03:58:54.736994 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.737045 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 03:58:54.737096 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 03:58:54.737147 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 03:58:54.737198 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 03:58:54.737249 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:58:54.737300 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 03:58:54.737351 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 03:58:54.737402 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 03:58:54.737452 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 03:58:54.737504 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 03:58:54.737585 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 03:58:54.737639 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 03:58:54.737690 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 03:58:54.737741 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 03:58:54.737792 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 03:58:54.737843 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 03:58:54.737894 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 03:58:54.737946 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 03:58:54.737996 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 03:58:54.738047 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 03:58:54.738098 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 03:58:54.738149 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 03:58:54.738200 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 03:58:54.738251 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 03:58:54.738302 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 03:58:54.738353 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 03:58:54.738403 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 03:58:54.738459 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 03:58:54.738511 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 03:58:54.738562 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 03:58:54.738612 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 03:58:54.738663 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 03:58:54.738713 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 03:58:54.738764 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 03:58:54.738813 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 03:58:54.738864 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 03:58:54.738914 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 03:58:54.738965 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 03:58:54.739015 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 03:58:54.739066 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 03:58:54.739115 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 03:58:54.739166 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 03:58:54.739216 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 03:58:54.739267 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 03:58:54.739317 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 03:58:54.739368 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 03:58:54.739418 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 03:58:54.739468 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 03:58:54.739519 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 03:58:54.739569 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 03:58:54.739620 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 03:58:54.739671 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 03:58:54.739721 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 03:58:54.739772 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 03:58:54.739822 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 03:58:54.739873 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 03:58:54.739929 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 03:58:54.739981 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 03:58:54.740032 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 03:58:54.740083 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 03:58:54.740134 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 03:58:54.740184 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 03:58:54.740235 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 03:58:54.740286 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 03:58:54.740337 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.740388 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 03:58:54.740439 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 03:58:54.740490 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 03:58:54.740541 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 03:58:54.740592 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 03:58:54.740643 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 03:58:54.740694 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I0930 03:58:54.740744 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 03:58:54.740795 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 03:58:54.740846 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 03:58:54.740896 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 03:58:54.740947 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.740998 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:58:54.741049 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 03:58:54.741100 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 03:58:54.741150 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:58:54.741201 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:58:54.741252 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 03:58:54.741303 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 03:58:54.741354 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 03:58:54.741405 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 03:58:54.741455 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:58:54.741511 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:58:54.741582 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:58:54.741635 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 03:58:54.741686 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.741737 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 03:58:54.741787 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:58:54.741837 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 03:58:54.741888 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 03:58:54.741940 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I0930 03:58:54.741991 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 03:58:54.742042 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 03:58:54.742092 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 03:58:54.742142 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 03:58:54.742193 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 03:58:54.742243 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 03:58:54.742294 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 03:58:54.742343 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 03:58:54.742394 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.742444 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 03:58:54.742496 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 03:58:54.742547 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 03:58:54.742598 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 03:58:54.742649 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 03:58:54.742699 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 03:58:54.742750 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 03:58:54.742801 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 03:58:54.742853 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 03:58:54.742903 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 03:58:54.742954 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 03:58:54.743004 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 03:58:54.743055 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 03:58:54.743111 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 03:58:54.743163 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 03:58:54.743215 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 03:58:54.743266 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 03:58:54.743318 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 03:58:54.743370 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.743427 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 03:58:54.743479 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 03:58:54.743530 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 03:58:54.743582 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 03:58:54.743633 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 03:58:54.743683 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 03:58:54.743734 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.743785 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:58:54.743836 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 03:58:54.743887 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 03:58:54.743937 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 03:58:54.743989 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 03:58:54.744040 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 03:58:54.744091 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 03:58:54.744141 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 03:58:54.744193 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 03:58:54.744243 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 03:58:54.744295 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:58:54.744347 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 03:58:54.744398 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 03:58:54.744449 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 03:58:54.744500 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 03:58:54.744551 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.744607 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 03:58:54.744660 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 03:58:54.744711 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 03:58:54.744761 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 03:58:54.744812 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.744863 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 03:58:54.744914 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 03:58:54.744965 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 03:58:54.745015 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 03:58:54.745066 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I0930 03:58:54.745116 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I0930 03:58:54.745167 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I0930 03:58:54.745217 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I0930 03:58:54.745267 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I0930 03:58:54.745317 140278725625664 base_runner.py:59] task.lm.stack.dtype : float32
I0930 03:58:54.745368 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I0930 03:58:54.745419 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.745469 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I0930 03:58:54.745519 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I0930 03:58:54.745589 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I0930 03:58:54.745641 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.745691 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:58:54.745742 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I0930 03:58:54.745792 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I0930 03:58:54.745843 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 03:58:54.745893 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 03:58:54.745944 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I0930 03:58:54.745994 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I0930 03:58:54.746044 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I0930 03:58:54.746095 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I0930 03:58:54.746145 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I0930 03:58:54.746195 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:58:54.746246 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 03:58:54.746296 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 03:58:54.746345 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I0930 03:58:54.746405 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I0930 03:58:54.746457 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.746508 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I0930 03:58:54.746559 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I0930 03:58:54.746610 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I0930 03:58:54.746660 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I0930 03:58:54.746711 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I0930 03:58:54.746762 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I0930 03:58:54.746812 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I0930 03:58:54.746863 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I0930 03:58:54.746914 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I0930 03:58:54.746966 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I0930 03:58:54.747016 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I0930 03:58:54.747067 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I0930 03:58:54.747119 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I0930 03:58:54.747169 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I0930 03:58:54.747220 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I0930 03:58:54.747271 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I0930 03:58:54.747322 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I0930 03:58:54.747374 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I0930 03:58:54.747425 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I0930 03:58:54.747476 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I0930 03:58:54.747526 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I0930 03:58:54.747578 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I0930 03:58:54.747629 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I0930 03:58:54.747679 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I0930 03:58:54.747730 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I0930 03:58:54.747781 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I0930 03:58:54.747832 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I0930 03:58:54.747884 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I0930 03:58:54.747934 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I0930 03:58:54.747985 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I0930 03:58:54.748035 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I0930 03:58:54.748086 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I0930 03:58:54.748136 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I0930 03:58:54.748187 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I0930 03:58:54.748238 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I0930 03:58:54.748292 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I0930 03:58:54.748344 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I0930 03:58:54.748394 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I0930 03:58:54.748444 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I0930 03:58:54.748494 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I0930 03:58:54.748545 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I0930 03:58:54.748596 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.748646 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I0930 03:58:54.748696 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I0930 03:58:54.748747 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I0930 03:58:54.748797 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I0930 03:58:54.748848 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I0930 03:58:54.748898 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I0930 03:58:54.748949 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I0930 03:58:54.748999 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I0930 03:58:54.749049 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I0930 03:58:54.749100 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I0930 03:58:54.749150 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I0930 03:58:54.749200 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I0930 03:58:54.749250 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I0930 03:58:54.749300 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I0930 03:58:54.749351 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I0930 03:58:54.749400 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I0930 03:58:54.749450 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I0930 03:58:54.749500 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I0930 03:58:54.749573 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I0930 03:58:54.749627 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I0930 03:58:54.749677 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I0930 03:58:54.749728 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I0930 03:58:54.749779 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I0930 03:58:54.749829 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I0930 03:58:54.749880 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I0930 03:58:54.749930 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I0930 03:58:54.749980 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I0930 03:58:54.750030 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I0930 03:58:54.750081 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.750136 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 03:58:54.750188 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I0930 03:58:54.750239 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I0930 03:58:54.750289 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I0930 03:58:54.750340 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I0930 03:58:54.750390 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I0930 03:58:54.750441 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I0930 03:58:54.750491 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I0930 03:58:54.750541 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I0930 03:58:54.750590 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I0930 03:58:54.750640 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.750691 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:58:54.750741 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I0930 03:58:54.750792 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I0930 03:58:54.750841 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:58:54.750891 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:58:54.750941 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I0930 03:58:54.750992 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I0930 03:58:54.751042 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I0930 03:58:54.751091 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I0930 03:58:54.751142 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:58:54.751192 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:58:54.751241 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:58:54.751291 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I0930 03:58:54.751341 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.751391 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I0930 03:58:54.751442 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:58:54.751492 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I0930 03:58:54.751543 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I0930 03:58:54.751593 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I0930 03:58:54.751643 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I0930 03:58:54.751693 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I0930 03:58:54.751744 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I0930 03:58:54.751795 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I0930 03:58:54.751845 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I0930 03:58:54.751895 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I0930 03:58:54.751945 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I0930 03:58:54.752000 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I0930 03:58:54.752052 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.752102 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I0930 03:58:54.752153 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 03:58:54.752203 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.752254 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 03:58:54.752304 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 03:58:54.752354 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.752404 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 03:58:54.752455 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 03:58:54.752504 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 03:58:54.752555 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 03:58:54.752605 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 03:58:54.752656 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 03:58:54.752706 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 03:58:54.752757 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I0930 03:58:54.752807 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 03:58:54.752857 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 03:58:54.752907 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 03:58:54.752957 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 03:58:54.753007 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 03:58:54.753057 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 03:58:54.753108 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 03:58:54.753158 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 03:58:54.753208 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 03:58:54.753259 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 03:58:54.753309 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 03:58:54.753360 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 03:58:54.753411 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 03:58:54.753461 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 03:58:54.753511 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 03:58:54.753585 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 03:58:54.753637 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 03:58:54.753688 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 03:58:54.753739 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 03:58:54.753790 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 03:58:54.753840 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 03:58:54.753890 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 03:58:54.753940 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 03:58:54.753990 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 03:58:54.754040 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 03:58:54.754090 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 03:58:54.754140 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 03:58:54.754191 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 03:58:54.754241 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 03:58:54.754291 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 03:58:54.754341 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 03:58:54.754391 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 03:58:54.754442 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 03:58:54.754492 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 03:58:54.754542 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 03:58:54.754591 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 03:58:54.754642 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 03:58:54.754692 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 03:58:54.754742 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 03:58:54.754792 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 03:58:54.754842 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 03:58:54.754893 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 03:58:54.754943 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 03:58:54.754993 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 03:58:54.755049 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 03:58:54.755100 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.755150 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 03:58:54.755199 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 03:58:54.755250 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 03:58:54.755300 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 03:58:54.755350 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 03:58:54.755400 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 03:58:54.755451 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 03:58:54.755501 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I0930 03:58:54.755550 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I0930 03:58:54.755600 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 03:58:54.755650 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 03:58:54.755700 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 03:58:54.755750 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 03:58:54.755800 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I0930 03:58:54.755850 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.755900 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:58:54.755950 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 03:58:54.756000 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 03:58:54.756050 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:58:54.756100 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:58:54.756150 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 03:58:54.756201 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 03:58:54.756250 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 03:58:54.756300 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 03:58:54.756351 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:58:54.756400 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:58:54.756453 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:58:54.756504 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 03:58:54.756554 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.756604 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 03:58:54.756659 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:58:54.756710 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 03:58:54.756760 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 03:58:54.756811 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 03:58:54.756861 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I0930 03:58:54.756911 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I0930 03:58:54.756962 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I0930 03:58:54.757012 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 03:58:54.757062 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 03:58:54.757112 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 03:58:54.757163 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 03:58:54.757214 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 03:58:54.757264 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.757315 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:58:54.757365 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 03:58:54.757416 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 03:58:54.757466 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 03:58:54.757516 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 03:58:54.757586 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 03:58:54.757638 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 03:58:54.757688 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 03:58:54.757739 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 03:58:54.757790 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 03:58:54.757841 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:58:54.757892 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 03:58:54.757942 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 03:58:54.757992 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 03:58:54.758043 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 03:58:54.758094 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.758144 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 03:58:54.758200 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 03:58:54.758251 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 03:58:54.758301 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 03:58:54.758352 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.758402 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I0930 03:58:54.758452 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 03:58:54.758502 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 03:58:54.758553 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 03:58:54.758603 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 03:58:54.758653 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I0930 03:58:54.758704 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 03:58:54.758754 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.758804 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 03:58:54.758854 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 03:58:54.758904 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 03:58:54.758954 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.759005 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 03:58:54.759055 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 03:58:54.759105 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 03:58:54.759156 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 03:58:54.759206 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:58:54.759256 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 03:58:54.759307 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 03:58:54.759357 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 03:58:54.759407 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 03:58:54.759457 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 03:58:54.759506 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 03:58:54.759556 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 03:58:54.759606 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 03:58:54.759656 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 03:58:54.759706 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 03:58:54.759760 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 03:58:54.759811 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 03:58:54.759861 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 03:58:54.759912 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 03:58:54.759962 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 03:58:54.760012 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 03:58:54.760062 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 03:58:54.760112 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 03:58:54.760162 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 03:58:54.760212 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 03:58:54.760263 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 03:58:54.760313 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 03:58:54.760363 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 03:58:54.760414 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 03:58:54.760464 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 03:58:54.760514 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 03:58:54.760564 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 03:58:54.760614 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 03:58:54.760664 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 03:58:54.760714 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 03:58:54.760763 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 03:58:54.760813 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 03:58:54.760864 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 03:58:54.760913 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 03:58:54.760963 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 03:58:54.761013 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 03:58:54.761063 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 03:58:54.761113 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 03:58:54.761162 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 03:58:54.761215 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 03:58:54.761266 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 03:58:54.761316 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 03:58:54.761366 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 03:58:54.761416 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 03:58:54.761466 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 03:58:54.761515 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 03:58:54.761589 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 03:58:54.761640 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 03:58:54.761691 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 03:58:54.761741 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 03:58:54.761792 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 03:58:54.761843 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 03:58:54.761893 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 03:58:54.761944 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 03:58:54.761994 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 03:58:54.762044 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 03:58:54.762094 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 03:58:54.762144 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 03:58:54.762195 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 03:58:54.762245 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.762295 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 03:58:54.762344 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 03:58:54.762395 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 03:58:54.762446 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 03:58:54.762495 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 03:58:54.762546 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 03:58:54.762596 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I0930 03:58:54.762646 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 03:58:54.762696 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 03:58:54.762751 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 03:58:54.762802 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 03:58:54.762852 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.762902 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 03:58:54.762951 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 03:58:54.763001 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 03:58:54.763051 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 03:58:54.763101 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 03:58:54.763150 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 03:58:54.763200 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 03:58:54.763249 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 03:58:54.763298 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 03:58:54.763348 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 03:58:54.763398 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 03:58:54.763447 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 03:58:54.763497 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 03:58:54.763547 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.763597 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 03:58:54.763647 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 03:58:54.763696 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 03:58:54.763746 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 03:58:54.763795 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I0930 03:58:54.763845 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 03:58:54.763895 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 03:58:54.763946 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 03:58:54.763996 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 03:58:54.764046 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 03:58:54.764096 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 03:58:54.764146 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 03:58:54.764196 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 03:58:54.764263 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.764348 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 03:58:54.764413 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 03:58:54.764466 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 03:58:54.764518 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 03:58:54.764569 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 03:58:54.764620 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 03:58:54.764671 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 03:58:54.764723 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 03:58:54.764774 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 03:58:54.764824 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 03:58:54.764874 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 03:58:54.764925 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 03:58:54.764975 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 03:58:54.765026 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 03:58:54.765077 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 03:58:54.765128 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 03:58:54.765178 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 03:58:54.765229 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 03:58:54.765280 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.765330 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 03:58:54.765380 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 03:58:54.765430 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 03:58:54.765480 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 03:58:54.765547 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 03:58:54.765604 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 03:58:54.765657 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.765708 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 03:58:54.765759 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 03:58:54.765810 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 03:58:54.765861 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 03:58:54.765913 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 03:58:54.765973 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 03:58:54.766026 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 03:58:54.766077 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 03:58:54.766127 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 03:58:54.766178 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 03:58:54.766228 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:58:54.766279 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 03:58:54.766329 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 03:58:54.766379 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 03:58:54.766428 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 03:58:54.766482 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.766534 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 03:58:54.766585 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 03:58:54.766636 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 03:58:54.766687 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 03:58:54.766738 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.766789 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 03:58:54.766840 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 03:58:54.766891 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 03:58:54.766941 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 03:58:54.766991 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I0930 03:58:54.767042 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I0930 03:58:54.767092 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I0930 03:58:54.767143 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I0930 03:58:54.767194 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I0930 03:58:54.767245 140278725625664 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I0930 03:58:54.767296 140278725625664 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I0930 03:58:54.767346 140278725625664 base_runner.py:59] task.lm.stack.is_eval : NoneType
I0930 03:58:54.767396 140278725625664 base_runner.py:59] task.lm.stack.is_inference : NoneType
I0930 03:58:54.767447 140278725625664 base_runner.py:59] task.lm.stack.is_transparent : False
I0930 03:58:54.767497 140278725625664 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I0930 03:58:54.767547 140278725625664 base_runner.py:59] task.lm.stack.model_dim : 2048
I0930 03:58:54.767598 140278725625664 base_runner.py:59] task.lm.stack.name : ''
I0930 03:58:54.767653 140278725625664 base_runner.py:59] task.lm.stack.normalize_encoder : False
I0930 03:58:54.767705 140278725625664 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I0930 03:58:54.767756 140278725625664 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I0930 03:58:54.767807 140278725625664 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I0930 03:58:54.767858 140278725625664 base_runner.py:59] task.lm.stack.packed_input : False
I0930 03:58:54.767909 140278725625664 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I0930 03:58:54.767960 140278725625664 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I0930 03:58:54.768011 140278725625664 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I0930 03:58:54.768061 140278725625664 base_runner.py:59] task.lm.stack.random_seed : NoneType
I0930 03:58:54.768111 140278725625664 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I0930 03:58:54.768161 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.768211 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I0930 03:58:54.768261 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I0930 03:58:54.768311 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I0930 03:58:54.768362 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I0930 03:58:54.768412 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I0930 03:58:54.768463 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I0930 03:58:54.768514 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I0930 03:58:54.768564 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I0930 03:58:54.768614 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I0930 03:58:54.768664 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I0930 03:58:54.768714 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I0930 03:58:54.768764 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I0930 03:58:54.768814 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I0930 03:58:54.768864 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I0930 03:58:54.768914 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I0930 03:58:54.768964 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I0930 03:58:54.769014 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I0930 03:58:54.769064 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I0930 03:58:54.769114 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I0930 03:58:54.769164 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I0930 03:58:54.769214 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.769264 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I0930 03:58:54.769314 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I0930 03:58:54.769365 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I0930 03:58:54.769415 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I0930 03:58:54.769464 140278725625664 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I0930 03:58:54.769514 140278725625664 base_runner.py:59] task.lm.stack.state_dtype : float32
I0930 03:58:54.769584 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I0930 03:58:54.769635 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.769691 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I0930 03:58:54.769743 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 03:58:54.769795 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I0930 03:58:54.769846 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I0930 03:58:54.769897 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I0930 03:58:54.769948 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 03:58:54.770000 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 03:58:54.770051 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I0930 03:58:54.770103 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I0930 03:58:54.770155 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I0930 03:58:54.770205 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I0930 03:58:54.770257 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I0930 03:58:54.770308 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 03:58:54.770359 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 03:58:54.770410 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 03:58:54.770461 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I0930 03:58:54.770511 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I0930 03:58:54.770562 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.770614 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I0930 03:58:54.770665 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I0930 03:58:54.770715 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I0930 03:58:54.770766 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I0930 03:58:54.770816 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I0930 03:58:54.770867 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I0930 03:58:54.770917 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I0930 03:58:54.770967 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I0930 03:58:54.771018 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I0930 03:58:54.771069 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I0930 03:58:54.771119 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I0930 03:58:54.771169 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I0930 03:58:54.771219 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I0930 03:58:54.771269 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I0930 03:58:54.771324 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I0930 03:58:54.771375 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I0930 03:58:54.771425 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I0930 03:58:54.771476 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I0930 03:58:54.771526 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I0930 03:58:54.771576 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I0930 03:58:54.771627 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I0930 03:58:54.771677 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I0930 03:58:54.771727 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I0930 03:58:54.771778 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I0930 03:58:54.771828 140278725625664 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I0930 03:58:54.771878 140278725625664 base_runner.py:59] task.lm.stack.vn.global_vn : False
I0930 03:58:54.771929 140278725625664 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I0930 03:58:54.771980 140278725625664 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I0930 03:58:54.772030 140278725625664 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I0930 03:58:54.772081 140278725625664 base_runner.py:59] task.lm.vn.global_vn : False
I0930 03:58:54.772131 140278725625664 base_runner.py:59] task.lm.vn.per_step_vn : False
I0930 03:58:54.772181 140278725625664 base_runner.py:59] task.lm.vn.scale : NoneType
I0930 03:58:54.772232 140278725625664 base_runner.py:59] task.lm.vn.seed : NoneType
I0930 03:58:54.772282 140278725625664 base_runner.py:59] task.lm.vocab_size : 32000
I0930 03:58:54.772333 140278725625664 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I0930 03:58:54.772383 140278725625664 base_runner.py:59] task.online_encoder : NoneType
I0930 03:58:54.772434 140278725625664 base_runner.py:59] task.params_init.method : 'xavier'
I0930 03:58:54.772485 140278725625664 base_runner.py:59] task.params_init.scale : 1.000001
I0930 03:58:54.772535 140278725625664 base_runner.py:59] task.params_init.seed : NoneType
I0930 03:58:54.772586 140278725625664 base_runner.py:59] task.random_seed : NoneType
I0930 03:58:54.772637 140278725625664 base_runner.py:59] task.skip_lp_regularization : NoneType
I0930 03:58:54.772688 140278725625664 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I0930 03:58:54.772739 140278725625664 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I0930 03:58:54.772789 140278725625664 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I0930 03:58:54.772840 140278725625664 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I0930 03:58:54.772890 140278725625664 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I0930 03:58:54.772940 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I0930 03:58:54.772990 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I0930 03:58:54.773041 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I0930 03:58:54.773091 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I0930 03:58:54.773141 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I0930 03:58:54.773191 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I0930 03:58:54.773241 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I0930 03:58:54.773295 140278725625664 base_runner.py:59] task.train.early_stop.min_steps : 0
I0930 03:58:54.773347 140278725625664 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I0930 03:58:54.773397 140278725625664 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I0930 03:58:54.773453 140278725625664 base_runner.py:59] task.train.early_stop.verbose : True
I0930 03:58:54.773506 140278725625664 base_runner.py:59] task.train.early_stop.window : 0
I0930 03:58:54.773579 140278725625664 base_runner.py:59] task.train.ema_decay : 0.0
I0930 03:58:54.773633 140278725625664 base_runner.py:59] task.train.enqueue_max_steps : -1
I0930 03:58:54.773683 140278725625664 base_runner.py:59] task.train.gate_gradients : False
I0930 03:58:54.773734 140278725625664 base_runner.py:59] task.train.grad_aggregation_method : 1
I0930 03:58:54.773784 140278725625664 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I0930 03:58:54.773835 140278725625664 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I0930 03:58:54.773885 140278725625664 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I0930 03:58:54.773935 140278725625664 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I0930 03:58:54.773984 140278725625664 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I0930 03:58:54.774034 140278725625664 base_runner.py:59] task.train.learner : NoneType
I0930 03:58:54.774085 140278725625664 base_runner.py:59] task.train.learning_rate : 0.5
I0930 03:58:54.774134 140278725625664 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I0930 03:58:54.774185 140278725625664 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 03:58:54.774235 140278725625664 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I0930 03:58:54.774286 140278725625664 base_runner.py:59] task.train.lr_schedule.dtype : float32
I0930 03:58:54.774337 140278725625664 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I0930 03:58:54.774388 140278725625664 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I0930 03:58:54.774438 140278725625664 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I0930 03:58:54.774488 140278725625664 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I0930 03:58:54.774539 140278725625664 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I0930 03:58:54.774589 140278725625664 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I0930 03:58:54.774639 140278725625664 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I0930 03:58:54.774690 140278725625664 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I0930 03:58:54.774741 140278725625664 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I0930 03:58:54.774791 140278725625664 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I0930 03:58:54.774841 140278725625664 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I0930 03:58:54.774892 140278725625664 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I0930 03:58:54.774942 140278725625664 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I0930 03:58:54.774993 140278725625664 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I0930 03:58:54.775043 140278725625664 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I0930 03:58:54.775093 140278725625664 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I0930 03:58:54.775143 140278725625664 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I0930 03:58:54.775193 140278725625664 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I0930 03:58:54.775243 140278725625664 base_runner.py:59] task.train.max_steps : 4000000
I0930 03:58:54.775293 140278725625664 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I0930 03:58:54.775342 140278725625664 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I0930 03:58:54.775398 140278725625664 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I0930 03:58:54.775450 140278725625664 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 03:58:54.775501 140278725625664 base_runner.py:59] task.train.optimizer.dtype : float32
I0930 03:58:54.775551 140278725625664 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I0930 03:58:54.775602 140278725625664 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I0930 03:58:54.775652 140278725625664 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I0930 03:58:54.775702 140278725625664 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I0930 03:58:54.775752 140278725625664 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I0930 03:58:54.775803 140278725625664 base_runner.py:59] task.train.optimizer.name : 'Adam'
I0930 03:58:54.775853 140278725625664 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I0930 03:58:54.775902 140278725625664 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I0930 03:58:54.775953 140278725625664 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I0930 03:58:54.776003 140278725625664 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I0930 03:58:54.776053 140278725625664 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I0930 03:58:54.776103 140278725625664 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I0930 03:58:54.776153 140278725625664 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I0930 03:58:54.776203 140278725625664 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I0930 03:58:54.776253 140278725625664 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I0930 03:58:54.776303 140278725625664 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I0930 03:58:54.776352 140278725625664 base_runner.py:59] task.train.save_interval_seconds : 600
I0930 03:58:54.776402 140278725625664 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I0930 03:58:54.776451 140278725625664 base_runner.py:59] task.train.save_max_to_keep : 100
I0930 03:58:54.776502 140278725625664 base_runner.py:59] task.train.start_up_delay_steps : 200
I0930 03:58:54.776556 140278725625664 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I0930 03:58:54.776607 140278725625664 base_runner.py:59] task.train.summary_interval_steps : 100
I0930 03:58:54.776657 140278725625664 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I0930 03:58:54.776707 140278725625664 base_runner.py:59] task.train.vn_start_step : 20000
I0930 03:58:54.776757 140278725625664 base_runner.py:59] task.train.vn_std : 0.0
I0930 03:58:54.776807 140278725625664 base_runner.py:59] task.vn.global_vn : False
I0930 03:58:54.776857 140278725625664 base_runner.py:59] task.vn.per_step_vn : False
I0930 03:58:54.776907 140278725625664 base_runner.py:59] task.vn.scale : NoneType
I0930 03:58:54.776957 140278725625664 base_runner.py:59] task.vn.seed : NoneType
I0930 03:58:54.777006 140278725625664 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I0930 03:58:54.777056 140278725625664 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I0930 03:58:54.777106 140278725625664 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I0930 03:58:54.777156 140278725625664 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I0930 03:58:54.777206 140278725625664 base_runner.py:59] train.early_stop.metric_history.minimize : True
I0930 03:58:54.777256 140278725625664 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I0930 03:58:54.777307 140278725625664 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I0930 03:58:54.777357 140278725625664 base_runner.py:59] train.early_stop.min_steps : 0
I0930 03:58:54.777407 140278725625664 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I0930 03:58:54.777462 140278725625664 base_runner.py:59] train.early_stop.tolerance : 0.0
I0930 03:58:54.777514 140278725625664 base_runner.py:59] train.early_stop.verbose : True
I0930 03:58:54.777584 140278725625664 base_runner.py:59] train.early_stop.window : 0
I0930 03:58:54.777636 140278725625664 base_runner.py:59] train.ema_decay : 0.0
I0930 03:58:54.777687 140278725625664 base_runner.py:59] train.enqueue_max_steps : -1
I0930 03:58:54.777736 140278725625664 base_runner.py:59] train.init_from_checkpoint_rules : {}
I0930 03:58:54.777786 140278725625664 base_runner.py:59] train.max_steps : 4000000
I0930 03:58:54.777836 140278725625664 base_runner.py:59] train.save_interval_seconds : 600
I0930 03:58:54.777887 140278725625664 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I0930 03:58:54.777937 140278725625664 base_runner.py:59] train.save_max_to_keep : 100
I0930 03:58:54.777987 140278725625664 base_runner.py:59] train.start_up_delay_steps : 200
I0930 03:58:54.778037 140278725625664 base_runner.py:59] train.summary_interval_steps : 100
I0930 03:58:54.778087 140278725625664 base_runner.py:59] train.tpu_steps_per_loop : 100
I0930 03:58:54.778137 140278725625664 base_runner.py:59] vn.global_vn : False
I0930 03:58:54.778187 140278725625664 base_runner.py:59] vn.per_step_vn : False
I0930 03:58:54.778238 140278725625664 base_runner.py:59] vn.scale : NoneType
I0930 03:58:54.778289 140278725625664 base_runner.py:59] vn.seed : NoneType
I0930 03:58:54.778339 140278725625664 base_runner.py:59] 
I0930 03:58:54.778445 140278725625664 base_runner.py:60] ============================================================
I0930 03:58:54.780606 140278725625664 base_runner.py:106] Starting ...
I0930 03:58:54.781411 140278725625664 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I0930 03:58:54.792287 140278725625664 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I0930 03:58:54.806478 140278725625664 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I0930 03:58:54.822860 140278725625664 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I0930 03:58:54.826768 140278725625664 base_input_generator.py:624] bucket_batch_limit [32]
I0930 03:58:54.889516 140278725625664 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I0930 03:58:54.889648 140278725625664 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I0930 03:58:54.889717 140278725625664 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I0930 03:58:54.889776 140278725625664 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I0930 03:58:54.889832 140278725625664 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I0930 03:58:54.889889 140278725625664 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I0930 03:58:54.890004 140278725625664 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I0930 03:58:54.890064 140278725625664 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I0930 03:58:54.890120 140278725625664 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I0930 03:58:54.890173 140278725625664 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I0930 03:58:54.890225 140278725625664 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I0930 03:58:54.890277 140278725625664 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I0930 03:58:54.890329 140278725625664 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I0930 03:58:54.890383 140278725625664 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I0930 03:58:54.890435 140278725625664 learner.py:351] Ignoring legacy param learner=None for optimization program
I0930 03:58:54.890521 140278725625664 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I0930 03:58:54.890577 140278725625664 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I0930 03:58:54.891032 140278725625664 learner.py:356] Learner params: allow_implicit_capture : NoneType
I0930 03:58:54.891116 140278725625664 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I0930 03:58:54.891180 140278725625664 learner.py:356] Learner params: bprop_variable_filter : NoneType
I0930 03:58:54.891238 140278725625664 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I0930 03:58:54.891291 140278725625664 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I0930 03:58:54.891344 140278725625664 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I0930 03:58:54.891396 140278725625664 learner.py:356] Learner params: colocate_gradients_with_ops : True
I0930 03:58:54.891448 140278725625664 learner.py:356] Learner params: dtype : float32
I0930 03:58:54.891499 140278725625664 learner.py:356] Learner params: fprop_dtype : NoneType
I0930 03:58:54.891551 140278725625664 learner.py:356] Learner params: gate_gradients : False
I0930 03:58:54.891602 140278725625664 learner.py:356] Learner params: grad_aggregation_method : 1
I0930 03:58:54.891654 140278725625664 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I0930 03:58:54.891705 140278725625664 learner.py:356] Learner params: grad_norm_tracker : NoneType
I0930 03:58:54.891763 140278725625664 learner.py:356] Learner params: inference_driver_name : NoneType
I0930 03:58:54.891816 140278725625664 learner.py:356] Learner params: is_eval : NoneType
I0930 03:58:54.891869 140278725625664 learner.py:356] Learner params: is_inference : NoneType
I0930 03:58:54.891920 140278725625664 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I0930 03:58:54.891972 140278725625664 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I0930 03:58:54.892024 140278725625664 learner.py:356] Learner params: learning_rate : 0.5
I0930 03:58:54.892075 140278725625664 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I0930 03:58:54.892126 140278725625664 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 03:58:54.892177 140278725625664 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I0930 03:58:54.892229 140278725625664 learner.py:356] Learner params: lr_schedule.dtype : float32
I0930 03:58:54.892281 140278725625664 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I0930 03:58:54.892332 140278725625664 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I0930 03:58:54.892383 140278725625664 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I0930 03:58:54.892434 140278725625664 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I0930 03:58:54.892485 140278725625664 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I0930 03:58:54.892536 140278725625664 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I0930 03:58:54.892588 140278725625664 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I0930 03:58:54.892639 140278725625664 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I0930 03:58:54.892691 140278725625664 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I0930 03:58:54.892742 140278725625664 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I0930 03:58:54.892794 140278725625664 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I0930 03:58:54.892845 140278725625664 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I0930 03:58:54.892897 140278725625664 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I0930 03:58:54.892947 140278725625664 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I0930 03:58:54.892998 140278725625664 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I0930 03:58:54.893049 140278725625664 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I0930 03:58:54.893100 140278725625664 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I0930 03:58:54.893151 140278725625664 learner.py:356] Learner params: name : 'loss'
I0930 03:58:54.893201 140278725625664 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I0930 03:58:54.893252 140278725625664 learner.py:356] Learner params: optimizer.beta1 : 0.9
I0930 03:58:54.893303 140278725625664 learner.py:356] Learner params: optimizer.beta2 : 0.997
I0930 03:58:54.893354 140278725625664 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 03:58:54.893405 140278725625664 learner.py:356] Learner params: optimizer.dtype : float32
I0930 03:58:54.893456 140278725625664 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I0930 03:58:54.893507 140278725625664 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I0930 03:58:54.893588 140278725625664 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I0930 03:58:54.893642 140278725625664 learner.py:356] Learner params: optimizer.is_eval : NoneType
I0930 03:58:54.893694 140278725625664 learner.py:356] Learner params: optimizer.is_inference : NoneType
I0930 03:58:54.893746 140278725625664 learner.py:356] Learner params: optimizer.name : 'Adam'
I0930 03:58:54.893797 140278725625664 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I0930 03:58:54.893853 140278725625664 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I0930 03:58:54.893905 140278725625664 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I0930 03:58:54.893957 140278725625664 learner.py:356] Learner params: optimizer.random_seed : NoneType
I0930 03:58:54.894008 140278725625664 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I0930 03:58:54.894058 140278725625664 learner.py:356] Learner params: optimizer.vn.global_vn : False
I0930 03:58:54.894110 140278725625664 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I0930 03:58:54.894161 140278725625664 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I0930 03:58:54.894212 140278725625664 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I0930 03:58:54.894264 140278725625664 learner.py:356] Learner params: params_init.method : 'xavier'
I0930 03:58:54.894315 140278725625664 learner.py:356] Learner params: params_init.scale : 1.000001
I0930 03:58:54.894366 140278725625664 learner.py:356] Learner params: params_init.seed : NoneType
I0930 03:58:54.894418 140278725625664 learner.py:356] Learner params: random_seed : NoneType
I0930 03:58:54.894469 140278725625664 learner.py:356] Learner params: skip_lp_regularization : NoneType
I0930 03:58:54.894521 140278725625664 learner.py:356] Learner params: vn.global_vn : False
I0930 03:58:54.894572 140278725625664 learner.py:356] Learner params: vn.per_step_vn : False
I0930 03:58:54.894624 140278725625664 learner.py:356] Learner params: vn.scale : NoneType
I0930 03:58:54.894675 140278725625664 learner.py:356] Learner params: vn.seed : NoneType
I0930 03:58:54.894727 140278725625664 learner.py:356] Learner params: 
I0930 03:58:55.284943 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I0930 03:58:55.287069 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.306667 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I0930 03:58:55.308675 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.311477 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I0930 03:58:55.313160 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.320407 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I0930 03:58:55.322425 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.325189 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I0930 03:58:55.326920 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.334222 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I0930 03:58:55.336229 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.338956 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I0930 03:58:55.340640 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.347947 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I0930 03:58:55.349980 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.352760 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I0930 03:58:55.354498 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.358727 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I0930 03:58:55.360446 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.364442 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I0930 03:58:55.366160 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.368917 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I0930 03:58:55.370651 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:55.380266 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:55.386862 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I0930 03:58:55.388953 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.391722 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I0930 03:58:55.393420 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:55.395407 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:55.401985 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I0930 03:58:55.403983 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.406790 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I0930 03:58:55.408493 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.413198 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I0930 03:58:55.415189 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.418018 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I0930 03:58:55.419715 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.440394 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I0930 03:58:55.442420 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.445111 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I0930 03:58:55.446963 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.454232 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I0930 03:58:55.456245 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.459045 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I0930 03:58:55.460757 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.468009 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I0930 03:58:55.470587 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.473272 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I0930 03:58:55.475020 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.482395 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I0930 03:58:55.484406 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.487138 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I0930 03:58:55.488970 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.492758 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I0930 03:58:55.494511 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.498489 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I0930 03:58:55.500208 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.503014 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I0930 03:58:55.504725 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:55.514302 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:55.520858 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I0930 03:58:55.522976 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.525697 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I0930 03:58:55.527422 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:55.529927 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:55.536517 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I0930 03:58:55.538562 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.541377 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I0930 03:58:55.543136 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.547852 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I0930 03:58:55.549574 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.552401 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I0930 03:58:55.554144 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.574654 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I0930 03:58:55.576667 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.579447 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I0930 03:58:55.581793 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.589118 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I0930 03:58:55.591163 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.594000 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I0930 03:58:55.595772 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.603159 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I0930 03:58:55.605227 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.607937 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I0930 03:58:55.609673 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.617023 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I0930 03:58:55.619094 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.621849 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I0930 03:58:55.623674 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.627506 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I0930 03:58:55.629234 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.633251 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I0930 03:58:55.634992 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.637830 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I0930 03:58:55.639556 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:55.649705 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:55.656287 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I0930 03:58:55.658390 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.661091 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I0930 03:58:55.662858 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:55.664851 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:55.671401 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I0930 03:58:55.673411 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.676227 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I0930 03:58:55.677984 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.682761 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I0930 03:58:55.684464 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.687325 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I0930 03:58:55.689037 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.879114 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I0930 03:58:55.881316 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.884200 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I0930 03:58:55.885962 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.893321 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I0930 03:58:55.895460 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.898235 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I0930 03:58:55.899969 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.907421 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I0930 03:58:55.909463 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.912196 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I0930 03:58:55.914128 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.921479 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I0930 03:58:55.923546 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.926436 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I0930 03:58:55.928178 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.932051 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I0930 03:58:55.933828 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.937913 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I0930 03:58:55.940333 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.943082 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I0930 03:58:55.945096 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:55.954891 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:55.961577 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I0930 03:58:55.963621 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.966363 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I0930 03:58:55.968093 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:55.970114 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:55.976688 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I0930 03:58:55.978817 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.981519 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I0930 03:58:55.983297 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.988108 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I0930 03:58:55.990041 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:55.992743 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I0930 03:58:55.994510 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.015531 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I0930 03:58:56.017590 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.020322 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I0930 03:58:56.022208 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.029576 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I0930 03:58:56.031620 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.034451 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I0930 03:58:56.036199 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.043554 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I0930 03:58:56.045620 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.048358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I0930 03:58:56.050133 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.057938 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I0930 03:58:56.059977 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.062843 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I0930 03:58:56.064651 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.068473 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I0930 03:58:56.070274 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.074319 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I0930 03:58:56.076043 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.078912 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I0930 03:58:56.080696 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.090487 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.097057 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I0930 03:58:56.099216 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.101956 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I0930 03:58:56.103722 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.105781 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.112914 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I0930 03:58:56.114985 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.117949 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I0930 03:58:56.119691 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.124741 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I0930 03:58:56.126496 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.129349 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I0930 03:58:56.131120 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.151621 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I0930 03:58:56.153861 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.156557 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I0930 03:58:56.158431 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.166347 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I0930 03:58:56.168406 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.171248 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I0930 03:58:56.173011 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.180359 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I0930 03:58:56.182485 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.185193 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I0930 03:58:56.186973 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.194408 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I0930 03:58:56.196446 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.199258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I0930 03:58:56.201129 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.204995 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I0930 03:58:56.206801 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.210870 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I0930 03:58:56.212614 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.215477 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I0930 03:58:56.217229 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.227542 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.234182 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I0930 03:58:56.236288 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.239044 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I0930 03:58:56.240802 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.242872 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.249441 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I0930 03:58:56.251516 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.254387 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I0930 03:58:56.256137 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.260974 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I0930 03:58:56.262741 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.265624 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I0930 03:58:56.267419 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.288444 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I0930 03:58:56.290512 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.293231 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I0930 03:58:56.295137 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.302481 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I0930 03:58:56.304534 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.307389 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I0930 03:58:56.309135 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.316454 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I0930 03:58:56.318586 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.321303 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I0930 03:58:56.323087 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.330587 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I0930 03:58:56.332640 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.335426 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I0930 03:58:56.337280 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.341215 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I0930 03:58:56.342997 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.347105 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I0930 03:58:56.348857 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.352175 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I0930 03:58:56.353973 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.363855 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.370450 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I0930 03:58:56.372570 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.375359 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I0930 03:58:56.377170 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.379245 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.385883 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I0930 03:58:56.387944 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.390805 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I0930 03:58:56.392575 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.397452 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I0930 03:58:56.399263 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.402219 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I0930 03:58:56.403979 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.425421 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I0930 03:58:56.427518 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.430280 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I0930 03:58:56.432137 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.439600 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I0930 03:58:56.441691 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.444535 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I0930 03:58:56.446329 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.453729 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I0930 03:58:56.455846 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.458604 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I0930 03:58:56.460367 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.468237 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I0930 03:58:56.470315 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.473095 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I0930 03:58:56.474997 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.478890 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I0930 03:58:56.480657 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.484819 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I0930 03:58:56.486602 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.489437 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I0930 03:58:56.491240 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.501422 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.508127 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I0930 03:58:56.510277 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.513036 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I0930 03:58:56.514835 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.516931 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.524116 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I0930 03:58:56.526494 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.529373 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I0930 03:58:56.531177 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.536064 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I0930 03:58:56.537871 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.540745 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I0930 03:58:56.542533 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.616591 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I0930 03:58:56.618671 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.621485 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I0930 03:58:56.623272 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.630634 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I0930 03:58:56.633191 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.635973 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I0930 03:58:56.637776 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.645121 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I0930 03:58:56.647211 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.650081 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I0930 03:58:56.651956 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.659321 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I0930 03:58:56.661374 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.664289 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I0930 03:58:56.666094 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.669970 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I0930 03:58:56.671765 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.675941 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I0930 03:58:56.677732 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.680586 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I0930 03:58:56.682388 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.692158 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.699369 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I0930 03:58:56.892858 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.896235 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I0930 03:58:56.898127 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:56.900323 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:56.907130 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I0930 03:58:56.909225 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.912122 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I0930 03:58:56.913948 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.918972 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I0930 03:58:56.920732 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.923686 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I0930 03:58:56.925454 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.946664 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I0930 03:58:56.948734 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.951605 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I0930 03:58:56.953387 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.960822 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I0930 03:58:56.962993 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.965766 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I0930 03:58:56.967558 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.974995 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I0930 03:58:56.977077 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.979857 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I0930 03:58:56.981767 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.989147 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I0930 03:58:56.991258 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.994176 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I0930 03:58:56.995950 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:56.999831 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I0930 03:58:57.001637 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.005839 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I0930 03:58:57.007610 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.010486 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I0930 03:58:57.012269 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.022713 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.029374 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I0930 03:58:57.031553 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.034338 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I0930 03:58:57.036131 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.038257 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.044868 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I0930 03:58:57.046980 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.049862 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I0930 03:58:57.051637 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.056568 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I0930 03:58:57.058377 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.061257 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I0930 03:58:57.063050 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.084698 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I0930 03:58:57.086797 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.089564 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I0930 03:58:57.091491 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.098872 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I0930 03:58:57.100944 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.103855 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I0930 03:58:57.105652 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.113023 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I0930 03:58:57.115191 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.117984 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I0930 03:58:57.119766 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.127233 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I0930 03:58:57.129319 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.132123 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I0930 03:58:57.134519 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.138469 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I0930 03:58:57.140248 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.144451 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I0930 03:58:57.146246 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.149096 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I0930 03:58:57.150905 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.160757 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.167457 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I0930 03:58:57.169636 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.172402 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I0930 03:58:57.174206 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.176298 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.183016 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I0930 03:58:57.185086 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.187983 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I0930 03:58:57.189796 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.194732 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I0930 03:58:57.196512 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.199939 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I0930 03:58:57.202135 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.222865 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I0930 03:58:57.224972 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.227748 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I0930 03:58:57.229681 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.237058 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I0930 03:58:57.239176 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.242137 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I0930 03:58:57.243910 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.251820 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I0930 03:58:57.254005 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.256784 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I0930 03:58:57.258592 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.266086 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I0930 03:58:57.268160 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.270999 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I0930 03:58:57.272894 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.276840 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I0930 03:58:57.278668 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.282915 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I0930 03:58:57.284700 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.287585 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I0930 03:58:57.289387 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.299320 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.305996 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I0930 03:58:57.308709 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.311536 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I0930 03:58:57.313342 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.315482 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.322432 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I0930 03:58:57.324530 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.327411 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I0930 03:58:57.329211 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.334249 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I0930 03:58:57.336013 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.338946 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I0930 03:58:57.340761 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.362095 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I0930 03:58:57.364186 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.366999 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I0930 03:58:57.368906 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.376360 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I0930 03:58:57.378471 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.381352 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I0930 03:58:57.383165 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.390607 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I0930 03:58:57.392764 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.395584 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I0930 03:58:57.397376 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.404911 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I0930 03:58:57.407057 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.409889 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I0930 03:58:57.411796 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.415740 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I0930 03:58:57.417566 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.421802 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I0930 03:58:57.423584 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.426507 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I0930 03:58:57.428314 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.438823 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.445499 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I0930 03:58:57.447693 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.450505 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I0930 03:58:57.452292 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.454450 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.461099 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I0930 03:58:57.463236 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.466152 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I0930 03:58:57.467971 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.472935 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I0930 03:58:57.474737 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.477679 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I0930 03:58:57.479486 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.500806 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I0930 03:58:57.502930 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.505727 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I0930 03:58:57.507647 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.515326 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I0930 03:58:57.517416 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.520332 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I0930 03:58:57.522159 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.529562 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I0930 03:58:57.531723 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.534535 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I0930 03:58:57.536337 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.543845 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I0930 03:58:57.545979 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.548802 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I0930 03:58:57.551204 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.555282 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I0930 03:58:57.557088 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.561298 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I0930 03:58:57.563101 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.566010 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I0930 03:58:57.567813 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.577710 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.584414 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I0930 03:58:57.586609 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.589387 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I0930 03:58:57.591256 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.593571 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.600212 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I0930 03:58:57.602362 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.605230 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I0930 03:58:57.607071 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.612062 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I0930 03:58:57.613895 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.617353 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I0930 03:58:57.619193 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.640007 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I0930 03:58:57.642131 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.644933 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I0930 03:58:57.646862 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.654293 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I0930 03:58:57.656390 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.659305 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I0930 03:58:57.661103 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.669026 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I0930 03:58:57.671242 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.674086 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I0930 03:58:57.675883 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.683397 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I0930 03:58:57.685510 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.688345 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I0930 03:58:57.690301 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.696643 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I0930 03:58:57.698495 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.702778 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I0930 03:58:57.704595 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.707509 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I0930 03:58:57.709340 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.719347 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.726075 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I0930 03:58:57.728888 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.731719 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I0930 03:58:57.733552 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.735705 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.742402 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I0930 03:58:57.744512 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.747438 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I0930 03:58:57.749261 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.754303 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I0930 03:58:57.756089 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.759040 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I0930 03:58:57.760856 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.782407 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I0930 03:58:57.784531 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.787367 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I0930 03:58:57.789296 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.796838 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I0930 03:58:57.798989 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.801920 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I0930 03:58:57.803739 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.811186 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I0930 03:58:57.813378 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.816200 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I0930 03:58:57.818046 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.825516 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I0930 03:58:57.827667 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.830533 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I0930 03:58:57.832470 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.836519 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I0930 03:58:57.838376 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.842741 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I0930 03:58:57.844546 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.847477 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I0930 03:58:57.849307 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.860051 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.866795 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I0930 03:58:57.868977 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.871896 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I0930 03:58:57.873761 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:57.875941 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:57.882641 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I0930 03:58:57.884783 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.887708 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I0930 03:58:57.889545 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.894633 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I0930 03:58:57.896620 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:57.899585 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I0930 03:58:57.901403 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.194284 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I0930 03:58:58.196584 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.199422 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I0930 03:58:58.201353 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.208767 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I0930 03:58:58.210909 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.213844 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I0930 03:58:58.215660 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.223107 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I0930 03:58:58.225299 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.228149 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I0930 03:58:58.229985 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.237469 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I0930 03:58:58.239615 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.242489 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I0930 03:58:58.244415 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.248932 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I0930 03:58:58.250784 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.255155 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I0930 03:58:58.256965 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.259894 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I0930 03:58:58.261739 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.271741 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.278541 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I0930 03:58:58.280731 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.283621 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I0930 03:58:58.285448 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.287648 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.294358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I0930 03:58:58.296485 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.299457 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I0930 03:58:58.301295 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.306416 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I0930 03:58:58.308216 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.311173 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I0930 03:58:58.312979 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.334253 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I0930 03:58:58.336385 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.339231 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I0930 03:58:58.341163 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.348678 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I0930 03:58:58.350830 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.353780 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I0930 03:58:58.355614 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.363114 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I0930 03:58:58.365826 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.368662 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I0930 03:58:58.370504 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.378027 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I0930 03:58:58.380150 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.383020 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I0930 03:58:58.384950 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.388985 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I0930 03:58:58.390853 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.395201 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I0930 03:58:58.397013 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.399955 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I0930 03:58:58.401823 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.411870 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.418633 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I0930 03:58:58.420832 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.423703 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I0930 03:58:58.425575 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.428279 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.435004 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I0930 03:58:58.437127 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.440060 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I0930 03:58:58.441928 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.447010 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I0930 03:58:58.448841 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.451830 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I0930 03:58:58.453663 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.474604 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I0930 03:58:58.476837 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.479671 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I0930 03:58:58.482152 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.489623 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I0930 03:58:58.491757 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.494706 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I0930 03:58:58.496552 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.504035 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I0930 03:58:58.506273 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.509090 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I0930 03:58:58.510940 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.518499 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I0930 03:58:58.520617 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.523494 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I0930 03:58:58.525453 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.529467 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I0930 03:58:58.531351 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.535704 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I0930 03:58:58.537552 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.540489 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I0930 03:58:58.542354 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.552992 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.559749 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I0930 03:58:58.561968 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.564798 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I0930 03:58:58.566679 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.568906 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.575618 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I0930 03:58:58.577780 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.580717 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I0930 03:58:58.582589 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.588053 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I0930 03:58:58.589900 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.592894 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I0930 03:58:58.594755 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.616305 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I0930 03:58:58.618473 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.621287 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I0930 03:58:58.623287 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.630762 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I0930 03:58:58.632888 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.635842 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I0930 03:58:58.637707 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.645140 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I0930 03:58:58.647380 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.650245 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I0930 03:58:58.652078 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.659574 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I0930 03:58:58.661734 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.664603 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I0930 03:58:58.666573 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.671078 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I0930 03:58:58.672930 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.677322 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I0930 03:58:58.679172 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.682128 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I0930 03:58:58.683977 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.694123 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.700883 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I0930 03:58:58.703103 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.705992 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I0930 03:58:58.707841 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.710094 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.716824 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I0930 03:58:58.719000 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.721942 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I0930 03:58:58.723794 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.728919 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I0930 03:58:58.730769 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.733764 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I0930 03:58:58.735597 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.757111 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I0930 03:58:58.759293 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.762173 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I0930 03:58:58.764132 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.771633 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I0930 03:58:58.773785 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.776738 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I0930 03:58:58.778590 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.786334 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I0930 03:58:58.789018 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.791955 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I0930 03:58:58.793899 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.801403 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I0930 03:58:58.803559 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.806446 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I0930 03:58:58.808392 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.812443 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I0930 03:58:58.814328 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.818745 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I0930 03:58:58.820574 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.823541 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I0930 03:58:58.825407 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.835591 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.842375 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I0930 03:58:58.844627 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.847653 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I0930 03:58:58.849519 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.852316 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.859080 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I0930 03:58:58.861233 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.864202 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I0930 03:58:58.866105 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.871265 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I0930 03:58:58.873127 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.876127 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I0930 03:58:58.878023 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.899302 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I0930 03:58:58.901459 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.904357 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I0930 03:58:58.906867 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.914373 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I0930 03:58:58.916528 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.919503 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I0930 03:58:58.921355 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.928860 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I0930 03:58:58.931083 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.933982 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I0930 03:58:58.935827 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.943437 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I0930 03:58:58.945613 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.948508 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I0930 03:58:58.950479 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.954530 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I0930 03:58:58.956410 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.960967 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I0930 03:58:58.962857 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.965822 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I0930 03:58:58.967689 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.978372 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:58.985148 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I0930 03:58:58.987408 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:58.990345 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I0930 03:58:58.992208 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:58.994479 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:59.001222 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I0930 03:58:59.003413 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.006411 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I0930 03:58:59.008275 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.013466 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I0930 03:58:59.015338 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.018354 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I0930 03:58:59.020208 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.041913 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I0930 03:58:59.044059 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.046943 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I0930 03:58:59.048904 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.056403 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I0930 03:58:59.058608 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.061590 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I0930 03:58:59.063442 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.071070 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I0930 03:58:59.073308 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.076195 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I0930 03:58:59.078065 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.085614 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I0930 03:58:59.087794 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.090746 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I0930 03:58:59.092707 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.097286 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I0930 03:58:59.099194 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.103687 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I0930 03:58:59.105547 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.108511 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I0930 03:58:59.110414 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:59.120569 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:59.127384 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I0930 03:58:59.129678 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.132558 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I0930 03:58:59.134451 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:59.136717 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:59.143463 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I0930 03:58:59.145714 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.148702 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I0930 03:58:59.150582 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.155766 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I0930 03:58:59.157635 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.160654 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I0930 03:58:59.162528 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.184123 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I0930 03:58:59.186324 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.189221 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I0930 03:58:59.191282 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.198807 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I0930 03:58:59.200981 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.203993 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I0930 03:58:59.205884 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.213339 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I0930 03:58:59.216100 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.219025 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I0930 03:58:59.220881 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.228446 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I0930 03:58:59.230644 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.233579 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I0930 03:58:59.235558 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.239680 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I0930 03:58:59.241580 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.246040 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I0930 03:58:59.247917 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.250919 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I0930 03:58:59.252796 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:59.262980 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:59.269816 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I0930 03:58:59.272058 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.274990 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I0930 03:58:59.276851 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:59.279689 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:59.286468 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I0930 03:58:59.288649 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.292673 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I0930 03:58:59.294582 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.299790 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I0930 03:58:59.301670 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.304687 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I0930 03:58:59.306584 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.381370 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I0930 03:58:59.383659 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.386574 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I0930 03:58:59.388453 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.645352 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I0930 03:58:59.647993 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.651214 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I0930 03:58:59.653213 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.660910 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I0930 03:58:59.663119 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.666168 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I0930 03:58:59.668035 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.675612 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I0930 03:58:59.677883 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.680822 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I0930 03:58:59.682731 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.686889 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I0930 03:58:59.688900 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.693518 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I0930 03:58:59.695419 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.698418 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I0930 03:58:59.700309 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:59.711310 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:59.718311 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I0930 03:58:59.720504 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.723441 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I0930 03:58:59.725309 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:59.727622 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:59.734426 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I0930 03:58:59.736897 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.739833 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I0930 03:58:59.741732 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.746973 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I0930 03:58:59.749026 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.751908 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I0930 03:58:59.753812 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.775624 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I0930 03:58:59.777831 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.780805 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I0930 03:58:59.782913 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.790445 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I0930 03:58:59.792618 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.795634 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I0930 03:58:59.797486 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.805079 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I0930 03:58:59.807285 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.810225 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I0930 03:58:59.812094 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.819679 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I0930 03:58:59.821888 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.824951 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I0930 03:58:59.826858 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.830981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I0930 03:58:59.832877 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.837422 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I0930 03:58:59.839318 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.842782 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I0930 03:58:59.844700 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:59.854931 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:59.861740 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I0930 03:58:59.863987 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.866927 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I0930 03:58:59.868807 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:59.871127 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:58:59.877968 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I0930 03:58:59.880146 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.883151 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I0930 03:58:59.885025 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.890306 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I0930 03:58:59.892177 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.895220 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I0930 03:58:59.897098 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.918798 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I0930 03:58:59.920991 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.923913 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I0930 03:58:59.925915 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.933386 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I0930 03:58:59.935589 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.938615 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I0930 03:58:59.940477 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.947996 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I0930 03:58:59.950272 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.953185 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I0930 03:58:59.955084 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.963104 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I0930 03:58:59.965281 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.968229 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I0930 03:58:59.970252 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.974363 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I0930 03:58:59.976410 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.980926 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I0930 03:58:59.982829 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:58:59.985826 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I0930 03:58:59.987728 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:58:59.997991 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.004786 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I0930 03:59:00.007081 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.010036 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I0930 03:59:00.012008 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.014344 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.021736 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I0930 03:59:00.023947 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.026987 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I0930 03:59:00.028893 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.034187 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I0930 03:59:00.036082 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.039140 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I0930 03:59:00.041034 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.062515 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I0930 03:59:00.064705 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.067638 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I0930 03:59:00.069658 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.077780 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I0930 03:59:00.079991 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.083274 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I0930 03:59:00.085140 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.092698 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I0930 03:59:00.094985 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.097897 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I0930 03:59:00.099783 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.107324 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I0930 03:59:00.109509 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.116938 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I0930 03:59:00.118965 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.123195 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I0930 03:59:00.125086 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.129676 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I0930 03:59:00.131580 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.134599 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I0930 03:59:00.136492 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.147404 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.154256 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I0930 03:59:00.156526 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.159482 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I0930 03:59:00.161373 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.163721 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.170566 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I0930 03:59:00.172773 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.175851 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I0930 03:59:00.177780 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.183057 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I0930 03:59:00.184965 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.188019 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I0930 03:59:00.189925 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.211956 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I0930 03:59:00.214193 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.217112 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I0930 03:59:00.219139 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.226693 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I0930 03:59:00.228893 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.231924 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I0930 03:59:00.233855 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.241384 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I0930 03:59:00.243690 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.246643 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I0930 03:59:00.248537 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.256160 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I0930 03:59:00.258400 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.261375 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I0930 03:59:00.263395 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.267610 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I0930 03:59:00.269515 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.274127 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I0930 03:59:00.276021 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.279488 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I0930 03:59:00.281401 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.291791 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.298887 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I0930 03:59:00.301181 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.304141 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I0930 03:59:00.306093 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.308453 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.315307 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I0930 03:59:00.317542 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.320601 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I0930 03:59:00.322533 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.327853 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I0930 03:59:00.329872 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.332907 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I0930 03:59:00.334846 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.356929 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I0930 03:59:00.359153 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.362087 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I0930 03:59:00.364094 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.371669 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I0930 03:59:00.373951 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.376991 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I0930 03:59:00.378915 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.386473 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I0930 03:59:00.388748 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.391732 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I0930 03:59:00.393677 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.401747 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I0930 03:59:00.403970 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.406970 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I0930 03:59:00.408969 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.413269 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I0930 03:59:00.415229 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.419861 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I0930 03:59:00.421805 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.424880 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I0930 03:59:00.426831 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.437415 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.444344 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I0930 03:59:00.446660 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.449656 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I0930 03:59:00.451572 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.453955 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.461454 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I0930 03:59:00.463721 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.466773 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I0930 03:59:00.468688 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.474076 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I0930 03:59:00.475976 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.479045 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I0930 03:59:00.480961 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.502686 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I0930 03:59:00.504925 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.507873 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I0930 03:59:00.509912 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.518040 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I0930 03:59:00.520272 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.523314 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I0930 03:59:00.525204 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.532741 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I0930 03:59:00.535057 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.538027 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I0930 03:59:00.539942 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.547557 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I0930 03:59:00.549802 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.552749 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I0930 03:59:00.554794 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.559066 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I0930 03:59:00.560969 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.565618 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I0930 03:59:00.567521 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.570555 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I0930 03:59:00.572486 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.583428 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.590287 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I0930 03:59:00.592575 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.595586 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I0930 03:59:00.597503 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.599887 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.606734 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I0930 03:59:00.608949 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.612014 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I0930 03:59:00.613949 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.619307 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I0930 03:59:00.621232 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.624321 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I0930 03:59:00.626295 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.648577 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I0930 03:59:00.650849 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.653812 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I0930 03:59:00.655843 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.663406 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I0930 03:59:00.665645 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.668691 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I0930 03:59:00.670649 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.678232 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I0930 03:59:00.680536 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.683500 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I0930 03:59:00.685416 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.693067 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I0930 03:59:00.695335 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.698362 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I0930 03:59:00.700389 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.704680 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I0930 03:59:00.706649 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.711428 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I0930 03:59:00.713352 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.716932 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I0930 03:59:00.718886 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.729438 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.736280 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I0930 03:59:00.738656 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.741662 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I0930 03:59:00.743590 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.746005 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.752808 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I0930 03:59:00.755074 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.758167 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I0930 03:59:00.760105 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.765636 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I0930 03:59:00.767583 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.770676 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I0930 03:59:00.772589 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.778317 140278725625664 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.785067 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I0930 03:59:00.787309 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.788203 140278725625664 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.795523 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I0930 03:59:00.797823 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.798743 140278725625664 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.805489 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I0930 03:59:00.807766 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.808670 140278725625664 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.815553 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I0930 03:59:00.817849 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.818757 140278725625664 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.825491 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I0930 03:59:00.827729 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.828628 140278725625664 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.835388 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I0930 03:59:00.837646 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.838553 140278725625664 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.845273 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I0930 03:59:00.847549 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.848439 140278725625664 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.855193 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I0930 03:59:00.857415 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.858335 140278725625664 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.865060 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I0930 03:59:00.867314 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.868216 140278725625664 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.875561 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I0930 03:59:00.877804 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.878727 140278725625664 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.885396 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I0930 03:59:00.887646 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.888539 140278725625664 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.895285 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I0930 03:59:00.897518 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.898440 140278725625664 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.905125 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I0930 03:59:00.907357 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.908251 140278725625664 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.914982 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I0930 03:59:00.917221 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.918156 140278725625664 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.924823 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I0930 03:59:00.927084 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 03:59:00.927977 140278725625664 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I0930 03:59:00.934741 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I0930 03:59:00.936951 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.939973 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I0930 03:59:00.942013 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.944913 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I0930 03:59:00.946867 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.950368 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I0930 03:59:00.952287 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.955263 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I0930 03:59:00.957151 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.960228 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I0930 03:59:00.962176 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.965104 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I0930 03:59:00.967051 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.970121 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I0930 03:59:00.972030 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.974970 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I0930 03:59:00.977025 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.979991 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I0930 03:59:00.981950 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.984983 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I0930 03:59:00.986912 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.989861 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I0930 03:59:00.991777 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.994851 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I0930 03:59:00.996757 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:00.999718 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I0930 03:59:01.001649 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:01.004649 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I0930 03:59:01.006595 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:01.009563 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I0930 03:59:01.011569 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:01.014503 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I0930 03:59:01.016618 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:01.998571 140278725625664 py_utils.py:1478] === worker 0 ===
I0930 03:59:02.014901 140278725625664 py_utils.py:1468] worker 0: global_step                                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015007 140278725625664 py_utils.py:1468] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015071 140278725625664 py_utils.py:1468] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015126 140278725625664 py_utils.py:1468] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015178 140278725625664 py_utils.py:1468] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015229 140278725625664 py_utils.py:1468] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015288 140278725625664 py_utils.py:1468] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015339 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015387 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015434 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015482 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015529 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015578 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015626 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015675 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015723 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015772 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015820 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015868 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015916 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.015964 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016012 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016059 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016108 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016159 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016208 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016257 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016305 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016352 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016400 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016448 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016495 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016542 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016590 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016638 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016685 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016733 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016780 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016827 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016875 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016922 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.016969 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017016 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017067 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017115 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017162 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017209 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017256 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017304 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017351 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017398 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017445 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017493 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017561 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017613 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017662 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017709 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017757 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017804 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017851 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017898 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017944 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.017997 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018045 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018091 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018138 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018185 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018232 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018278 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018324 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018370 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018416 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018464 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018510 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018557 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018604 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018651 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018697 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018744 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018791 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018846 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018895 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018943 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.018990 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019037 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019084 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019131 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019178 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019225 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019272 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019318 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019365 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019411 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019458 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019505 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019552 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019598 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019645 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019692 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019745 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019793 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019840 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019888 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019935 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.019982 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020029 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020076 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020123 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020170 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020218 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020265 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020313 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020360 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020408 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020455 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020503 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020550 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020602 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020651 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020698 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020746 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020793 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020840 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020888 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020935 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.020982 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021029 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021077 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021124 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021171 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021218 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021266 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021313 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021361 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021408 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021455 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021507 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021579 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021628 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021676 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021723 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021770 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021817 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021864 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021911 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.021958 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022005 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022051 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022099 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022146 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022193 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022240 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022287 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022335 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022387 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022435 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022483 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022530 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022578 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022625 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022673 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022720 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022767 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022814 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022861 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022908 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.022954 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023002 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023049 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023096 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023143 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023190 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023237 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023289 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023340 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023388 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023435 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023483 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023530 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023577 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023624 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023672 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023719 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023766 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023814 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023861 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023908 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.023955 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024003 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024050 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024098 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024149 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024198 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024245 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024292 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024340 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024387 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024435 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024482 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024530 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024577 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024625 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024672 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024718 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024765 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024813 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024860 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024907 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.024954 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025002 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025053 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025102 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025149 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025196 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025244 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025291 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025339 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025386 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025434 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025481 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025542 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025597 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025645 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025693 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025741 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025789 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025835 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025882 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025935 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.025983 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026030 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026077 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026125 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026172 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026219 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026266 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026313 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026360 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026407 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026454 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026502 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026549 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026596 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026643 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026690 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026737 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026785 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026836 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026884 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026932 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.026979 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027026 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027073 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027120 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027167 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027213 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027260 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027307 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027354 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027401 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027448 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027495 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027541 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027588 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027634 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027686 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027735 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027782 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027830 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027877 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027924 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.027971 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028018 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028066 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028113 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028160 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028208 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028255 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028302 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028349 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028396 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028443 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028490 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028538 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028589 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028637 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028684 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028731 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028778 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028825 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028872 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028919 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.028966 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029012 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029059 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029106 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029154 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029200 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029248 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029295 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029343 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029390 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029444 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029493 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029556 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029608 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029656 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029704 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029751 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029799 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029845 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029892 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029939 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.029986 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030033 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030080 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030127 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030173 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030219 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030266 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030313 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030364 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030412 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030459 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030507 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030553 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030600 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030647 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030694 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030741 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030788 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030836 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030883 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030930 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.030977 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031024 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031071 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031119 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031166 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031219 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031271 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031319 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031367 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031414 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031461 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031508 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031555 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031602 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031649 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031696 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031743 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031789 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031837 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031883 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031931 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.031978 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032025 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032071 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032123 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032170 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032217 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032264 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032310 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032357 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032404 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032451 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032498 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032545 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032592 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032639 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032685 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032732 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032778 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032825 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032872 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032919 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.032966 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033017 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033065 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033112 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033159 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033206 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033253 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033300 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033347 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033397 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033445 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033493 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033563 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033615 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033663 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033711 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033758 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033805 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033853 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033906 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.033954 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034001 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034048 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034095 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034142 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034190 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034236 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034284 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034331 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034378 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034425 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034472 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034519 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034566 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034613 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034660 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034707 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034755 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034807 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034855 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034902 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034950 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.034997 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035044 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035091 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035138 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035185 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035232 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035279 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035326 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035372 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035418 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035465 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035511 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035558 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035604 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035654 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035702 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035748 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035795 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035842 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035889 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035936 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.035982 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036029 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036076 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036123 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036170 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036217 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036264 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036312 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036359 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036406 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036452 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036499 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036550 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036598 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036645 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036692 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036739 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036786 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036833 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036880 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036926 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.036974 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037020 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037067 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037114 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037162 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037209 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037256 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037303 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037350 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037402 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037450 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037497 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037560 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037611 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037659 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037705 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037752 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037799 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037847 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037894 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037941 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.037989 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038035 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038083 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038130 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038177 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038224 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038271 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038323 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038371 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038417 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038463 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038511 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038558 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038605 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038652 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038698 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038745 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038791 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038838 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038885 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038932 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.038979 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039026 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039073 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039121 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039172 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039220 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039267 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039314 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039361 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039408 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039455 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039502 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039549 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039597 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039644 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039691 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039738 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039786 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039833 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039880 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039927 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.039974 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040022 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040074 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040122 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040169 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040216 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040263 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040310 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040357 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040404 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040450 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040497 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040544 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040591 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040638 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040686 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040733 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040780 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040826 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040873 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040925 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.040973 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041020 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041067 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041113 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041160 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041207 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041254 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041301 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041348 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041396 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041443 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041490 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041553 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041605 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041652 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041699 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041747 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041794 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041846 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041893 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041940 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.041987 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042034 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042081 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042127 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042174 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042221 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042267 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042314 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042361 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042408 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042455 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042501 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042548 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042596 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042643 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042694 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042742 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042790 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042836 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042883 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042929 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.042976 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043023 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043070 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043117 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043164 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043210 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043257 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043304 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043351 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043398 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043448 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043496 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043543 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043595 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043642 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043689 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043736 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043782 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043829 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043876 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043923 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.043970 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044018 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044065 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044112 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044159 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044206 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044253 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044301 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044348 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044395 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044442 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044492 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044540 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044587 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044634 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044681 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044728 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044775 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044821 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044868 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044914 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.044960 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045007 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045054 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045101 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045147 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045194 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045241 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045287 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045338 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045386 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045432 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045479 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045541 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045595 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045643 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045690 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045737 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045784 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045830 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045877 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045925 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.045972 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046019 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046065 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046112 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046158 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046205 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046260 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046308 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046354 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046401 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046447 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046494 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046540 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046587 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046634 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046681 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046728 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046775 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046822 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046869 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046915 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.046962 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047009 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047055 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047107 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047154 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047202 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047249 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047296 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047343 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047390 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047436 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047483 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047530 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047577 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047623 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047670 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047717 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047764 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047810 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047857 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047904 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.047951 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048002 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048049 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048097 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048143 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048189 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048236 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048282 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048330 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048376 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048422 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048469 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048516 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048563 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048610 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048657 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048703 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048750 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048796 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048847 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048894 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048942 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.048989 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049035 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049082 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049129 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049176 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049223 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049270 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049317 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049365 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049411 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049458 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049505 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049570 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049618 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049666 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049713 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049766 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049814 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049861 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049908 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.049955 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050003 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050050 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050097 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050144 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050192 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050239 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050286 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050333 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050379 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050426 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050473 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050520 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050566 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050617 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050666 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050713 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050760 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050807 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050854 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050902 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050948 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.050995 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051042 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051089 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051136 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051183 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051230 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051278 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051324 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051371 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051418 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051464 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051516 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051565 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051612 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051658 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051705 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051751 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051798 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051846 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051893 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051940 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.051988 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052035 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052082 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052129 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052176 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052223 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052270 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052317 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052369 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052417 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052464 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052511 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052558 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052605 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052651 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052698 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052745 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052792 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052838 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052885 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052932 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.052978 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053025 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053072 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053119 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053166 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053212 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053265 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053313 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053360 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053407 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053455 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053520 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053569 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053616 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053663 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053710 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053757 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053804 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053851 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053898 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053946 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.053993 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054041 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054088 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054141 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054189 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054237 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054285 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054332 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054379 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054427 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054474 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054521 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054568 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054615 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054662 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054708 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054756 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054803 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054850 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054897 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054944 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.054991 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055043 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055091 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055139 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055186 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055233 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055279 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055326 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055373 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055420 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055467 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055514 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055561 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055608 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055655 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055702 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055749 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055796 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055843 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055894 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055942 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.055989 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056036 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056083 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056130 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056177 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056225 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056272 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056319 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056366 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056413 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056459 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056506 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056553 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056600 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056647 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056694 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056741 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056791 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056839 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056886 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056933 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.056980 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057027 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057073 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057120 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057167 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057214 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057260 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057307 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057353 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057399 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057445 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057491 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057556 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057607 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057655 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057707 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057755 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057802 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057849 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057895 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057942 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.057989 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058036 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058082 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058129 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058176 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058223 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058269 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058316 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058363 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058409 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058456 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058503 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058554 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058602 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058648 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058696 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058743 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058789 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058836 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058883 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058929 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.058976 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059023 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059070 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059117 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059163 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059210 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059256 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059303 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059349 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059396 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059446 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059494 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059540 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059587 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059633 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059680 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059726 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059772 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059819 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059866 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059912 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.059958 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060005 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060051 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060097 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060144 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060190 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060236 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060287 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060334 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060381 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060427 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060474 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060520 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060567 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060614 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060661 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060707 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060753 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060800 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060847 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060894 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060940 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.060987 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061033 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061080 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061127 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061177 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061224 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061276 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061323 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061370 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061416 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061463 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061510 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061574 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061623 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061670 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061717 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061764 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061811 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061859 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061905 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061952 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.061999 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062050 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062098 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062145 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062192 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062238 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062284 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062331 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062378 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062424 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062471 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062517 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062564 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062610 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062658 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062704 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062752 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062799 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062846 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062894 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062945 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.062994 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063040 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063088 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063135 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063182 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063230 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063278 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063324 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063372 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063418 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063464 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063512 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063562 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063609 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063656 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063703 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063750 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063804 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063853 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063899 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063946 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.063993 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064040 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064087 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064133 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064180 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064227 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064273 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064320 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064367 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064414 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064460 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064507 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064554 140278725625664 py_utils.py:1468] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 03:59:02.064627 140278725625664 py_utils.py:1484] ==========
I0930 03:59:04.751599 140278725625664 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 03:59:07.070250 140278725625664 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:59:09.683366 140278725625664 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:59:11.933131 140278725625664 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:59:14.735391 140278725625664 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W0930 03:59:16.730118 140278725625664 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:59:16.859824 140278725625664 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 03:59:19.313997 140278725625664 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:59:19.443364 140278725625664 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 03:59:21.394665 140278725625664 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:59:21.540521 140278725625664 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 03:59:24.111577 140278725625664 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 03:59:24.625974 140278725625664 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 03:59:27.292732 140278725625664 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:59:30.598153 140278725625664 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:59:33.906064 140278725625664 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:59:36.042507 140278725625664 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I0930 03:59:36.047048 140278725625664 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I0930 03:59:36.143251 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I0930 03:59:36.145232 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I0930 03:59:36.154325 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I0930 03:59:36.154426 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.154496 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.154558 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.154616 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.154672 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.154728 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.154783 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.154840 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.154894 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.154949 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.155003 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.155057 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.155123 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.155179 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.155233 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.155287 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.155340 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.155393 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.155446 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.155499 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.155552 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.155605 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.155657 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.155710 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.155763 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.155818 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.155872 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.155925 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.155978 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.156030 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.156084 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.156137 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.156190 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.156243 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.156296 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.156349 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.156407 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.156461 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.156514 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.156567 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.156619 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.156672 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.156728 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.156781 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.156834 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.156887 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.156940 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.156993 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.157046 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.157098 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.157151 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.157203 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.157262 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.157315 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.157368 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.157420 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.157473 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.157541 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.157603 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.157658 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.157712 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.157771 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.157825 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.157879 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.157932 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.157985 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.158039 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.158091 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.158144 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.158197 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.158251 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.158304 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.158357 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.158410 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.158463 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.158517 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.158571 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.158625 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.158677 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.158730 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.158782 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.158835 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.158888 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.158941 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.158993 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.159054 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.159112 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.159167 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.159219 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.159272 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.159325 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.159378 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.159431 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.159485 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.159538 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.159592 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.159645 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.159698 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.159751 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.159804 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.159857 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.159909 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.159962 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.160014 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.160067 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.160119 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.160171 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.160224 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.160276 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.160330 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.160388 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.160443 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.160495 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.160552 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.160604 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.160656 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.160709 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.160762 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.160815 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.160867 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.160920 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.160973 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.161026 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.161078 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.161131 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.161183 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.161235 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.161289 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.161342 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.161394 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.161447 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.161499 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.161577 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.161632 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.161692 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.161746 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.161799 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.161852 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.161904 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.161957 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.162010 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.162062 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.162114 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.162168 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.162221 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.162275 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.162328 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.162381 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.162434 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.162487 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.162539 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.162591 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.162643 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.162696 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.162749 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.162801 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.162854 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.162906 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.162959 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.163016 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.163070 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.163123 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.163175 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.163228 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.163280 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.163333 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.163385 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.163438 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.163491 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.163544 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.163596 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.163649 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.163702 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.163753 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.163806 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.163858 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.163910 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.163963 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.164016 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.164068 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.164120 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.164172 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.164224 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.164281 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.164334 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.164386 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.164438 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.164490 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.164541 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.164593 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.164645 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.164697 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.164748 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.164800 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.164853 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.164905 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.164957 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.165009 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.165062 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.165114 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.165166 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.165218 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.165270 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.165322 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.165374 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.165426 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.165479 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.165544 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.165608 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.165662 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.165714 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.165767 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.165820 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.165873 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.165926 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.165979 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.166031 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.166084 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.166137 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.166190 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.166243 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.166294 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.166346 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.166398 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.166450 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.166502 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.166554 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.166606 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.166659 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.166712 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.166764 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.166817 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.166875 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.166927 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.166980 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.167033 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.167087 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.167139 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.167191 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.167244 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.167297 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.167349 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.167402 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.167455 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.167508 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.167560 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.167613 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.167665 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.167718 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.167770 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.167822 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.167874 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.167927 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.167979 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.168031 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.168083 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.168136 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.168193 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.168246 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.168298 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.168351 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.168403 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.168456 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.168508 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.168561 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.168613 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.168665 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.168718 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.168770 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.168821 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.168873 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.168925 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.168977 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.169029 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.169081 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.169133 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.169190 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.169243 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.169295 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.169348 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.169401 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.169459 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.169513 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.169585 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.169640 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.169693 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.169746 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.169798 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.169851 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.169903 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.169956 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.170008 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.170065 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.170119 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.170171 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.170224 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.170277 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.170330 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.170383 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.170435 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.170488 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.170540 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.170592 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.170645 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.170696 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.170748 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.170806 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.170860 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.170912 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.170964 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.171017 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.171069 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.171121 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.171175 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.171228 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.171282 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.171334 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.171386 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.171439 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.171491 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.171543 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.171596 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.171648 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.171700 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.171753 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.171806 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.171858 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.171909 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.171961 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.172013 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.172070 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.172126 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.172188 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.172242 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.172294 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.172347 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.172400 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.172452 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.172503 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.172554 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.172606 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.172658 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.172711 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.172765 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.172816 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.172869 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.172921 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.172976 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.173028 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.173081 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.173133 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.173186 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.173238 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.173291 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.173343 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.173400 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.173453 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.173506 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.173583 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.173637 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.173690 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.173743 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.173796 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.173848 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.173902 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.173955 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.174008 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.174061 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.174113 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.174165 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.174218 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.174270 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.174322 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.174374 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.174426 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.174479 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.174530 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.174582 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.174634 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.174692 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.174745 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.174797 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.174848 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.174900 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.174952 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.175004 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.175055 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.175107 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.175158 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.175208 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.175260 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.175311 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.175364 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.175415 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.175467 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.175519 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.175571 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.175623 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.175675 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.175728 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.175780 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.175832 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.175884 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.175940 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.175994 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.176045 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.176096 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.176148 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.176200 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.176252 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.176305 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.176356 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.176408 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.176460 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.176521 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.176573 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.176626 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.176678 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.176730 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.176783 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.176835 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.176886 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.176939 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.176991 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.177043 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.177095 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.177146 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.177198 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.177256 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.177309 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.177361 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.177413 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.177465 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.177516 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.177593 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.177647 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.177700 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.177752 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.177805 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.177858 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.177911 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.177963 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.178015 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.178067 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.178119 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.178173 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.178225 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.178277 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.178329 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.178382 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.178434 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.178486 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.178544 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.178598 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.178651 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.178703 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.178756 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.178808 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.178861 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.178913 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.178965 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.179017 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.179069 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.179122 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.179173 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.179229 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.179282 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.179335 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.179387 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.179439 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.179491 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.179543 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.179595 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.179647 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.179699 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.179751 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.179804 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.179861 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.179913 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.179966 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.180018 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.180071 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.180123 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.180175 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.180228 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.180280 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.180333 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.180385 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.180437 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.180489 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.180541 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.180593 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.180645 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.180697 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.180749 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.180801 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.180853 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.180905 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.180957 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.181009 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.181062 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.181119 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.181174 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.181226 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.181278 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.181331 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.181383 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.181435 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.181488 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.181556 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.181614 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.181667 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.181720 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.181772 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.181824 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.181875 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.181926 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.181978 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.182031 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.182084 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.182136 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.182188 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.182240 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.182291 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.182344 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.182396 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.182453 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.182507 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 03:59:36.182559 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 03:59:36.182612 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 03:59:36.182664 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 03:59:36.182716 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I0930 03:59:36.182768 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I0930 03:59:36.182819 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 03:59:36.182872 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 03:59:36.182924 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 03:59:36.182977 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 03:59:36.183029 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 03:59:36.183081 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 03:59:36.183133 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 03:59:36.183186 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 03:59:36.183238 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 03:59:36.183290 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I0930 03:59:36.183342 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I0930 03:59:36.183396 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I0930 03:59:36.183454 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I0930 03:59:36.183508 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I0930 03:59:36.183562 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I0930 03:59:36.183615 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I0930 03:59:36.183667 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I0930 03:59:36.183720 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I0930 03:59:36.183773 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I0930 03:59:36.183830 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I0930 03:59:36.183883 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I0930 03:59:36.183936 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I0930 03:59:36.183988 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I0930 03:59:36.184039 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I0930 03:59:36.184091 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I0930 03:59:36.184143 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I0930 03:59:36.184195 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I0930 03:59:36.184247 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I0930 03:59:36.184298 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I0930 03:59:36.184350 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I0930 03:59:36.184401 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I0930 03:59:36.184453 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I0930 03:59:36.184506 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I0930 03:59:36.184558 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I0930 03:59:36.184610 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I0930 03:59:36.184662 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I0930 03:59:36.184715 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I0930 03:59:36.184767 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I0930 03:59:36.184818 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I0930 03:59:36.184870 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I0930 03:59:36.184922 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I0930 03:59:36.184973 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I0930 03:59:36.185025 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I0930 03:59:42.095020 140278725625664 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:59:48.363343 140278725625664 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 03:59:54.384130 140278725625664 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:00:00.530259 140278725625664 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 04:00:12.445674 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I0930 04:00:12.445938 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.446027 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.446110 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.446180 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.446249 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.446316 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.446381 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.446446 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.446515 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.446578 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.446646 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.446709 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.446776 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.446839 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.446918 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.446983 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.447046 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.447108 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.447171 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.447237 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.447300 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.447365 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.447427 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.447488 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.447576 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.447650 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.447714 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.447780 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.447842 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.447914 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.447978 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.448043 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.448106 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.448169 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.448231 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.448293 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.448358 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.448421 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.448486 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.448548 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.448610 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.448672 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.448737 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.448800 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.448872 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.448935 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.449001 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.449063 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.449129 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.449192 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.449254 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.449316 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.449379 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.449444 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.449507 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.449602 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.449667 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.449730 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.449792 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.449866 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.449930 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.449998 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.450061 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.450127 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.450190 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.450256 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.450318 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.450381 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.450443 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.450505 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.450569 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.450631 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.450695 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.450763 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.450825 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.450888 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.450954 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.451018 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.451084 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.451146 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.451211 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.451273 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.451339 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.451401 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.451464 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.451526 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.451588 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.451654 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.451722 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.451789 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.451852 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.451914 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.451977 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.452044 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.452106 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.452172 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.452234 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.452300 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.452362 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.452426 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.452488 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.452551 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.452612 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.452681 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.452748 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.452811 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.452876 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.452939 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.453000 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.453063 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.453129 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.453192 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.453256 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.453319 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.453384 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.453447 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.453512 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.453602 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.453666 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.453727 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.453789 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.453854 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.453916 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.453980 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.454043 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.454104 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.454166 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.454232 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.454293 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.454359 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.454421 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.454485 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.454552 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.454619 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.454681 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.454743 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.454807 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.454871 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.454936 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.454999 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.455064 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.455126 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.455188 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.455250 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.455315 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.455377 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.455445 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.455512 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.455579 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.455643 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.455709 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.455772 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.455834 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.455897 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.455959 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.456024 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.456087 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.456151 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.456213 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.456276 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.456338 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.456404 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.456472 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.456538 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.456599 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.456664 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.456726 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.456790 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.456852 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.456913 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.456975 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.457037 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.457103 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.457166 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.457231 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.457293 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.457365 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.457429 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.457495 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.457577 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.457647 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.457710 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.457776 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.457838 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.457903 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.457967 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.458028 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.458089 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.458150 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.458215 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.458276 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.458346 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.458410 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.458471 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.458534 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.458600 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.458661 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.458727 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.458788 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.458853 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.458916 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.458982 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.459043 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.459105 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.459166 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.459233 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.459300 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.459362 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.459427 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.459490 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.459551 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.459614 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.459680 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.459742 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.459808 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.459871 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.459936 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.459998 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.460063 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.460126 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.460194 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.460256 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.460319 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.460385 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.460448 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.460514 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.460576 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.460638 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.460699 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.460766 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.460829 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.460894 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.460958 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.461023 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.461086 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.461157 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.461220 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.461282 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.461344 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.461407 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.461472 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.461555 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.461627 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.461692 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.461755 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.461817 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.461884 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.461947 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.462013 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.462081 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.462149 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.462212 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.462277 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.462339 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.462402 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.462464 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.462526 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.462591 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.462653 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.462718 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.462780 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.462843 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.462905 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.462972 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.463039 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.463107 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.463169 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.463235 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.463297 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.463363 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.463424 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.463486 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.463548 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.463611 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.463676 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.463740 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.463805 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.463868 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.463931 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.463998 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.464065 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.464128 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.464196 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.464259 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.464324 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.464388 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.464454 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.464516 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.464578 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.464641 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.464703 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.464768 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.464835 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.464902 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.464971 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.465034 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.465096 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.465163 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.465227 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.465293 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.465356 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.465422 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.465484 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.465567 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.465633 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.465696 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.465757 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.465819 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.465889 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.465953 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.466019 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.466082 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.466144 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.466206 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.466271 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.466334 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.466399 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.466461 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.466527 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.466589 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.466654 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.466716 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.466777 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.466843 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.466906 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.466972 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.467035 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.467100 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.467163 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.467223 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.467284 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.467350 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.467411 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.467477 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.467538 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.467604 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.467666 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.467735 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.467798 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.467860 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.467921 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.467982 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.468047 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.468110 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.468175 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.468236 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.468299 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.468361 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.468426 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.468488 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.468554 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.468615 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.468686 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.468749 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.468814 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.468876 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.468937 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.468999 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.469061 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.469126 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.469188 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.469253 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.469315 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.469377 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.469437 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.469503 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.469585 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.469657 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.469721 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.469788 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.469850 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.469916 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.469977 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.470039 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.470101 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.470163 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.470228 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.470290 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.470355 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.470418 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.470480 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.470547 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.470614 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.470677 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.470742 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.470804 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.470870 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.470932 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.470997 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.471058 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.471120 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.471181 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.471242 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.471308 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.471370 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.471435 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.471502 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.471564 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.471626 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.471691 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.471753 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.471818 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.471879 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.471944 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.472006 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.472072 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.472139 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.472201 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.472263 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.472325 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.472390 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.472459 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.472525 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.472588 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.472650 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.472712 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.472778 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.472840 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.472906 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.472969 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.473034 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.473097 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.473162 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.473224 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.473286 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.473353 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.473415 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.473481 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.473565 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.473634 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.473697 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.473758 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.473820 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.473886 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.473948 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.474014 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.474076 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.474142 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.474205 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.474269 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.474337 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.474400 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.474461 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.474524 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.474590 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.474653 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.474717 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.474780 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.474841 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.474906 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.474972 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.475035 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.475100 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.475162 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.475232 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.475295 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.475359 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.475420 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.475481 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.475542 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.475604 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.475669 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.475731 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.475795 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.475857 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.475919 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.475980 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.476046 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.476108 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.476178 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.476241 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.476307 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.476369 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.476433 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.476495 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.476556 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.476617 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.476679 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.476743 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.476805 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.476870 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.476932 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.476993 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.477055 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.477126 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.477189 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.477255 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.477317 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.477384 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.477446 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.477511 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.477593 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.477657 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.477719 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.477781 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.477846 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.477908 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.477974 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.478036 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.478102 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.478164 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.478230 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.478293 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.478359 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.478422 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.478487 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.478550 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.478614 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.478677 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.478739 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.478801 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.478864 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.478929 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.478996 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.479062 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.479124 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.479185 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.479246 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.479312 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.479374 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.479439 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.479501 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.479566 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.479628 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.479694 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.479755 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.479815 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.479876 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:00:12.479943 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:00:12.480010 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.480072 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:00:12.480137 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.480199 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.480262 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:00:12.480323 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.480388 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.480450 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.480515 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.480576 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.480640 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.480702 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:00:12.480767 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.480833 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.480896 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:00:12.480958 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481021 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481083 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481145 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481207 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481268 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481330 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481391 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481452 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481512 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481593 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481657 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481718 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481780 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481841 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481902 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:00:12.481968 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482035 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482102 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482167 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482233 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482298 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482363 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482429 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482493 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482559 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482625 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482690 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482755 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482821 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482885 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:12.482951 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:00:22.937022 140278725625664 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f94ac239978>>
I0930 04:00:28.503315 140278725625664 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I0930 04:00:28.506516 140278725625664 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I0930 04:00:28.512001 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I0930 04:00:28.517360 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I0930 04:00:28.522710 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I0930 04:00:28.528024 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I0930 04:00:28.533261 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I0930 04:00:28.538597 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I0930 04:00:28.543839 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I0930 04:00:28.549156 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I0930 04:00:28.554433 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I0930 04:00:28.559748 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I0930 04:00:28.564960 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I0930 04:00:28.570293 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I0930 04:00:28.575513 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I0930 04:00:28.580831 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I0930 04:00:28.584811 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I0930 04:00:28.588784 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I0930 04:00:28.593954 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I0930 04:00:28.599284 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I0930 04:00:28.604541 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I0930 04:00:28.609894 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I0930 04:00:28.615125 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I0930 04:00:28.620989 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I0930 04:00:28.626338 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I0930 04:00:28.631565 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I0930 04:00:28.636879 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I0930 04:00:28.642161 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I0930 04:00:28.647527 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I0930 04:00:28.652753 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I0930 04:00:28.658129 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I0930 04:00:28.663343 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I0930 04:00:28.668658 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I0930 04:00:28.673951 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I0930 04:00:28.679258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I0930 04:00:28.684587 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I0930 04:00:28.689828 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I0930 04:00:28.695149 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I0930 04:00:28.700399 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I0930 04:00:28.705732 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I0930 04:00:28.710997 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I0930 04:00:28.716317 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I0930 04:00:28.721581 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I0930 04:00:28.726898 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I0930 04:00:28.732124 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I0930 04:00:28.737924 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I0930 04:00:28.743221 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I0930 04:00:28.748443 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I0930 04:00:28.753896 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I0930 04:00:28.759118 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I0930 04:00:28.763108 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I0930 04:00:28.767122 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I0930 04:00:28.772263 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I0930 04:00:28.777623 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I0930 04:00:28.782981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I0930 04:00:28.788200 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I0930 04:00:28.793544 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I0930 04:00:28.798803 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I0930 04:00:28.804131 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I0930 04:00:28.809427 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I0930 04:00:28.814814 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I0930 04:00:28.820074 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I0930 04:00:28.825419 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I0930 04:00:28.830694 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I0930 04:00:28.836030 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I0930 04:00:28.841372 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I0930 04:00:28.846623 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I0930 04:00:28.852437 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I0930 04:00:28.857695 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I0930 04:00:28.863055 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I0930 04:00:28.868293 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I0930 04:00:28.873644 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I0930 04:00:28.878875 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I0930 04:00:28.884315 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I0930 04:00:28.889704 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I0930 04:00:28.895049 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I0930 04:00:28.900383 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I0930 04:00:28.905657 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I0930 04:00:28.911068 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I0930 04:00:28.916334 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I0930 04:00:28.921711 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I0930 04:00:28.926930 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I0930 04:00:28.932289 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I0930 04:00:28.937518 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I0930 04:00:28.941671 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I0930 04:00:28.945664 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I0930 04:00:28.950849 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I0930 04:00:28.956203 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I0930 04:00:28.961591 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I0930 04:00:28.966826 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I0930 04:00:28.972694 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I0930 04:00:28.977973 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I0930 04:00:28.983306 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I0930 04:00:28.988553 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I0930 04:00:28.993933 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I0930 04:00:28.999196 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I0930 04:00:29.004560 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I0930 04:00:29.009907 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I0930 04:00:29.015171 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I0930 04:00:29.020519 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I0930 04:00:29.025796 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I0930 04:00:29.031156 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I0930 04:00:29.036398 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I0930 04:00:29.041737 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I0930 04:00:29.047010 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I0930 04:00:29.052362 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I0930 04:00:29.057676 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I0930 04:00:29.062994 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I0930 04:00:29.068222 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I0930 04:00:29.073609 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I0930 04:00:29.078964 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I0930 04:00:29.084236 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I0930 04:00:29.090078 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I0930 04:00:29.095331 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I0930 04:00:29.100675 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I0930 04:00:29.105967 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I0930 04:00:29.111325 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I0930 04:00:29.116564 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I0930 04:00:29.120690 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I0930 04:00:29.124632 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I0930 04:00:29.129811 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I0930 04:00:29.135157 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I0930 04:00:29.140604 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I0930 04:00:29.145897 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I0930 04:00:29.151254 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I0930 04:00:29.156488 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I0930 04:00:29.161876 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I0930 04:00:29.167129 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I0930 04:00:29.172460 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I0930 04:00:29.177757 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I0930 04:00:29.183107 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I0930 04:00:29.188458 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I0930 04:00:29.193761 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I0930 04:00:29.199107 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I0930 04:00:29.204400 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I0930 04:00:29.210223 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I0930 04:00:29.215488 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I0930 04:00:29.220843 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I0930 04:00:29.226169 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I0930 04:00:29.231526 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I0930 04:00:29.236774 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I0930 04:00:29.242205 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I0930 04:00:29.247427 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I0930 04:00:29.252795 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I0930 04:00:29.258149 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I0930 04:00:29.263438 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I0930 04:00:29.268821 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I0930 04:00:29.274126 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I0930 04:00:29.279517 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I0930 04:00:29.284788 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I0930 04:00:29.290169 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I0930 04:00:29.295445 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I0930 04:00:29.299625 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I0930 04:00:29.303553 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I0930 04:00:29.308695 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I0930 04:00:29.314086 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I0930 04:00:29.319447 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I0930 04:00:29.324731 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I0930 04:00:29.330563 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I0930 04:00:29.335839 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I0930 04:00:29.341205 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I0930 04:00:29.346486 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I0930 04:00:29.351848 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I0930 04:00:29.357098 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I0930 04:00:29.362535 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I0930 04:00:29.367870 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I0930 04:00:29.373153 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I0930 04:00:29.378545 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I0930 04:00:29.383809 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I0930 04:00:29.389162 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I0930 04:00:29.394445 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I0930 04:00:29.399809 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I0930 04:00:29.405092 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I0930 04:00:29.410478 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I0930 04:00:29.415735 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I0930 04:00:29.421071 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I0930 04:00:29.426370 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I0930 04:00:29.431718 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I0930 04:00:29.437120 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I0930 04:00:29.442420 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I0930 04:00:29.448228 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I0930 04:00:29.453486 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I0930 04:00:29.458862 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I0930 04:00:29.464122 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I0930 04:00:29.469495 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I0930 04:00:29.474832 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I0930 04:00:29.478943 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I0930 04:00:29.482868 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I0930 04:00:29.488037 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I0930 04:00:29.493434 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I0930 04:00:29.498856 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I0930 04:00:29.504153 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I0930 04:00:29.509512 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I0930 04:00:29.514891 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I0930 04:00:29.520246 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I0930 04:00:29.525555 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I0930 04:00:29.530940 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I0930 04:00:29.536220 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I0930 04:00:29.541630 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I0930 04:00:29.547001 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I0930 04:00:29.552298 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I0930 04:00:29.557712 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I0930 04:00:29.562976 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I0930 04:00:29.568899 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I0930 04:00:29.574202 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I0930 04:00:29.579585 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I0930 04:00:29.584858 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I0930 04:00:29.590264 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I0930 04:00:29.595531 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I0930 04:00:29.600907 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I0930 04:00:29.606213 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I0930 04:00:29.611585 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I0930 04:00:29.616965 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I0930 04:00:29.622291 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I0930 04:00:29.627658 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I0930 04:00:29.632921 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I0930 04:00:29.638330 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I0930 04:00:29.643585 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I0930 04:00:29.648959 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I0930 04:00:29.654270 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I0930 04:00:29.658400 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I0930 04:00:29.662352 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I0930 04:00:29.667525 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I0930 04:00:29.672878 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I0930 04:00:29.678303 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I0930 04:00:29.683590 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I0930 04:00:29.689424 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I0930 04:00:29.694728 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I0930 04:00:29.700106 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I0930 04:00:29.705469 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I0930 04:00:29.710868 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I0930 04:00:29.716143 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I0930 04:00:29.721515 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I0930 04:00:29.726958 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I0930 04:00:29.732247 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I0930 04:00:29.737642 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I0930 04:00:29.742897 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I0930 04:00:29.748258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I0930 04:00:29.753570 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I0930 04:00:29.758923 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I0930 04:00:29.764394 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I0930 04:00:29.769773 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I0930 04:00:29.775028 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I0930 04:00:29.780392 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I0930 04:00:29.785694 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I0930 04:00:29.791055 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I0930 04:00:29.796434 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I0930 04:00:29.801749 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I0930 04:00:29.807604 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I0930 04:00:29.812875 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I0930 04:00:29.818295 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I0930 04:00:29.823542 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I0930 04:00:29.828935 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I0930 04:00:29.834240 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I0930 04:00:29.838359 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I0930 04:00:29.842311 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I0930 04:00:29.847538 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I0930 04:00:29.852926 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I0930 04:00:29.858355 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I0930 04:00:29.863610 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I0930 04:00:29.868989 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I0930 04:00:29.874310 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I0930 04:00:29.879690 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I0930 04:00:29.884990 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I0930 04:00:29.890398 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I0930 04:00:29.895695 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I0930 04:00:29.901098 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I0930 04:00:29.906510 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I0930 04:00:29.911776 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I0930 04:00:29.917178 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I0930 04:00:29.922481 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I0930 04:00:29.928327 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I0930 04:00:29.933626 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I0930 04:00:29.939009 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I0930 04:00:29.944295 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I0930 04:00:29.949672 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I0930 04:00:29.954965 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I0930 04:00:29.960333 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I0930 04:00:29.965615 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I0930 04:00:29.971034 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I0930 04:00:29.976410 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I0930 04:00:29.981738 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I0930 04:00:29.987114 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I0930 04:00:29.992372 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I0930 04:00:29.997796 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I0930 04:00:30.003038 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I0930 04:00:30.008427 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I0930 04:00:30.013741 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I0930 04:00:30.017880 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I0930 04:00:30.021825 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I0930 04:00:30.027026 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I0930 04:00:30.032485 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I0930 04:00:30.037904 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I0930 04:00:30.043206 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I0930 04:00:30.049032 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I0930 04:00:30.054370 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I0930 04:00:30.059781 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I0930 04:00:30.065065 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I0930 04:00:30.070494 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I0930 04:00:30.075781 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I0930 04:00:30.081236 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I0930 04:00:30.086684 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I0930 04:00:30.092014 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I0930 04:00:30.097422 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I0930 04:00:30.102719 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I0930 04:00:30.108105 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I0930 04:00:30.113374 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I0930 04:00:30.118785 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I0930 04:00:30.124118 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I0930 04:00:30.129486 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I0930 04:00:30.134799 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I0930 04:00:30.140230 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I0930 04:00:30.145552 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I0930 04:00:30.150938 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I0930 04:00:30.156349 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I0930 04:00:30.161678 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I0930 04:00:30.167506 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I0930 04:00:30.172786 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I0930 04:00:30.178215 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I0930 04:00:30.183497 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I0930 04:00:30.188897 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I0930 04:00:30.194233 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I0930 04:00:30.198397 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I0930 04:00:30.202351 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I0930 04:00:30.207557 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I0930 04:00:30.212953 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I0930 04:00:30.218379 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I0930 04:00:30.223736 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I0930 04:00:30.229122 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I0930 04:00:30.234452 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I0930 04:00:30.239843 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I0930 04:00:30.245100 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I0930 04:00:30.250553 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I0930 04:00:30.255836 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I0930 04:00:30.261233 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I0930 04:00:30.266649 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I0930 04:00:30.271944 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I0930 04:00:30.277351 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I0930 04:00:30.282652 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I0930 04:00:30.288513 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I0930 04:00:30.293814 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I0930 04:00:30.299209 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I0930 04:00:30.304516 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I0930 04:00:30.309959 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I0930 04:00:30.315269 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I0930 04:00:30.320673 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I0930 04:00:30.325983 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I0930 04:00:30.331459 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I0930 04:00:30.336827 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I0930 04:00:30.342144 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I0930 04:00:30.347544 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I0930 04:00:30.352838 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I0930 04:00:30.358270 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I0930 04:00:30.363547 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I0930 04:00:30.368942 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I0930 04:00:30.374252 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I0930 04:00:30.378413 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I0930 04:00:30.382359 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I0930 04:00:30.387590 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I0930 04:00:30.393028 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I0930 04:00:30.398439 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I0930 04:00:30.403734 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I0930 04:00:30.409591 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I0930 04:00:30.414947 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I0930 04:00:30.420342 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I0930 04:00:30.425663 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I0930 04:00:30.431057 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I0930 04:00:30.436378 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I0930 04:00:30.441944 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I0930 04:00:30.447356 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I0930 04:00:30.452644 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I0930 04:00:30.458091 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I0930 04:00:30.463364 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I0930 04:00:30.468764 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I0930 04:00:30.474116 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I0930 04:00:30.479527 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I0930 04:00:30.484877 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I0930 04:00:30.490310 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I0930 04:00:30.495603 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I0930 04:00:30.501015 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I0930 04:00:30.506314 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I0930 04:00:30.511741 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I0930 04:00:30.517140 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I0930 04:00:30.522547 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I0930 04:00:30.528396 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I0930 04:00:30.533713 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I0930 04:00:30.539134 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I0930 04:00:30.544414 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I0930 04:00:30.549854 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I0930 04:00:30.555159 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I0930 04:00:30.559334 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I0930 04:00:30.563288 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I0930 04:00:30.568506 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I0930 04:00:30.573965 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I0930 04:00:30.579403 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I0930 04:00:30.584743 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I0930 04:00:30.590170 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I0930 04:00:30.595465 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I0930 04:00:30.600870 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I0930 04:00:30.606209 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I0930 04:00:30.611602 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I0930 04:00:30.616912 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I0930 04:00:30.622375 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I0930 04:00:30.627785 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I0930 04:00:30.633088 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I0930 04:00:30.638520 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I0930 04:00:30.643828 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I0930 04:00:30.649741 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I0930 04:00:30.655027 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I0930 04:00:30.660444 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I0930 04:00:30.665787 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I0930 04:00:30.671206 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I0930 04:00:30.676505 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I0930 04:00:30.681956 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I0930 04:00:30.687299 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I0930 04:00:30.692745 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I0930 04:00:30.698163 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I0930 04:00:30.703473 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I0930 04:00:30.708891 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I0930 04:00:30.714246 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I0930 04:00:30.719665 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I0930 04:00:30.725007 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I0930 04:00:30.730431 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I0930 04:00:30.735711 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I0930 04:00:30.739886 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I0930 04:00:30.743867 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I0930 04:00:30.749095 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I0930 04:00:30.754557 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I0930 04:00:30.759978 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I0930 04:00:30.765281 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I0930 04:00:30.771177 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I0930 04:00:30.776483 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I0930 04:00:30.781951 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I0930 04:00:30.787259 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I0930 04:00:30.792724 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I0930 04:00:30.798091 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I0930 04:00:30.803518 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I0930 04:00:30.808920 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I0930 04:00:30.814250 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I0930 04:00:30.819745 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I0930 04:00:30.825095 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I0930 04:00:30.830513 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I0930 04:00:30.835819 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I0930 04:00:30.841252 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I0930 04:00:30.846702 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I0930 04:00:30.852120 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I0930 04:00:30.857443 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I0930 04:00:30.862887 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I0930 04:00:30.868204 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I0930 04:00:30.873668 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I0930 04:00:30.879074 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I0930 04:00:30.884459 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I0930 04:00:30.890418 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I0930 04:00:30.895739 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I0930 04:00:30.901215 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I0930 04:00:30.906565 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I0930 04:00:30.911966 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I0930 04:00:30.917268 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I0930 04:00:30.921451 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I0930 04:00:30.925456 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I0930 04:00:30.930692 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I0930 04:00:30.936100 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I0930 04:00:30.941551 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I0930 04:00:30.946894 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I0930 04:00:30.952313 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I0930 04:00:30.957708 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I0930 04:00:30.963132 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I0930 04:00:30.968438 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I0930 04:00:30.973907 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I0930 04:00:30.979257 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I0930 04:00:30.984706 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I0930 04:00:30.990168 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I0930 04:00:30.995479 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I0930 04:00:31.000932 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I0930 04:00:31.006294 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I0930 04:00:31.012187 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I0930 04:00:31.017498 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I0930 04:00:31.022961 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I0930 04:00:31.028457 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I0930 04:00:31.033906 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I0930 04:00:31.039230 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I0930 04:00:31.044650 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I0930 04:00:31.049990 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I0930 04:00:31.055440 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I0930 04:00:31.060919 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I0930 04:00:31.066423 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I0930 04:00:31.071846 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I0930 04:00:31.077205 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I0930 04:00:31.082651 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I0930 04:00:31.087992 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I0930 04:00:31.093408 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I0930 04:00:31.098791 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I0930 04:00:31.102973 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I0930 04:00:31.106988 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I0930 04:00:31.112236 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I0930 04:00:31.117729 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I0930 04:00:31.123220 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I0930 04:00:31.128554 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I0930 04:00:31.134509 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I0930 04:00:31.139832 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I0930 04:00:31.145262 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I0930 04:00:31.150663 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I0930 04:00:31.156071 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I0930 04:00:31.161400 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I0930 04:00:31.166872 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I0930 04:00:31.172293 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I0930 04:00:31.177656 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I0930 04:00:31.183111 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I0930 04:00:31.188446 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I0930 04:00:31.193916 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I0930 04:00:31.199223 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I0930 04:00:31.204655 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I0930 04:00:31.209993 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I0930 04:00:31.215431 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I0930 04:00:31.220737 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I0930 04:00:31.226227 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I0930 04:00:31.231580 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I0930 04:00:31.237001 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I0930 04:00:31.242469 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I0930 04:00:31.247802 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I0930 04:00:31.253743 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I0930 04:00:31.259076 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I0930 04:00:31.264540 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I0930 04:00:31.269904 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I0930 04:00:31.275335 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I0930 04:00:31.280658 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I0930 04:00:31.284894 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I0930 04:00:31.288888 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I0930 04:00:31.294169 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I0930 04:00:31.299646 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I0930 04:00:31.305114 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I0930 04:00:31.310497 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I0930 04:00:31.315993 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I0930 04:00:31.321315 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I0930 04:00:31.326807 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I0930 04:00:31.332116 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I0930 04:00:31.337590 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I0930 04:00:31.342981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I0930 04:00:31.348444 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I0930 04:00:31.353919 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I0930 04:00:31.359249 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I0930 04:00:31.364719 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I0930 04:00:31.370093 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I0930 04:00:31.376005 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I0930 04:00:31.381345 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I0930 04:00:31.386801 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I0930 04:00:31.392158 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I0930 04:00:31.397623 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I0930 04:00:31.402964 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I0930 04:00:31.408394 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I0930 04:00:31.413762 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I0930 04:00:31.419203 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I0930 04:00:31.424630 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I0930 04:00:31.430010 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I0930 04:00:31.435488 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I0930 04:00:31.440818 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I0930 04:00:31.446316 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I0930 04:00:31.451651 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I0930 04:00:31.457100 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I0930 04:00:31.462515 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I0930 04:00:31.466712 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I0930 04:00:31.470741 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I0930 04:00:31.476257 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I0930 04:00:31.481874 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I0930 04:00:31.487371 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I0930 04:00:31.492738 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I0930 04:00:31.498687 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I0930 04:00:31.504060 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I0930 04:00:31.509566 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I0930 04:00:31.514970 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I0930 04:00:31.520419 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I0930 04:00:31.525820 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I0930 04:00:31.531260 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I0930 04:00:31.536757 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I0930 04:00:31.542127 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I0930 04:00:31.547599 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I0930 04:00:31.552949 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I0930 04:00:31.558433 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I0930 04:00:31.563777 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I0930 04:00:31.569260 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I0930 04:00:31.574642 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I0930 04:00:31.580099 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I0930 04:00:31.585463 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I0930 04:00:31.590958 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I0930 04:00:31.596299 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I0930 04:00:31.601795 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I0930 04:00:31.607241 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I0930 04:00:31.612563 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I0930 04:00:31.618597 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I0930 04:00:31.623977 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I0930 04:00:31.629444 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I0930 04:00:31.634824 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I0930 04:00:31.640241 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I0930 04:00:31.645629 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I0930 04:00:31.649874 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I0930 04:00:31.653905 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I0930 04:00:31.659158 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I0930 04:00:31.664654 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I0930 04:00:31.670139 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I0930 04:00:31.675525 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I0930 04:00:31.680983 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I0930 04:00:31.686517 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I0930 04:00:31.692085 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I0930 04:00:31.697464 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I0930 04:00:31.702938 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I0930 04:00:31.708335 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I0930 04:00:31.713816 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I0930 04:00:31.719261 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I0930 04:00:31.724659 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I0930 04:00:31.730150 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I0930 04:00:31.735526 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I0930 04:00:31.741401 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I0930 04:00:31.746785 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I0930 04:00:31.752258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I0930 04:00:31.757658 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I0930 04:00:31.763096 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I0930 04:00:31.768495 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I0930 04:00:31.773989 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I0930 04:00:31.779368 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I0930 04:00:31.784830 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I0930 04:00:31.790308 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I0930 04:00:31.795692 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I0930 04:00:31.801143 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I0930 04:00:31.806557 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I0930 04:00:31.811992 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I0930 04:00:31.817363 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I0930 04:00:31.822833 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I0930 04:00:31.828213 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I0930 04:00:31.832433 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I0930 04:00:31.836470 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I0930 04:00:31.841753 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I0930 04:00:31.847229 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I0930 04:00:31.852674 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I0930 04:00:31.858054 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I0930 04:00:31.863993 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I0930 04:00:31.869347 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I0930 04:00:31.875114 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I0930 04:00:31.880483 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I0930 04:00:31.886015 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I0930 04:00:31.891418 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I0930 04:00:31.896890 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I0930 04:00:31.902399 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I0930 04:00:31.907749 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I0930 04:00:31.913203 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I0930 04:00:31.918597 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I0930 04:00:31.924057 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I0930 04:00:31.929392 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I0930 04:00:31.934878 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I0930 04:00:31.940271 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I0930 04:00:31.945748 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I0930 04:00:31.951114 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I0930 04:00:31.956576 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I0930 04:00:31.962011 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I0930 04:00:31.967478 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I0930 04:00:31.972959 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I0930 04:00:31.978424 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I0930 04:00:31.984371 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I0930 04:00:31.989812 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I0930 04:00:31.995285 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I0930 04:00:32.000692 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I0930 04:00:32.006196 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I0930 04:00:32.011590 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I0930 04:00:32.015861 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I0930 04:00:32.019886 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I0930 04:00:32.025142 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I0930 04:00:32.030670 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I0930 04:00:32.036144 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I0930 04:00:32.041484 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I0930 04:00:32.046973 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I0930 04:00:32.052601 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I0930 04:00:32.058197 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I0930 04:00:32.063542 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I0930 04:00:32.069003 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I0930 04:00:32.074404 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I0930 04:00:32.079907 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I0930 04:00:32.085373 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I0930 04:00:32.090780 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I0930 04:00:32.096266 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I0930 04:00:32.101655 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I0930 04:00:32.107583 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I0930 04:00:32.113017 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I0930 04:00:32.118515 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I0930 04:00:32.123897 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I0930 04:00:32.129379 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I0930 04:00:32.134758 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I0930 04:00:32.140236 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I0930 04:00:32.145721 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I0930 04:00:32.151221 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I0930 04:00:32.156724 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I0930 04:00:32.162108 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I0930 04:00:32.167597 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I0930 04:00:32.172981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I0930 04:00:32.178509 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I0930 04:00:32.183883 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I0930 04:00:32.189349 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I0930 04:00:32.194779 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I0930 04:00:32.199024 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I0930 04:00:32.203076 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I0930 04:00:32.208338 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I0930 04:00:32.213836 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I0930 04:00:32.219343 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I0930 04:00:32.224696 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I0930 04:00:32.230685 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I0930 04:00:32.236042 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I0930 04:00:32.241554 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I0930 04:00:32.246933 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I0930 04:00:32.252451 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I0930 04:00:32.257872 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I0930 04:00:32.263327 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I0930 04:00:32.268795 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I0930 04:00:32.274214 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I0930 04:00:32.279751 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I0930 04:00:32.285109 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I0930 04:00:32.290623 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I0930 04:00:32.295979 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I0930 04:00:32.301492 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I0930 04:00:32.306885 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I0930 04:00:32.312369 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I0930 04:00:32.317757 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I0930 04:00:32.323245 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I0930 04:00:32.328616 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I0930 04:00:32.334204 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I0930 04:00:32.339679 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I0930 04:00:32.345031 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I0930 04:00:32.350988 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I0930 04:00:32.356358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I0930 04:00:32.361898 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I0930 04:00:32.367276 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I0930 04:00:32.372758 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I0930 04:00:32.378337 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I0930 04:00:32.382601 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I0930 04:00:32.386637 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I0930 04:00:32.391940 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I0930 04:00:32.397475 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I0930 04:00:32.402984 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I0930 04:00:32.408350 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I0930 04:00:32.413862 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I0930 04:00:32.419268 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I0930 04:00:32.424769 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I0930 04:00:32.430228 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I0930 04:00:32.435686 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I0930 04:00:32.441103 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I0930 04:00:32.446616 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I0930 04:00:32.452101 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I0930 04:00:32.457482 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I0930 04:00:32.463006 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I0930 04:00:32.468407 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I0930 04:00:32.474418 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I0930 04:00:32.479818 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I0930 04:00:32.485331 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I0930 04:00:32.490781 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I0930 04:00:32.496286 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I0930 04:00:32.501720 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I0930 04:00:32.507190 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I0930 04:00:32.512659 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I0930 04:00:32.518178 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I0930 04:00:32.523669 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I0930 04:00:32.529064 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I0930 04:00:32.534611 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I0930 04:00:32.540012 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I0930 04:00:32.545504 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I0930 04:00:32.550985 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I0930 04:00:32.556524 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I0930 04:00:32.561960 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I0930 04:00:32.566227 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I0930 04:00:32.570300 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I0930 04:00:32.575608 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I0930 04:00:32.581173 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I0930 04:00:32.586703 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I0930 04:00:32.592082 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I0930 04:00:32.598137 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I0930 04:00:32.603536 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I0930 04:00:32.609048 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I0930 04:00:32.614486 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I0930 04:00:32.619953 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I0930 04:00:32.625346 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I0930 04:00:32.630843 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I0930 04:00:32.636356 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I0930 04:00:32.641793 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I0930 04:00:32.647277 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I0930 04:00:32.652665 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I0930 04:00:32.658184 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I0930 04:00:32.663568 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I0930 04:00:32.669080 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I0930 04:00:32.674505 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I0930 04:00:32.680014 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I0930 04:00:32.685405 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I0930 04:00:32.690982 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I0930 04:00:32.696391 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I0930 04:00:32.701976 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I0930 04:00:32.707651 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I0930 04:00:32.713045 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I0930 04:00:32.719134 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I0930 04:00:32.724535 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I0930 04:00:32.730082 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I0930 04:00:32.735548 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I0930 04:00:32.741020 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I0930 04:00:32.746471 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I0930 04:00:32.750765 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I0930 04:00:32.754868 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I0930 04:00:32.760164 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I0930 04:00:32.765693 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I0930 04:00:32.771190 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I0930 04:00:32.776570 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I0930 04:00:32.782120 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I0930 04:00:32.787518 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I0930 04:00:32.793028 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I0930 04:00:32.798446 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I0930 04:00:32.803922 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I0930 04:00:32.809351 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I0930 04:00:32.814849 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I0930 04:00:32.820347 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I0930 04:00:32.825783 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I0930 04:00:32.831315 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I0930 04:00:32.836740 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I0930 04:00:32.842809 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I0930 04:00:32.848219 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I0930 04:00:32.853788 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I0930 04:00:32.859262 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I0930 04:00:32.864763 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I0930 04:00:32.870184 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I0930 04:00:32.875664 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I0930 04:00:32.881130 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I0930 04:00:32.886723 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I0930 04:00:32.892249 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I0930 04:00:32.897658 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I0930 04:00:32.903178 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I0930 04:00:32.908593 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I0930 04:00:32.914113 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I0930 04:00:32.919492 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I0930 04:00:32.925010 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I0930 04:00:32.930450 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I0930 04:00:32.934694 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I0930 04:00:32.938761 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I0930 04:00:32.944074 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I0930 04:00:32.949630 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I0930 04:00:32.955141 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I0930 04:00:32.960545 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I0930 04:00:32.966513 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I0930 04:00:32.971991 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I0930 04:00:32.977551 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I0930 04:00:32.982980 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I0930 04:00:32.988460 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I0930 04:00:32.993914 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I0930 04:00:32.999503 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I0930 04:00:33.005018 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I0930 04:00:33.010475 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I0930 04:00:33.015981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I0930 04:00:33.021391 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I0930 04:00:33.026978 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I0930 04:00:33.032375 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I0930 04:00:33.037897 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I0930 04:00:33.043324 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I0930 04:00:33.048810 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I0930 04:00:33.054211 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I0930 04:00:33.059725 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I0930 04:00:33.065125 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I0930 04:00:33.070691 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I0930 04:00:33.076189 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I0930 04:00:33.081644 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I0930 04:00:33.087622 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I0930 04:00:33.093054 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I0930 04:00:33.098592 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I0930 04:00:33.104035 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I0930 04:00:33.109502 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I0930 04:00:33.114970 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I0930 04:00:33.119253 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I0930 04:00:33.123351 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I0930 04:00:33.128662 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I0930 04:00:33.134217 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I0930 04:00:33.139727 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I0930 04:00:33.145150 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I0930 04:00:33.150703 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I0930 04:00:33.156117 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I0930 04:00:33.161638 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I0930 04:00:33.167064 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I0930 04:00:33.172570 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I0930 04:00:33.178082 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I0930 04:00:33.183608 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I0930 04:00:33.189105 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I0930 04:00:33.194588 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I0930 04:00:33.200093 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I0930 04:00:33.205502 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I0930 04:00:33.211604 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I0930 04:00:33.216999 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I0930 04:00:33.222569 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I0930 04:00:33.227989 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I0930 04:00:33.233563 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I0930 04:00:33.238972 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I0930 04:00:33.244480 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I0930 04:00:33.249946 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I0930 04:00:33.255490 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I0930 04:00:33.260995 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I0930 04:00:33.266446 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I0930 04:00:33.271987 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I0930 04:00:33.277406 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I0930 04:00:33.282951 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I0930 04:00:33.288398 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I0930 04:00:33.293926 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I0930 04:00:33.299343 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I0930 04:00:33.303627 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I0930 04:00:33.307730 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I0930 04:00:33.313042 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I0930 04:00:33.318590 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I0930 04:00:33.324115 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I0930 04:00:33.329504 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I0930 04:00:33.335624 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I0930 04:00:33.341032 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I0930 04:00:34.091217 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I0930 04:00:34.097276 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I0930 04:00:34.102843 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I0930 04:00:34.108349 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I0930 04:00:34.113812 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I0930 04:00:34.119358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I0930 04:00:34.124816 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I0930 04:00:34.130386 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I0930 04:00:34.135831 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I0930 04:00:34.141370 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I0930 04:00:34.146826 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I0930 04:00:34.152403 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I0930 04:00:34.157844 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I0930 04:00:34.163370 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I0930 04:00:34.168942 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I0930 04:00:34.174409 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I0930 04:00:34.179945 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I0930 04:00:34.185374 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I0930 04:00:34.190936 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I0930 04:00:34.196383 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I0930 04:00:34.201979 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I0930 04:00:34.207418 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I0930 04:00:34.213499 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I0930 04:00:34.218980 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I0930 04:00:34.224496 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I0930 04:00:34.230073 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I0930 04:00:34.234297 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I0930 04:00:34.238416 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I0930 04:00:34.243866 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I0930 04:00:34.249324 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I0930 04:00:34.254872 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I0930 04:00:34.260298 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I0930 04:00:34.265841 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I0930 04:00:34.271379 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I0930 04:00:34.276822 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I0930 04:00:34.282377 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I0930 04:00:34.287808 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I0930 04:00:34.293389 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I0930 04:00:34.298854 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I0930 04:00:34.304404 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I0930 04:00:34.309854 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I0930 04:00:34.315407 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I0930 04:00:34.320846 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I0930 04:00:34.326422 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I0930 04:00:34.332410 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I0930 04:00:34.337893 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I0930 04:00:34.343451 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I0930 04:00:34.348891 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I0930 04:00:34.354470 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I0930 04:00:34.359901 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I0930 04:00:34.365411 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I0930 04:00:34.370860 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I0930 04:00:34.376417 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I0930 04:00:34.381880 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I0930 04:00:34.387403 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I0930 04:00:34.392845 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I0930 04:00:34.398394 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I0930 04:00:34.403961 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I0930 04:00:34.409466 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I0930 04:00:34.415061 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I0930 04:00:34.419265 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I0930 04:00:34.423386 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I0930 04:00:34.428817 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I0930 04:00:34.434497 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I0930 04:00:34.440024 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I0930 04:00:34.445565 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I0930 04:00:34.451010 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I0930 04:00:34.457033 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I0930 04:00:34.462484 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I0930 04:00:34.468041 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I0930 04:00:34.473494 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I0930 04:00:34.479120 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I0930 04:00:34.484545 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I0930 04:00:34.490122 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I0930 04:00:34.495563 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I0930 04:00:34.501121 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I0930 04:00:34.506699 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I0930 04:00:34.512149 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I0930 04:00:34.517737 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I0930 04:00:34.523185 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I0930 04:00:34.528740 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I0930 04:00:34.534189 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I0930 04:00:34.539730 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I0930 04:00:34.545164 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I0930 04:00:34.550742 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I0930 04:00:34.556200 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I0930 04:00:34.561778 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I0930 04:00:34.567316 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I0930 04:00:34.572736 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I0930 04:00:34.578816 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I0930 04:00:34.584249 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I0930 04:00:34.589822 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I0930 04:00:34.595276 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I0930 04:00:34.600818 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I0930 04:00:34.605045 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I0930 04:00:34.609151 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I0930 04:00:34.614645 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I0930 04:00:34.620196 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I0930 04:00:34.625848 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I0930 04:00:34.631384 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I0930 04:00:34.636886 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I0930 04:00:34.642459 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I0930 04:00:34.647912 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I0930 04:00:34.653456 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I0930 04:00:34.658903 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I0930 04:00:34.664485 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I0930 04:00:34.669939 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I0930 04:00:34.675480 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I0930 04:00:34.681022 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I0930 04:00:34.686489 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I0930 04:00:34.692081 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I0930 04:00:34.697550 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I0930 04:00:34.703600 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I0930 04:00:34.709091 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I0930 04:00:34.714685 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I0930 04:00:34.720201 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I0930 04:00:34.725782 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I0930 04:00:34.731254 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I0930 04:00:34.736799 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I0930 04:00:34.742287 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I0930 04:00:34.747858 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I0930 04:00:34.753424 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I0930 04:00:34.758894 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I0930 04:00:34.764465 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I0930 04:00:34.769929 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I0930 04:00:34.775508 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I0930 04:00:34.780978 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I0930 04:00:34.786546 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I0930 04:00:34.790784 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I0930 04:00:34.794923 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I0930 04:00:34.800431 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I0930 04:00:34.806046 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I0930 04:00:34.811484 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I0930 04:00:34.817022 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I0930 04:00:34.822492 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I0930 04:00:34.828520 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I0930 04:00:34.834018 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I0930 04:00:34.839600 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I0930 04:00:34.845054 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I0930 04:00:34.850642 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I0930 04:00:34.856137 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I0930 04:00:34.861725 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I0930 04:00:34.867266 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I0930 04:00:34.872718 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I0930 04:00:34.878306 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I0930 04:00:34.883796 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I0930 04:00:34.889356 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I0930 04:00:34.894867 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I0930 04:00:34.900411 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I0930 04:00:34.905893 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I0930 04:00:34.911445 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I0930 04:00:34.916908 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I0930 04:00:34.922478 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I0930 04:00:34.927971 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I0930 04:00:34.933545 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I0930 04:00:34.939120 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I0930 04:00:34.944621 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I0930 04:00:34.950680 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I0930 04:00:34.956142 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I0930 04:00:34.961724 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I0930 04:00:34.967291 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I0930 04:00:34.972837 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I0930 04:00:34.977069 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I0930 04:00:34.981228 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I0930 04:00:34.986725 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I0930 04:00:34.992292 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I0930 04:00:34.997766 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I0930 04:00:35.003625 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I0930 04:00:35.009092 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I0930 04:00:35.014729 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I0930 04:00:35.020202 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I0930 04:00:35.025814 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I0930 04:00:35.031276 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I0930 04:00:35.036849 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I0930 04:00:35.042341 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I0930 04:00:35.047894 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I0930 04:00:35.053452 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I0930 04:00:35.058924 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I0930 04:00:35.064506 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I0930 04:00:35.069999 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I0930 04:00:35.076046 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I0930 04:00:35.081558 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I0930 04:00:35.087148 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I0930 04:00:35.092619 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I0930 04:00:35.098223 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I0930 04:00:35.103709 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I0930 04:00:35.109261 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I0930 04:00:35.114748 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I0930 04:00:35.120293 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I0930 04:00:35.125884 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I0930 04:00:35.131358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I0930 04:00:35.136914 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I0930 04:00:35.142437 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I0930 04:00:35.148002 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I0930 04:00:35.153443 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I0930 04:00:35.159007 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I0930 04:00:35.164486 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I0930 04:00:35.170101 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I0930 04:00:35.175545 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I0930 04:00:35.181129 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I0930 04:00:35.186616 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I0930 04:00:35.192167 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I0930 04:00:35.198180 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I0930 04:00:35.203661 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I0930 04:00:35.209214 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I0930 04:00:35.214679 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I0930 04:00:35.220242 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I0930 04:00:35.225820 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I0930 04:00:35.231407 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I0930 04:00:35.236872 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I0930 04:00:35.242485 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I0930 04:00:35.247943 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I0930 04:00:35.253469 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I0930 04:00:35.258990 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I0930 04:00:35.264560 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I0930 04:00:35.270137 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I0930 04:00:35.275626 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I0930 04:00:35.281179 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I0930 04:00:35.286749 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I0930 04:00:35.292308 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I0930 04:00:35.297786 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I0930 04:00:35.303344 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I0930 04:00:35.308810 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I0930 04:00:35.314415 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I0930 04:00:35.320193 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I0930 04:00:35.326265 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I0930 04:00:35.331824 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I0930 04:00:35.337312 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I0930 04:00:35.342952 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I0930 04:00:35.348449 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I0930 04:00:35.354060 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I0930 04:00:35.359552 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I0930 04:00:35.365131 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I0930 04:00:35.370665 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I0930 04:00:35.376242 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I0930 04:00:35.381733 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I0930 04:00:35.387305 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I0930 04:00:35.392899 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I0930 04:00:35.398417 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I0930 04:00:35.404006 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I0930 04:00:35.409488 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I0930 04:00:35.415095 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I0930 04:00:35.420565 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I0930 04:00:35.426162 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I0930 04:00:35.431641 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I0930 04:00:35.437208 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I0930 04:00:35.442780 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I0930 04:00:35.448926 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I0930 04:00:36.226452 140278725625664 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I0930 04:00:36.228733 140278725625664 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:39.003708 140278725625664 py_utils.py:1468] MODEL ANALYSIS: 
I0930 04:00:39.003909 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.emb.src_token_emb.wm                               (32000, 2048)          65536000 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var
I0930 04:00:39.003978 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.004036 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.004088 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.004138 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.004187 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.004235 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.004282 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.004343 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.004394 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.004442 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.004490 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.004537 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.004585 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.004632 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.004679 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.004725 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.004773 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.004820 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.004867 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.004914 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.004960 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.005007 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.005060 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.005107 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.005155 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.005202 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.005249 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.005297 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.005345 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.005393 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.005440 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.005488 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.005560 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.005614 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.005663 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.005711 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.005759 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.005805 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.005858 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.005906 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.005954 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.006001 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.006048 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.006096 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.006143 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.006191 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.006238 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.006285 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.006333 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.006380 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.006428 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.006475 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.006522 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.006573 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.006621 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.006668 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.006714 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.006761 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.006807 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.006855 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.006901 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.006948 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.006996 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.007043 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.007090 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.007136 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.007183 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.007229 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.007280 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.007328 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.007375 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.007422 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.007468 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.007515 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.007563 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.007609 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.007657 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.007704 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.007751 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.007799 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.007845 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.007896 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.007945 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.007992 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.008044 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.008092 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.008139 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.008186 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.008233 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.008280 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.008327 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.008374 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.008421 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.008468 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.008515 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.008563 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.008609 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.008655 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.008702 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.008753 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.008801 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.008849 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.008896 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.008943 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.008990 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.009037 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.009083 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.009129 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.009176 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.009223 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.009270 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.009317 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.009364 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.009411 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.009458 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.009509 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.009576 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.009625 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.009673 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.009720 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.009767 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.009814 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.009861 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.009907 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.009953 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.009999 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.010046 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.010093 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.010140 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.010188 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.010240 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.010288 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.010335 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.010382 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.010429 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.010476 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.010523 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.010570 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.010616 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.010663 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.010710 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.010757 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.010803 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.010850 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.010898 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.010944 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.010999 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.011049 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.011097 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.011144 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.011192 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.011239 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.011286 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.011333 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.011379 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.011426 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.011473 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.011521 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.011568 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.011615 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.011662 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.011714 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.011762 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.011809 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.011857 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.011904 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.011951 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.011998 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.012046 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.012093 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.012141 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.012187 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.012234 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.012281 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.012327 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.012374 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.012420 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.012472 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.012520 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.012567 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.012615 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.012662 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.012709 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.012757 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.012804 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.012851 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.012899 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.012946 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.012994 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.013041 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.013088 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.013135 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.013187 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.013235 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.013283 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.013331 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.013378 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.013425 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.013472 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.013519 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.013587 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.013636 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.013684 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.013732 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.013780 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.013827 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.013875 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.013926 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.013974 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.014022 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.014069 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.014117 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.014165 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.014213 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.014261 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.014309 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.014356 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.014403 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.014451 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.014498 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.014545 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.014593 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.014643 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.014691 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.014738 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.014785 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.014832 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.014878 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.014926 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.014973 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.015021 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.015069 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.015116 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.015163 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.015209 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.015256 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.015303 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.015350 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.015403 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.015451 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.015499 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.015546 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.015592 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.015639 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.015686 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.015733 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.015780 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.015827 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.015875 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.015923 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.015970 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.016017 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.016065 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.016116 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.016165 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.016212 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.016259 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.016306 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.016352 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.016399 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.016447 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.016493 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.016541 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.016588 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.016635 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.016682 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.016729 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.016777 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.016824 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.016875 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.016923 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.016971 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.017018 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.017066 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.017112 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.017159 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.017205 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.017252 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.017298 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.017345 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.017392 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.017439 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.017486 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.017550 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.017608 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.017657 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.017704 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.017752 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.017798 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.017845 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.017891 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.017938 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.017985 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.018031 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.018078 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.018124 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.018171 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.018218 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.018265 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.018317 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.018365 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.018413 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.018459 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.018506 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.018553 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.018600 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.018648 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.018696 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.018743 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.018790 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.018837 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.018884 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.018931 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.018978 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.019025 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.019084 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.019132 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.019180 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.019227 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.019275 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.019322 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.019369 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.019416 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.019464 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.019510 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.019557 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.019604 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.019650 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.019696 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.019742 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.019793 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.019841 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.019888 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.019935 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.019982 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.020029 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.020076 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.020123 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.020170 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.020217 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.020264 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.020311 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.020358 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.020405 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.020452 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.020498 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.020550 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.020598 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.020646 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.020693 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.020740 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.020787 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.020834 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.020881 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.020929 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.020976 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.021023 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.021075 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.021122 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.021169 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.021216 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.021267 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.021315 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.021362 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.021409 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.021457 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.021504 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.021568 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.021617 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.021665 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.021711 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.021759 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.021805 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.021853 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.021900 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.021947 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.021999 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.022046 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.022093 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.022139 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.022194 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.022241 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.022287 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.022334 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.022380 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.022427 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.022473 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.022528 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.022575 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.022621 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.022667 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.022714 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.022766 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.022814 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.022860 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.022907 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.022953 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.023000 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.023047 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.023094 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.023140 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.023187 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.023234 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.023281 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.023328 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.023375 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.023422 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.023474 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.023523 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.023570 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.023617 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.023684 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.023737 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.023784 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.023832 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.023879 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.023936 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.023983 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.024030 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.024076 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.024124 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.024171 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.024233 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.024281 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.024329 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.024377 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.024425 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.024471 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.024518 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.024565 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.024612 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.024659 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.024706 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.024754 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.024801 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.024849 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.024897 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.024944 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.024999 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.025048 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.025095 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.025142 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.025189 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.025236 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.025283 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.025330 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.025376 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.025423 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.025469 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.025516 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.025585 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.025634 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.025681 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.025735 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.025783 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.025831 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.025879 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.025926 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.025974 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.026020 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.026067 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.026113 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.026160 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.026206 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.026253 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.026299 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.026346 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.026393 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.026444 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.026491 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.026538 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.026594 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.026641 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.026687 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.026734 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.026780 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.026828 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.026874 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.026920 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.026967 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.027013 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.027060 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.027106 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.027153 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.027204 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.027252 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.027298 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.027346 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.027393 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.027440 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.027487 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.027534 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.027581 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.027627 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.027675 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.027722 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.027768 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.027814 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.027861 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.027912 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.027959 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.028006 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.028053 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.028100 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.028147 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.028193 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.028239 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.028286 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.028333 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.028380 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.028427 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.028473 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.028520 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.028566 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.028613 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.028664 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.028712 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.028759 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.028806 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.028853 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.028900 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.028946 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.028994 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.029041 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.029088 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.029135 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.029181 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.029229 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var
I0930 04:00:39.029275 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var
I0930 04:00:39.029322 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var
I0930 04:00:39.029373 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var
I0930 04:00:39.029421 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var
I0930 04:00:39.029467 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var
I0930 04:00:39.029513 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 04:00:39.029585 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 04:00:39.029634 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 04:00:39.029682 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 04:00:39.029729 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 04:00:39.029776 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var
I0930 04:00:39.029823 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 04:00:39.029879 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var
I0930 04:00:39.029925 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 04:00:39.029971 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var
I0930 04:00:39.030018 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var
I0930 04:00:39.030064 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_0                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var
I0930 04:00:39.030116 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_1                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var
I0930 04:00:39.030164 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_10                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var
I0930 04:00:39.030211 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_11                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var
I0930 04:00:39.030257 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_12                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var
I0930 04:00:39.030304 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_13                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var
I0930 04:00:39.030350 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_14                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var
I0930 04:00:39.030397 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_15                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var
I0930 04:00:39.030443 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_2                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var
I0930 04:00:39.030489 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_3                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var
I0930 04:00:39.030535 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_4                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var
I0930 04:00:39.030582 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_5                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var
I0930 04:00:39.030628 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_6                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var
I0930 04:00:39.030674 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_7                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var
I0930 04:00:39.030720 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_8                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var
I0930 04:00:39.030766 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_9                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var
I0930 04:00:39.030812 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_0                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var
I0930 04:00:39.030858 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_1                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var
I0930 04:00:39.030904 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_10                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var
I0930 04:00:39.030956 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_11                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var
I0930 04:00:39.031003 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_12                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var
I0930 04:00:39.031049 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_13                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var
I0930 04:00:39.031099 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_14                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var
I0930 04:00:39.031147 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_15                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var
I0930 04:00:39.031194 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_2                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var
I0930 04:00:39.031240 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_3                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var
I0930 04:00:39.031286 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_4                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var
I0930 04:00:39.031333 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_5                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var
I0930 04:00:39.031379 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_6                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var
I0930 04:00:39.031425 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_7                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var
I0930 04:00:39.031472 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_8                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var
I0930 04:00:39.031518 140278725625664 py_utils.py:1468] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_9                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var
I0930 04:00:39.031565 140278725625664 py_utils.py:1468] MODEL ANALYSIS: ====================================================================================================
I0930 04:00:39.031612 140278725625664 py_utils.py:1468] MODEL ANALYSIS: total #params: 1742572800
I0930 04:00:39.031659 140278725625664 py_utils.py:1468] MODEL ANALYSIS: 
I0930 04:00:58.695075 140278725625664 trainer.py:1501] Job trainer_client start
I0930 04:00:58.709001 140278725625664 base_runner.py:57] ============================================================
I0930 04:00:58.714779 140278725625664 base_runner.py:59] allow_implicit_capture : NoneType
I0930 04:00:58.714884 140278725625664 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I0930 04:00:58.714949 140278725625664 base_runner.py:59] cluster.add_summary : NoneType
I0930 04:00:58.715005 140278725625664 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I0930 04:00:58.715058 140278725625664 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I0930 04:00:58.715121 140278725625664 base_runner.py:59] cluster.controller.devices_per_split : 1
I0930 04:00:58.715173 140278725625664 base_runner.py:59] cluster.controller.gpus_per_replica : 1
I0930 04:00:58.715223 140278725625664 base_runner.py:59] cluster.controller.name : '/job:local'
I0930 04:00:58.715272 140278725625664 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I0930 04:00:58.715322 140278725625664 base_runner.py:59] cluster.controller.replicas : 1
I0930 04:00:58.715371 140278725625664 base_runner.py:59] cluster.controller.targets : ''
I0930 04:00:58.715420 140278725625664 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I0930 04:00:58.715469 140278725625664 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I0930 04:00:58.715518 140278725625664 base_runner.py:59] cluster.decoder.devices_per_split : 1
I0930 04:00:58.715567 140278725625664 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I0930 04:00:58.715615 140278725625664 base_runner.py:59] cluster.decoder.name : '/job:local'
I0930 04:00:58.715664 140278725625664 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I0930 04:00:58.715713 140278725625664 base_runner.py:59] cluster.decoder.replicas : 1
I0930 04:00:58.715761 140278725625664 base_runner.py:59] cluster.decoder.targets : ''
I0930 04:00:58.715810 140278725625664 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I0930 04:00:58.715859 140278725625664 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I0930 04:00:58.715908 140278725625664 base_runner.py:59] cluster.evaler.devices_per_split : 1
I0930 04:00:58.715957 140278725625664 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I0930 04:00:58.716005 140278725625664 base_runner.py:59] cluster.evaler.name : '/job:local'
I0930 04:00:58.716054 140278725625664 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I0930 04:00:58.716103 140278725625664 base_runner.py:59] cluster.evaler.replicas : 1
I0930 04:00:58.716151 140278725625664 base_runner.py:59] cluster.evaler.targets : ''
I0930 04:00:58.716200 140278725625664 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I0930 04:00:58.716249 140278725625664 base_runner.py:59] cluster.input.cpus_per_replica : 1
I0930 04:00:58.716298 140278725625664 base_runner.py:59] cluster.input.devices_per_split : 1
I0930 04:00:58.716346 140278725625664 base_runner.py:59] cluster.input.gpus_per_replica : 0
I0930 04:00:58.716395 140278725625664 base_runner.py:59] cluster.input.name : '/job:local'
I0930 04:00:58.716444 140278725625664 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I0930 04:00:58.716492 140278725625664 base_runner.py:59] cluster.input.replicas : 0
I0930 04:00:58.716541 140278725625664 base_runner.py:59] cluster.input.targets : ''
I0930 04:00:58.716590 140278725625664 base_runner.py:59] cluster.input.tpus_per_replica : 0
I0930 04:00:58.716639 140278725625664 base_runner.py:59] cluster.job : 'trainer_client'
I0930 04:00:58.716688 140278725625664 base_runner.py:59] cluster.logdir : ''
I0930 04:00:58.716737 140278725625664 base_runner.py:59] cluster.mode : 'sync'
I0930 04:00:58.716785 140278725625664 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I0930 04:00:58.716834 140278725625664 base_runner.py:59] cluster.ps.devices_per_split : 1
I0930 04:00:58.716883 140278725625664 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I0930 04:00:58.716931 140278725625664 base_runner.py:59] cluster.ps.name : '/job:local'
I0930 04:00:58.716980 140278725625664 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I0930 04:00:58.717029 140278725625664 base_runner.py:59] cluster.ps.replicas : 1
I0930 04:00:58.717078 140278725625664 base_runner.py:59] cluster.ps.targets : ''
I0930 04:00:58.717127 140278725625664 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I0930 04:00:58.717175 140278725625664 base_runner.py:59] cluster.task : 0
I0930 04:00:58.717223 140278725625664 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I0930 04:00:58.717272 140278725625664 base_runner.py:59] cluster.worker.devices_per_split : 1
I0930 04:00:58.717320 140278725625664 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I0930 04:00:58.717374 140278725625664 base_runner.py:59] cluster.worker.name : '/job:local'
I0930 04:00:58.717423 140278725625664 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I0930 04:00:58.717472 140278725625664 base_runner.py:59] cluster.worker.replicas : 1
I0930 04:00:58.717535 140278725625664 base_runner.py:59] cluster.worker.targets : ''
I0930 04:00:58.717593 140278725625664 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I0930 04:00:58.717643 140278725625664 base_runner.py:59] dtype : float32
I0930 04:00:58.717692 140278725625664 base_runner.py:59] fprop_dtype : NoneType
I0930 04:00:58.717741 140278725625664 base_runner.py:59] inference_driver_name : NoneType
I0930 04:00:58.717789 140278725625664 base_runner.py:59] input.allow_implicit_capture : NoneType
I0930 04:00:58.717838 140278725625664 base_runner.py:59] input.bucket_adjust_every_n : 0
I0930 04:00:58.717888 140278725625664 base_runner.py:59] input.bucket_batch_limit : [32]
I0930 04:00:58.717937 140278725625664 base_runner.py:59] input.bucket_upper_bound : [1024]
I0930 04:00:58.717986 140278725625664 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I0930 04:00:58.718035 140278725625664 base_runner.py:59] input.dtype : float32
I0930 04:00:58.718084 140278725625664 base_runner.py:59] input.file_buffer_size : 10000000
I0930 04:00:58.718133 140278725625664 base_runner.py:59] input.file_datasource : NoneType
I0930 04:00:58.718182 140278725625664 base_runner.py:59] input.file_parallelism : 10
I0930 04:00:58.718230 140278725625664 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I0930 04:00:58.718286 140278725625664 base_runner.py:59] input.file_random_seed : 301
I0930 04:00:58.718336 140278725625664 base_runner.py:59] input.fixed_input_shape : True
I0930 04:00:58.718385 140278725625664 base_runner.py:59] input.flush_every_n : 0
I0930 04:00:58.718434 140278725625664 base_runner.py:59] input.fprop_dtype : NoneType
I0930 04:00:58.718482 140278725625664 base_runner.py:59] input.inference_driver_name : NoneType
I0930 04:00:58.718531 140278725625664 base_runner.py:59] input.is_eval : NoneType
I0930 04:00:58.718580 140278725625664 base_runner.py:59] input.is_inference : NoneType
I0930 04:00:58.718629 140278725625664 base_runner.py:59] input.name : '1bwds_train_set'
I0930 04:00:58.718677 140278725625664 base_runner.py:59] input.num_batcher_threads : 16
I0930 04:00:58.718726 140278725625664 base_runner.py:59] input.num_samples : 0
I0930 04:00:58.718775 140278725625664 base_runner.py:59] input.pad_to_max_seq_length : False
I0930 04:00:58.718823 140278725625664 base_runner.py:59] input.params_init.method : 'xavier'
I0930 04:00:58.718872 140278725625664 base_runner.py:59] input.params_init.scale : 1.000001
I0930 04:00:58.718920 140278725625664 base_runner.py:59] input.params_init.seed : NoneType
I0930 04:00:58.718969 140278725625664 base_runner.py:59] input.random_seed : NoneType
I0930 04:00:58.719017 140278725625664 base_runner.py:59] input.remote.max_inflights_per_target : 32
I0930 04:00:58.719067 140278725625664 base_runner.py:59] input.remote.shardable_batch : False
I0930 04:00:58.719115 140278725625664 base_runner.py:59] input.require_sequential_order : False
I0930 04:00:58.719164 140278725625664 base_runner.py:59] input.skip_lp_regularization : NoneType
I0930 04:00:58.719213 140278725625664 base_runner.py:59] input.source_max_length : NoneType
I0930 04:00:58.719263 140278725625664 base_runner.py:59] input.target_max_length : 1024
I0930 04:00:58.719311 140278725625664 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I0930 04:00:58.719360 140278725625664 base_runner.py:59] input.tokenizer.append_eos : True
I0930 04:00:58.719409 140278725625664 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I0930 04:00:58.719458 140278725625664 base_runner.py:59] input.tokenizer.dtype : float32
I0930 04:00:58.719507 140278725625664 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I0930 04:00:58.719560 140278725625664 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I0930 04:00:58.719610 140278725625664 base_runner.py:59] input.tokenizer.is_eval : NoneType
I0930 04:00:58.719659 140278725625664 base_runner.py:59] input.tokenizer.is_inference : NoneType
I0930 04:00:58.719707 140278725625664 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I0930 04:00:58.719755 140278725625664 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I0930 04:00:58.719804 140278725625664 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I0930 04:00:58.719852 140278725625664 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I0930 04:00:58.719901 140278725625664 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I0930 04:00:58.719950 140278725625664 base_runner.py:59] input.tokenizer.random_seed : NoneType
I0930 04:00:58.719999 140278725625664 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I0930 04:00:58.720047 140278725625664 base_runner.py:59] input.tokenizer.target_eos_id : 2
I0930 04:00:58.720096 140278725625664 base_runner.py:59] input.tokenizer.target_sos_id : 1
I0930 04:00:58.720145 140278725625664 base_runner.py:59] input.tokenizer.target_unk_id : 0
I0930 04:00:58.720195 140278725625664 base_runner.py:59] input.tokenizer.vn.global_vn : False
I0930 04:00:58.720243 140278725625664 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I0930 04:00:58.720292 140278725625664 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I0930 04:00:58.720341 140278725625664 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I0930 04:00:58.720390 140278725625664 base_runner.py:59] input.tokenizer.vocab_size : 32000
I0930 04:00:58.720439 140278725625664 base_runner.py:59] input.tokenizer_dict : {}
I0930 04:00:58.720487 140278725625664 base_runner.py:59] input.tpu_infeed_parallelism : 1
I0930 04:00:58.720537 140278725625664 base_runner.py:59] input.use_chaining : False
I0930 04:00:58.720585 140278725625664 base_runner.py:59] input.use_per_host_infeed : False
I0930 04:00:58.720634 140278725625664 base_runner.py:59] input.use_within_batch_mixing : False
I0930 04:00:58.720683 140278725625664 base_runner.py:59] input.vn.global_vn : False
I0930 04:00:58.720732 140278725625664 base_runner.py:59] input.vn.per_step_vn : False
I0930 04:00:58.720780 140278725625664 base_runner.py:59] input.vn.scale : NoneType
I0930 04:00:58.720829 140278725625664 base_runner.py:59] input.vn.seed : NoneType
I0930 04:00:58.720878 140278725625664 base_runner.py:59] is_eval : NoneType
I0930 04:00:58.720926 140278725625664 base_runner.py:59] is_inference : NoneType
I0930 04:00:58.720975 140278725625664 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:187'
I0930 04:00:58.721025 140278725625664 base_runner.py:59] name : ''
I0930 04:00:58.721074 140278725625664 base_runner.py:59] params_init.method : 'xavier'
I0930 04:00:58.721123 140278725625664 base_runner.py:59] params_init.scale : 1.000001
I0930 04:00:58.721172 140278725625664 base_runner.py:59] params_init.seed : NoneType
I0930 04:00:58.721220 140278725625664 base_runner.py:59] random_seed : NoneType
I0930 04:00:58.721269 140278725625664 base_runner.py:59] skip_lp_regularization : NoneType
I0930 04:00:58.721318 140278725625664 base_runner.py:59] task.allow_implicit_capture : NoneType
I0930 04:00:58.721368 140278725625664 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I0930 04:00:58.721416 140278725625664 base_runner.py:59] task.decoder : NoneType
I0930 04:00:58.721465 140278725625664 base_runner.py:59] task.dtype : float32
I0930 04:00:58.721513 140278725625664 base_runner.py:59] task.encoder : NoneType
I0930 04:00:58.721587 140278725625664 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I0930 04:00:58.721637 140278725625664 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I0930 04:00:58.721690 140278725625664 base_runner.py:59] task.eval.samples_per_summary : 0
I0930 04:00:58.721746 140278725625664 base_runner.py:59] task.eval.start_decoder_after : 0
I0930 04:00:58.721795 140278725625664 base_runner.py:59] task.eval.start_eval_after : 0
I0930 04:00:58.721844 140278725625664 base_runner.py:59] task.fprop_dtype : NoneType
I0930 04:00:58.721892 140278725625664 base_runner.py:59] task.inference_driver_name : NoneType
I0930 04:00:58.721941 140278725625664 base_runner.py:59] task.input : NoneType
I0930 04:00:58.721989 140278725625664 base_runner.py:59] task.is_eval : NoneType
I0930 04:00:58.722038 140278725625664 base_runner.py:59] task.is_inference : NoneType
I0930 04:00:58.722086 140278725625664 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I0930 04:00:58.722135 140278725625664 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I0930 04:00:58.722183 140278725625664 base_runner.py:59] task.lm.dtype : float32
I0930 04:00:58.722231 140278725625664 base_runner.py:59] task.lm.fprop_dtype : NoneType
I0930 04:00:58.722279 140278725625664 base_runner.py:59] task.lm.inference_driver_name : NoneType
I0930 04:00:58.722327 140278725625664 base_runner.py:59] task.lm.is_eval : NoneType
I0930 04:00:58.722375 140278725625664 base_runner.py:59] task.lm.is_inference : NoneType
I0930 04:00:58.722424 140278725625664 base_runner.py:59] task.lm.name : 'transformerlm'
I0930 04:00:58.722472 140278725625664 base_runner.py:59] task.lm.params_init.method : 'xavier'
I0930 04:00:58.722521 140278725625664 base_runner.py:59] task.lm.params_init.scale : 1.000001
I0930 04:00:58.722570 140278725625664 base_runner.py:59] task.lm.params_init.seed : NoneType
I0930 04:00:58.722618 140278725625664 base_runner.py:59] task.lm.random_seed : NoneType
I0930 04:00:58.722666 140278725625664 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I0930 04:00:58.722715 140278725625664 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I0930 04:00:58.722763 140278725625664 base_runner.py:59] task.lm.stack.batch_dim : 1
I0930 04:00:58.722811 140278725625664 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I0930 04:00:58.722860 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.722909 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 04:00:58.722957 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I0930 04:00:58.723006 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I0930 04:00:58.723054 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I0930 04:00:58.723102 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I0930 04:00:58.723150 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I0930 04:00:58.723199 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I0930 04:00:58.723247 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I0930 04:00:58.723295 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I0930 04:00:58.723345 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I0930 04:00:58.723393 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.723441 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 04:00:58.723490 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I0930 04:00:58.723539 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I0930 04:00:58.723587 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 04:00:58.723636 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 04:00:58.723690 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I0930 04:00:58.723739 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I0930 04:00:58.723788 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I0930 04:00:58.723837 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I0930 04:00:58.723887 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 04:00:58.723936 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 04:00:58.723985 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 04:00:58.724034 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I0930 04:00:58.724082 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.724131 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I0930 04:00:58.724180 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 04:00:58.724229 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I0930 04:00:58.724278 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I0930 04:00:58.724327 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I0930 04:00:58.724375 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I0930 04:00:58.724423 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I0930 04:00:58.724472 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I0930 04:00:58.724520 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I0930 04:00:58.724569 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I0930 04:00:58.724618 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I0930 04:00:58.724666 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I0930 04:00:58.724715 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I0930 04:00:58.724764 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.724812 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I0930 04:00:58.724861 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 04:00:58.724910 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.724959 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 04:00:58.725008 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 04:00:58.725057 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.725106 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 04:00:58.725154 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 04:00:58.725203 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 04:00:58.725252 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 04:00:58.725300 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 04:00:58.725349 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 04:00:58.725401 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 04:00:58.725452 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I0930 04:00:58.725501 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 04:00:58.725567 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 04:00:58.725618 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 04:00:58.725668 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 04:00:58.725717 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 04:00:58.725765 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 04:00:58.725814 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 04:00:58.725863 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 04:00:58.725912 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 04:00:58.725961 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 04:00:58.726010 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 04:00:58.726058 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 04:00:58.726107 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 04:00:58.726156 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 04:00:58.726206 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 04:00:58.726255 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 04:00:58.726304 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 04:00:58.726353 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 04:00:58.726402 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 04:00:58.726450 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 04:00:58.726499 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 04:00:58.726547 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 04:00:58.726595 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 04:00:58.726644 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 04:00:58.726692 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 04:00:58.726740 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 04:00:58.726793 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 04:00:58.726843 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 04:00:58.726891 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 04:00:58.726940 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 04:00:58.726988 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 04:00:58.727038 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 04:00:58.727086 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 04:00:58.727135 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 04:00:58.727184 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 04:00:58.727232 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 04:00:58.727281 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 04:00:58.727329 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 04:00:58.727378 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 04:00:58.727426 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 04:00:58.727475 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 04:00:58.727523 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 04:00:58.727572 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 04:00:58.727620 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 04:00:58.727669 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 04:00:58.727718 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.727767 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 04:00:58.727815 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 04:00:58.727864 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 04:00:58.727912 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 04:00:58.727961 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 04:00:58.728009 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 04:00:58.728058 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 04:00:58.728106 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I0930 04:00:58.728154 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I0930 04:00:58.728203 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 04:00:58.728251 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 04:00:58.728307 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 04:00:58.728357 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 04:00:58.728406 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I0930 04:00:58.728454 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.728502 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 04:00:58.728551 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 04:00:58.728599 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 04:00:58.728648 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 04:00:58.728697 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 04:00:58.728745 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 04:00:58.728794 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 04:00:58.728842 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 04:00:58.728889 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 04:00:58.728938 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 04:00:58.728986 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 04:00:58.729034 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 04:00:58.729083 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 04:00:58.729131 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.729179 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 04:00:58.729228 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 04:00:58.729276 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 04:00:58.729325 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 04:00:58.729373 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 04:00:58.729421 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I0930 04:00:58.729470 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I0930 04:00:58.729518 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I0930 04:00:58.729585 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 04:00:58.729635 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 04:00:58.729683 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 04:00:58.729732 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 04:00:58.729781 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 04:00:58.729829 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.729878 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 04:00:58.729931 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 04:00:58.729980 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 04:00:58.730029 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 04:00:58.730077 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 04:00:58.730126 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 04:00:58.730176 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 04:00:58.730225 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 04:00:58.730274 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 04:00:58.730323 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 04:00:58.730372 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 04:00:58.730420 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 04:00:58.730469 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 04:00:58.730518 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 04:00:58.730566 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 04:00:58.730615 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.730663 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 04:00:58.730712 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 04:00:58.730760 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 04:00:58.730808 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 04:00:58.730856 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.730904 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I0930 04:00:58.730952 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 04:00:58.731000 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 04:00:58.731049 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 04:00:58.731098 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 04:00:58.731146 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I0930 04:00:58.731194 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 04:00:58.731242 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.731290 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 04:00:58.731338 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 04:00:58.731391 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 04:00:58.731441 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.731489 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 04:00:58.731537 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 04:00:58.731585 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 04:00:58.731633 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 04:00:58.731681 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 04:00:58.731729 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 04:00:58.731778 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 04:00:58.731826 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 04:00:58.731874 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 04:00:58.731922 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 04:00:58.731970 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 04:00:58.732019 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 04:00:58.732067 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 04:00:58.732115 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 04:00:58.732164 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 04:00:58.732212 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 04:00:58.732260 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 04:00:58.732308 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 04:00:58.732357 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 04:00:58.732405 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 04:00:58.732453 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 04:00:58.732500 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 04:00:58.732549 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 04:00:58.732597 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 04:00:58.732645 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 04:00:58.732697 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 04:00:58.732746 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 04:00:58.732798 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 04:00:58.732848 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 04:00:58.732896 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 04:00:58.732944 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 04:00:58.732992 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 04:00:58.733041 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 04:00:58.733090 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 04:00:58.733137 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 04:00:58.733186 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 04:00:58.733233 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 04:00:58.733281 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 04:00:58.733330 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 04:00:58.733378 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 04:00:58.733426 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 04:00:58.733474 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 04:00:58.733538 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 04:00:58.733592 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 04:00:58.733642 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 04:00:58.733690 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 04:00:58.733739 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 04:00:58.733787 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 04:00:58.733835 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 04:00:58.733884 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 04:00:58.733932 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 04:00:58.733980 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 04:00:58.734028 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 04:00:58.734077 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 04:00:58.734125 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 04:00:58.734174 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 04:00:58.734226 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 04:00:58.734275 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 04:00:58.734324 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 04:00:58.734372 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 04:00:58.734421 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 04:00:58.734469 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 04:00:58.734517 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 04:00:58.734565 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 04:00:58.734614 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.734662 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 04:00:58.734710 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 04:00:58.734759 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 04:00:58.734807 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 04:00:58.734855 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 04:00:58.734904 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 04:00:58.734953 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I0930 04:00:58.735001 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 04:00:58.735050 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 04:00:58.735098 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 04:00:58.735147 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 04:00:58.735196 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.735244 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 04:00:58.735302 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 04:00:58.735351 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 04:00:58.735399 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 04:00:58.735447 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 04:00:58.735495 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 04:00:58.735543 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 04:00:58.735591 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 04:00:58.735639 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 04:00:58.735687 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 04:00:58.735739 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 04:00:58.735788 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 04:00:58.735837 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 04:00:58.735885 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.735934 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 04:00:58.735982 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 04:00:58.736031 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 04:00:58.736080 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 04:00:58.736127 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I0930 04:00:58.736175 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 04:00:58.736223 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 04:00:58.736271 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 04:00:58.736319 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 04:00:58.736367 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 04:00:58.736415 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 04:00:58.736464 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 04:00:58.736512 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 04:00:58.736560 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.736608 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 04:00:58.736655 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 04:00:58.736703 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 04:00:58.736751 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 04:00:58.736799 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 04:00:58.736847 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 04:00:58.736895 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 04:00:58.736943 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 04:00:58.736991 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 04:00:58.737039 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 04:00:58.737087 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 04:00:58.737135 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 04:00:58.737183 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 04:00:58.737230 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 04:00:58.737282 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 04:00:58.737330 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 04:00:58.737378 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 04:00:58.737425 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 04:00:58.737473 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.737531 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 04:00:58.737587 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 04:00:58.737637 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 04:00:58.737685 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 04:00:58.737734 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 04:00:58.737782 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 04:00:58.737830 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.737879 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 04:00:58.737926 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 04:00:58.737974 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 04:00:58.738022 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 04:00:58.738070 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 04:00:58.738118 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 04:00:58.738167 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 04:00:58.738215 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 04:00:58.738263 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 04:00:58.738312 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 04:00:58.738368 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 04:00:58.738417 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 04:00:58.738465 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 04:00:58.738513 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 04:00:58.738562 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 04:00:58.738610 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.738659 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 04:00:58.738714 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 04:00:58.738763 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 04:00:58.738811 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 04:00:58.738860 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.738909 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 04:00:58.738958 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 04:00:58.739007 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 04:00:58.739055 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 04:00:58.739103 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I0930 04:00:58.739152 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I0930 04:00:58.739200 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I0930 04:00:58.739249 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I0930 04:00:58.739297 140278725625664 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I0930 04:00:58.739346 140278725625664 base_runner.py:59] task.lm.stack.dtype : float32
I0930 04:00:58.739395 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I0930 04:00:58.739444 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.739492 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I0930 04:00:58.739541 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I0930 04:00:58.739589 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I0930 04:00:58.739638 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.739686 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 04:00:58.739734 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I0930 04:00:58.739782 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I0930 04:00:58.739830 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 04:00:58.739879 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 04:00:58.739927 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I0930 04:00:58.739975 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I0930 04:00:58.740024 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I0930 04:00:58.740072 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I0930 04:00:58.740120 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I0930 04:00:58.740169 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 04:00:58.740218 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 04:00:58.740266 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 04:00:58.740315 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I0930 04:00:58.740363 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I0930 04:00:58.740416 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.740465 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I0930 04:00:58.740514 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I0930 04:00:58.740562 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I0930 04:00:58.740611 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I0930 04:00:58.740659 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I0930 04:00:58.740708 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I0930 04:00:58.740756 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I0930 04:00:58.740804 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I0930 04:00:58.740853 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I0930 04:00:58.740901 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I0930 04:00:58.740949 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I0930 04:00:58.740997 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I0930 04:00:58.741045 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I0930 04:00:58.741093 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I0930 04:00:58.741141 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I0930 04:00:58.741189 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I0930 04:00:58.741237 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I0930 04:00:58.741285 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I0930 04:00:58.741333 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I0930 04:00:58.741382 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I0930 04:00:58.741430 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I0930 04:00:58.741477 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I0930 04:00:58.741537 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I0930 04:00:58.741591 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I0930 04:00:58.741641 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I0930 04:00:58.741689 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I0930 04:00:58.741737 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I0930 04:00:58.741786 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I0930 04:00:58.741834 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I0930 04:00:58.741882 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I0930 04:00:58.741931 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I0930 04:00:58.741979 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I0930 04:00:58.742027 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I0930 04:00:58.742076 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I0930 04:00:58.742125 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I0930 04:00:58.742173 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I0930 04:00:58.742225 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I0930 04:00:58.742274 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I0930 04:00:58.742323 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I0930 04:00:58.742372 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I0930 04:00:58.742420 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I0930 04:00:58.742468 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.742516 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I0930 04:00:58.742565 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I0930 04:00:58.742613 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I0930 04:00:58.742661 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I0930 04:00:58.742710 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I0930 04:00:58.742758 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I0930 04:00:58.742806 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I0930 04:00:58.742854 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I0930 04:00:58.742901 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I0930 04:00:58.742949 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I0930 04:00:58.742997 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I0930 04:00:58.743045 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I0930 04:00:58.743093 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I0930 04:00:58.743141 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I0930 04:00:58.743189 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I0930 04:00:58.743238 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I0930 04:00:58.743286 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I0930 04:00:58.743333 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I0930 04:00:58.743381 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I0930 04:00:58.743430 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I0930 04:00:58.743478 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I0930 04:00:58.743526 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I0930 04:00:58.743573 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I0930 04:00:58.743621 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I0930 04:00:58.743669 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I0930 04:00:58.743717 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I0930 04:00:58.743765 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I0930 04:00:58.743813 140278725625664 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I0930 04:00:58.743861 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.743910 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 04:00:58.743962 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I0930 04:00:58.744011 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I0930 04:00:58.744059 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I0930 04:00:58.744107 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I0930 04:00:58.744156 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I0930 04:00:58.744204 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I0930 04:00:58.744252 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I0930 04:00:58.744301 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I0930 04:00:58.744349 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I0930 04:00:58.744398 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.744447 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 04:00:58.744496 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I0930 04:00:58.744544 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I0930 04:00:58.744592 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 04:00:58.744641 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 04:00:58.744689 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I0930 04:00:58.744737 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I0930 04:00:58.744786 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I0930 04:00:58.744834 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I0930 04:00:58.744882 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 04:00:58.744930 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 04:00:58.744979 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 04:00:58.745028 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I0930 04:00:58.745075 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.745123 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I0930 04:00:58.745171 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 04:00:58.745219 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I0930 04:00:58.745267 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I0930 04:00:58.745315 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I0930 04:00:58.745364 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I0930 04:00:58.745412 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I0930 04:00:58.745460 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I0930 04:00:58.745508 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I0930 04:00:58.745577 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I0930 04:00:58.745628 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I0930 04:00:58.745677 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I0930 04:00:58.745725 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I0930 04:00:58.745779 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.745828 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I0930 04:00:58.745876 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 04:00:58.745924 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.745972 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 04:00:58.746020 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 04:00:58.746069 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.746117 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 04:00:58.746165 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 04:00:58.746214 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 04:00:58.746262 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 04:00:58.746310 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 04:00:58.746359 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 04:00:58.746407 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 04:00:58.746456 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I0930 04:00:58.746504 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 04:00:58.746553 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 04:00:58.746601 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 04:00:58.746649 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 04:00:58.746698 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 04:00:58.746746 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 04:00:58.746795 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 04:00:58.746843 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 04:00:58.746898 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 04:00:58.746946 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 04:00:58.746994 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 04:00:58.747043 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 04:00:58.747092 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 04:00:58.747140 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 04:00:58.747189 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 04:00:58.747241 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 04:00:58.747290 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 04:00:58.747339 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 04:00:58.747388 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 04:00:58.747436 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 04:00:58.747485 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 04:00:58.747533 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 04:00:58.747581 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 04:00:58.747630 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 04:00:58.747678 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 04:00:58.747727 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 04:00:58.747775 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 04:00:58.747823 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 04:00:58.747872 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 04:00:58.747920 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 04:00:58.747968 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 04:00:58.748017 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 04:00:58.748065 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 04:00:58.748114 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 04:00:58.748162 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 04:00:58.748210 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 04:00:58.748259 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 04:00:58.748307 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 04:00:58.748355 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 04:00:58.748405 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 04:00:58.748455 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 04:00:58.748503 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 04:00:58.748551 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 04:00:58.748600 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 04:00:58.748653 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 04:00:58.748702 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.748750 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 04:00:58.748799 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 04:00:58.748847 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 04:00:58.748895 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 04:00:58.748943 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 04:00:58.748991 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 04:00:58.749039 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 04:00:58.749087 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I0930 04:00:58.749135 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I0930 04:00:58.749184 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 04:00:58.749233 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 04:00:58.749282 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 04:00:58.749329 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 04:00:58.749377 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I0930 04:00:58.749426 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.749474 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 04:00:58.749534 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 04:00:58.749589 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 04:00:58.749640 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 04:00:58.749689 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 04:00:58.749737 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 04:00:58.749786 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 04:00:58.749835 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 04:00:58.749883 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 04:00:58.749931 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 04:00:58.749979 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 04:00:58.750027 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 04:00:58.750076 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 04:00:58.750124 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.750173 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 04:00:58.750221 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 04:00:58.750275 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 04:00:58.750324 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 04:00:58.750373 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 04:00:58.750422 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I0930 04:00:58.750470 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I0930 04:00:58.750519 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I0930 04:00:58.750567 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 04:00:58.750616 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 04:00:58.750664 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 04:00:58.750713 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 04:00:58.750761 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 04:00:58.750809 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.750857 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 04:00:58.750905 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 04:00:58.750953 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 04:00:58.751001 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 04:00:58.751049 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 04:00:58.751097 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 04:00:58.751146 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 04:00:58.751194 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 04:00:58.751242 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 04:00:58.751290 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 04:00:58.751338 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 04:00:58.751386 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 04:00:58.751435 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 04:00:58.751483 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 04:00:58.751532 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 04:00:58.751580 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.751630 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 04:00:58.751678 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 04:00:58.751731 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 04:00:58.751785 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 04:00:58.751835 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.751883 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I0930 04:00:58.751931 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 04:00:58.751980 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 04:00:58.752027 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 04:00:58.752076 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 04:00:58.752124 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I0930 04:00:58.752172 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 04:00:58.752220 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.752269 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 04:00:58.752317 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 04:00:58.752366 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 04:00:58.752415 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.752463 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 04:00:58.752511 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 04:00:58.752560 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 04:00:58.752609 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 04:00:58.752657 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 04:00:58.752706 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 04:00:58.752755 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 04:00:58.752804 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 04:00:58.752852 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 04:00:58.752900 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 04:00:58.752949 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 04:00:58.752997 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 04:00:58.753045 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 04:00:58.753093 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 04:00:58.753141 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 04:00:58.753194 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 04:00:58.753243 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 04:00:58.753295 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 04:00:58.753344 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 04:00:58.753392 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 04:00:58.753441 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 04:00:58.753489 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 04:00:58.753556 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 04:00:58.753610 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 04:00:58.753659 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 04:00:58.753708 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 04:00:58.753757 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 04:00:58.753806 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 04:00:58.753855 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 04:00:58.753903 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 04:00:58.753952 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 04:00:58.754001 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 04:00:58.754049 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 04:00:58.754098 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 04:00:58.754147 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 04:00:58.754195 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 04:00:58.754244 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 04:00:58.754292 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 04:00:58.754341 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 04:00:58.754389 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 04:00:58.754438 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 04:00:58.754487 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 04:00:58.754535 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 04:00:58.754585 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 04:00:58.754633 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 04:00:58.754687 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 04:00:58.754737 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 04:00:58.754786 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 04:00:58.754834 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 04:00:58.754883 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 04:00:58.754932 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 04:00:58.754981 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 04:00:58.755029 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 04:00:58.755078 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 04:00:58.755126 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 04:00:58.755175 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 04:00:58.755223 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 04:00:58.755272 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 04:00:58.755321 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 04:00:58.755370 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 04:00:58.755419 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 04:00:58.755468 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 04:00:58.755516 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 04:00:58.755565 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 04:00:58.755613 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.755661 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 04:00:58.755710 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 04:00:58.755759 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 04:00:58.755808 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 04:00:58.755857 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 04:00:58.755905 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 04:00:58.755954 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I0930 04:00:58.756013 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 04:00:58.756071 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 04:00:58.756124 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 04:00:58.756173 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 04:00:58.756222 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.756270 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 04:00:58.756318 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 04:00:58.756366 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 04:00:58.756415 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 04:00:58.756463 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 04:00:58.756511 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 04:00:58.756560 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 04:00:58.756609 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 04:00:58.756657 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 04:00:58.756705 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 04:00:58.756754 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 04:00:58.756804 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 04:00:58.756853 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 04:00:58.756901 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.756949 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 04:00:58.756998 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 04:00:58.757046 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 04:00:58.757094 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 04:00:58.757142 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I0930 04:00:58.757191 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 04:00:58.757240 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 04:00:58.757289 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 04:00:58.757337 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 04:00:58.757386 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 04:00:58.757434 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 04:00:58.757483 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 04:00:58.757546 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 04:00:58.757601 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.757651 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 04:00:58.757700 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 04:00:58.757753 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 04:00:58.757803 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 04:00:58.757851 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 04:00:58.757899 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 04:00:58.757948 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 04:00:58.757997 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 04:00:58.758045 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 04:00:58.758093 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 04:00:58.758142 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 04:00:58.758190 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 04:00:58.758239 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 04:00:58.758287 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 04:00:58.758336 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 04:00:58.758385 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 04:00:58.758434 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 04:00:58.758485 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 04:00:58.758535 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.758584 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 04:00:58.758633 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 04:00:58.758681 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 04:00:58.758729 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 04:00:58.758778 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 04:00:58.758826 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 04:00:58.758874 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.758921 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 04:00:58.758969 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 04:00:58.759017 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 04:00:58.759066 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 04:00:58.759114 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 04:00:58.759163 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 04:00:58.759217 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 04:00:58.759267 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 04:00:58.759315 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 04:00:58.759363 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 04:00:58.759412 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 04:00:58.759459 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 04:00:58.759508 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 04:00:58.759556 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 04:00:58.759604 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 04:00:58.759653 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.759701 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 04:00:58.759749 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 04:00:58.759800 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 04:00:58.759848 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 04:00:58.759897 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.759945 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 04:00:58.759993 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 04:00:58.760042 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 04:00:58.760091 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 04:00:58.760139 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I0930 04:00:58.760188 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I0930 04:00:58.760236 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I0930 04:00:58.760284 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I0930 04:00:58.760333 140278725625664 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I0930 04:00:58.760380 140278725625664 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I0930 04:00:58.760429 140278725625664 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I0930 04:00:58.760477 140278725625664 base_runner.py:59] task.lm.stack.is_eval : NoneType
I0930 04:00:58.760525 140278725625664 base_runner.py:59] task.lm.stack.is_inference : NoneType
I0930 04:00:58.760573 140278725625664 base_runner.py:59] task.lm.stack.is_transparent : False
I0930 04:00:58.760621 140278725625664 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I0930 04:00:58.760670 140278725625664 base_runner.py:59] task.lm.stack.model_dim : 2048
I0930 04:00:58.760718 140278725625664 base_runner.py:59] task.lm.stack.name : ''
I0930 04:00:58.760766 140278725625664 base_runner.py:59] task.lm.stack.normalize_encoder : False
I0930 04:00:58.760819 140278725625664 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I0930 04:00:58.760867 140278725625664 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I0930 04:00:58.760915 140278725625664 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I0930 04:00:58.760963 140278725625664 base_runner.py:59] task.lm.stack.packed_input : False
I0930 04:00:58.761010 140278725625664 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I0930 04:00:58.761059 140278725625664 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I0930 04:00:58.761107 140278725625664 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I0930 04:00:58.761155 140278725625664 base_runner.py:59] task.lm.stack.random_seed : NoneType
I0930 04:00:58.761203 140278725625664 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I0930 04:00:58.761252 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.761300 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I0930 04:00:58.761348 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I0930 04:00:58.761396 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I0930 04:00:58.761444 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I0930 04:00:58.761492 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I0930 04:00:58.761556 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I0930 04:00:58.761608 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I0930 04:00:58.761656 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I0930 04:00:58.761705 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I0930 04:00:58.761754 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I0930 04:00:58.761802 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I0930 04:00:58.761851 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I0930 04:00:58.761899 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I0930 04:00:58.761948 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I0930 04:00:58.761996 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I0930 04:00:58.762044 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I0930 04:00:58.762092 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I0930 04:00:58.762141 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I0930 04:00:58.762189 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I0930 04:00:58.762237 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I0930 04:00:58.762286 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.762335 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I0930 04:00:58.762383 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I0930 04:00:58.762431 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I0930 04:00:58.762479 140278725625664 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I0930 04:00:58.762527 140278725625664 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I0930 04:00:58.762575 140278725625664 base_runner.py:59] task.lm.stack.state_dtype : float32
I0930 04:00:58.762624 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I0930 04:00:58.762672 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.762725 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I0930 04:00:58.762775 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 04:00:58.762823 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I0930 04:00:58.762872 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I0930 04:00:58.762920 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I0930 04:00:58.762969 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 04:00:58.763017 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 04:00:58.763066 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I0930 04:00:58.763114 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I0930 04:00:58.763163 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I0930 04:00:58.763211 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I0930 04:00:58.763260 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I0930 04:00:58.763308 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 04:00:58.763356 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 04:00:58.763404 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 04:00:58.763453 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I0930 04:00:58.763501 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I0930 04:00:58.763550 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.763599 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I0930 04:00:58.763648 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I0930 04:00:58.763696 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I0930 04:00:58.763744 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I0930 04:00:58.763792 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I0930 04:00:58.763840 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I0930 04:00:58.763888 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I0930 04:00:58.763937 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I0930 04:00:58.763986 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I0930 04:00:58.764034 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I0930 04:00:58.764082 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I0930 04:00:58.764131 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I0930 04:00:58.764179 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I0930 04:00:58.764227 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I0930 04:00:58.764280 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I0930 04:00:58.764329 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I0930 04:00:58.764377 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I0930 04:00:58.764426 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I0930 04:00:58.764474 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I0930 04:00:58.764523 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I0930 04:00:58.764570 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I0930 04:00:58.764619 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I0930 04:00:58.764667 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I0930 04:00:58.764716 140278725625664 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I0930 04:00:58.764764 140278725625664 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I0930 04:00:58.764811 140278725625664 base_runner.py:59] task.lm.stack.vn.global_vn : False
I0930 04:00:58.764859 140278725625664 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I0930 04:00:58.764907 140278725625664 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I0930 04:00:58.764954 140278725625664 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I0930 04:00:58.765002 140278725625664 base_runner.py:59] task.lm.vn.global_vn : False
I0930 04:00:58.765050 140278725625664 base_runner.py:59] task.lm.vn.per_step_vn : False
I0930 04:00:58.765099 140278725625664 base_runner.py:59] task.lm.vn.scale : NoneType
I0930 04:00:58.765147 140278725625664 base_runner.py:59] task.lm.vn.seed : NoneType
I0930 04:00:58.765195 140278725625664 base_runner.py:59] task.lm.vocab_size : 32000
I0930 04:00:58.765243 140278725625664 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I0930 04:00:58.765291 140278725625664 base_runner.py:59] task.online_encoder : NoneType
I0930 04:00:58.765340 140278725625664 base_runner.py:59] task.params_init.method : 'xavier'
I0930 04:00:58.765388 140278725625664 base_runner.py:59] task.params_init.scale : 1.000001
I0930 04:00:58.765437 140278725625664 base_runner.py:59] task.params_init.seed : NoneType
I0930 04:00:58.765485 140278725625664 base_runner.py:59] task.random_seed : NoneType
I0930 04:00:58.765546 140278725625664 base_runner.py:59] task.skip_lp_regularization : NoneType
I0930 04:00:58.765600 140278725625664 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I0930 04:00:58.765650 140278725625664 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I0930 04:00:58.765699 140278725625664 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I0930 04:00:58.765747 140278725625664 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I0930 04:00:58.765796 140278725625664 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I0930 04:00:58.765845 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I0930 04:00:58.765893 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I0930 04:00:58.765942 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I0930 04:00:58.765990 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I0930 04:00:58.766039 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I0930 04:00:58.766088 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I0930 04:00:58.766137 140278725625664 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I0930 04:00:58.766185 140278725625664 base_runner.py:59] task.train.early_stop.min_steps : 0
I0930 04:00:58.766238 140278725625664 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I0930 04:00:58.766287 140278725625664 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I0930 04:00:58.766335 140278725625664 base_runner.py:59] task.train.early_stop.verbose : True
I0930 04:00:58.766384 140278725625664 base_runner.py:59] task.train.early_stop.window : 0
I0930 04:00:58.766432 140278725625664 base_runner.py:59] task.train.ema_decay : 0.0
I0930 04:00:58.766480 140278725625664 base_runner.py:59] task.train.enqueue_max_steps : -1
I0930 04:00:58.766529 140278725625664 base_runner.py:59] task.train.gate_gradients : False
I0930 04:00:58.766577 140278725625664 base_runner.py:59] task.train.grad_aggregation_method : 1
I0930 04:00:58.766631 140278725625664 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I0930 04:00:58.766679 140278725625664 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I0930 04:00:58.766727 140278725625664 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I0930 04:00:58.766776 140278725625664 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I0930 04:00:58.766824 140278725625664 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I0930 04:00:58.766872 140278725625664 base_runner.py:59] task.train.learner : NoneType
I0930 04:00:58.766921 140278725625664 base_runner.py:59] task.train.learning_rate : 0.5
I0930 04:00:58.766969 140278725625664 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I0930 04:00:58.767017 140278725625664 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 04:00:58.767065 140278725625664 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I0930 04:00:58.767114 140278725625664 base_runner.py:59] task.train.lr_schedule.dtype : float32
I0930 04:00:58.767162 140278725625664 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I0930 04:00:58.767210 140278725625664 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I0930 04:00:58.767259 140278725625664 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I0930 04:00:58.767308 140278725625664 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I0930 04:00:58.767356 140278725625664 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I0930 04:00:58.767405 140278725625664 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I0930 04:00:58.767452 140278725625664 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I0930 04:00:58.767501 140278725625664 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I0930 04:00:58.767549 140278725625664 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I0930 04:00:58.767596 140278725625664 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I0930 04:00:58.767644 140278725625664 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I0930 04:00:58.767693 140278725625664 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I0930 04:00:58.767742 140278725625664 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I0930 04:00:58.767790 140278725625664 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I0930 04:00:58.767838 140278725625664 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I0930 04:00:58.767886 140278725625664 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I0930 04:00:58.767934 140278725625664 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I0930 04:00:58.767982 140278725625664 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I0930 04:00:58.768030 140278725625664 base_runner.py:59] task.train.max_steps : 4000000
I0930 04:00:58.768078 140278725625664 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I0930 04:00:58.768126 140278725625664 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I0930 04:00:58.768174 140278725625664 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I0930 04:00:58.768226 140278725625664 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 04:00:58.768275 140278725625664 base_runner.py:59] task.train.optimizer.dtype : float32
I0930 04:00:58.768323 140278725625664 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I0930 04:00:58.768372 140278725625664 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I0930 04:00:58.768420 140278725625664 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I0930 04:00:58.768468 140278725625664 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I0930 04:00:58.768519 140278725625664 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I0930 04:00:58.768568 140278725625664 base_runner.py:59] task.train.optimizer.name : 'Adam'
I0930 04:00:58.768617 140278725625664 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I0930 04:00:58.768666 140278725625664 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I0930 04:00:58.768714 140278725625664 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I0930 04:00:58.768763 140278725625664 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I0930 04:00:58.768812 140278725625664 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I0930 04:00:58.768860 140278725625664 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I0930 04:00:58.768908 140278725625664 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I0930 04:00:58.768956 140278725625664 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I0930 04:00:58.769004 140278725625664 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I0930 04:00:58.769052 140278725625664 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I0930 04:00:58.769100 140278725625664 base_runner.py:59] task.train.save_interval_seconds : 600
I0930 04:00:58.769148 140278725625664 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I0930 04:00:58.769195 140278725625664 base_runner.py:59] task.train.save_max_to_keep : 100
I0930 04:00:58.769243 140278725625664 base_runner.py:59] task.train.start_up_delay_steps : 200
I0930 04:00:58.769291 140278725625664 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I0930 04:00:58.769338 140278725625664 base_runner.py:59] task.train.summary_interval_steps : 100
I0930 04:00:58.769386 140278725625664 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I0930 04:00:58.769433 140278725625664 base_runner.py:59] task.train.vn_start_step : 20000
I0930 04:00:58.769480 140278725625664 base_runner.py:59] task.train.vn_std : 0.0
I0930 04:00:58.769545 140278725625664 base_runner.py:59] task.vn.global_vn : False
I0930 04:00:58.769598 140278725625664 base_runner.py:59] task.vn.per_step_vn : False
I0930 04:00:58.769648 140278725625664 base_runner.py:59] task.vn.scale : NoneType
I0930 04:00:58.769696 140278725625664 base_runner.py:59] task.vn.seed : NoneType
I0930 04:00:58.769744 140278725625664 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I0930 04:00:58.769793 140278725625664 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I0930 04:00:58.769840 140278725625664 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I0930 04:00:58.769889 140278725625664 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I0930 04:00:58.769937 140278725625664 base_runner.py:59] train.early_stop.metric_history.minimize : True
I0930 04:00:58.769986 140278725625664 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I0930 04:00:58.770034 140278725625664 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I0930 04:00:58.770082 140278725625664 base_runner.py:59] train.early_stop.min_steps : 0
I0930 04:00:58.770130 140278725625664 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I0930 04:00:58.770178 140278725625664 base_runner.py:59] train.early_stop.tolerance : 0.0
I0930 04:00:58.770230 140278725625664 base_runner.py:59] train.early_stop.verbose : True
I0930 04:00:58.770279 140278725625664 base_runner.py:59] train.early_stop.window : 0
I0930 04:00:58.770327 140278725625664 base_runner.py:59] train.ema_decay : 0.0
I0930 04:00:58.770375 140278725625664 base_runner.py:59] train.enqueue_max_steps : -1
I0930 04:00:58.770424 140278725625664 base_runner.py:59] train.init_from_checkpoint_rules : {}
I0930 04:00:58.770473 140278725625664 base_runner.py:59] train.max_steps : 4000000
I0930 04:00:58.770521 140278725625664 base_runner.py:59] train.save_interval_seconds : 600
I0930 04:00:58.770570 140278725625664 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I0930 04:00:58.770619 140278725625664 base_runner.py:59] train.save_max_to_keep : 100
I0930 04:00:58.770667 140278725625664 base_runner.py:59] train.start_up_delay_steps : 200
I0930 04:00:58.770716 140278725625664 base_runner.py:59] train.summary_interval_steps : 100
I0930 04:00:58.770764 140278725625664 base_runner.py:59] train.tpu_steps_per_loop : 100
I0930 04:00:58.770812 140278725625664 base_runner.py:59] vn.global_vn : False
I0930 04:00:58.770861 140278725625664 base_runner.py:59] vn.per_step_vn : False
I0930 04:00:58.770909 140278725625664 base_runner.py:59] vn.scale : NoneType
I0930 04:00:58.770958 140278725625664 base_runner.py:59] vn.seed : NoneType
I0930 04:00:58.771006 140278725625664 base_runner.py:59] 
I0930 04:00:58.771092 140278725625664 base_runner.py:60] ============================================================
I0930 04:00:58.773442 140278725625664 base_runner.py:106] Starting ...
I0930 04:00:58.773700 140278725625664 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I0930 04:00:58.783072 140278725625664 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I0930 04:00:58.796689 140278725625664 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I0930 04:00:58.812886 140278725625664 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I0930 04:00:58.816224 140278725625664 base_input_generator.py:624] bucket_batch_limit [32]
I0930 04:00:58.865831 140278725625664 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I0930 04:00:58.865933 140278725625664 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I0930 04:00:58.865998 140278725625664 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I0930 04:00:58.866055 140278725625664 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I0930 04:00:58.866107 140278725625664 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I0930 04:00:58.866159 140278725625664 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I0930 04:00:58.866264 140278725625664 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I0930 04:00:58.866319 140278725625664 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I0930 04:00:58.866373 140278725625664 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I0930 04:00:58.866422 140278725625664 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I0930 04:00:58.866471 140278725625664 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I0930 04:00:58.866520 140278725625664 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I0930 04:00:58.866567 140278725625664 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I0930 04:00:58.866618 140278725625664 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I0930 04:00:58.866667 140278725625664 learner.py:351] Ignoring legacy param learner=None for optimization program
I0930 04:00:58.866748 140278725625664 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I0930 04:00:58.866802 140278725625664 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I0930 04:00:58.867249 140278725625664 learner.py:356] Learner params: allow_implicit_capture : NoneType
I0930 04:00:58.867331 140278725625664 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I0930 04:00:58.867393 140278725625664 learner.py:356] Learner params: bprop_variable_filter : NoneType
I0930 04:00:58.867448 140278725625664 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I0930 04:00:58.867499 140278725625664 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I0930 04:00:58.867550 140278725625664 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I0930 04:00:58.867600 140278725625664 learner.py:356] Learner params: colocate_gradients_with_ops : True
I0930 04:00:58.867650 140278725625664 learner.py:356] Learner params: dtype : float32
I0930 04:00:58.867699 140278725625664 learner.py:356] Learner params: fprop_dtype : NoneType
I0930 04:00:58.867749 140278725625664 learner.py:356] Learner params: gate_gradients : False
I0930 04:00:58.867799 140278725625664 learner.py:356] Learner params: grad_aggregation_method : 1
I0930 04:00:58.867848 140278725625664 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I0930 04:00:58.867897 140278725625664 learner.py:356] Learner params: grad_norm_tracker : NoneType
I0930 04:00:58.867946 140278725625664 learner.py:356] Learner params: inference_driver_name : NoneType
I0930 04:00:58.868001 140278725625664 learner.py:356] Learner params: is_eval : NoneType
I0930 04:00:58.868053 140278725625664 learner.py:356] Learner params: is_inference : NoneType
I0930 04:00:58.868102 140278725625664 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I0930 04:00:58.868152 140278725625664 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I0930 04:00:58.868200 140278725625664 learner.py:356] Learner params: learning_rate : 0.5
I0930 04:00:58.868250 140278725625664 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I0930 04:00:58.868299 140278725625664 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 04:00:58.868348 140278725625664 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I0930 04:00:58.868396 140278725625664 learner.py:356] Learner params: lr_schedule.dtype : float32
I0930 04:00:58.868445 140278725625664 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I0930 04:00:58.868493 140278725625664 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I0930 04:00:58.868542 140278725625664 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I0930 04:00:58.868592 140278725625664 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I0930 04:00:58.868640 140278725625664 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I0930 04:00:58.868689 140278725625664 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I0930 04:00:58.868738 140278725625664 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I0930 04:00:58.868787 140278725625664 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I0930 04:00:58.868836 140278725625664 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I0930 04:00:58.868884 140278725625664 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I0930 04:00:58.868932 140278725625664 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I0930 04:00:58.868981 140278725625664 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I0930 04:00:58.869032 140278725625664 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I0930 04:00:58.869083 140278725625664 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I0930 04:00:58.869132 140278725625664 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I0930 04:00:58.869180 140278725625664 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I0930 04:00:58.869229 140278725625664 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I0930 04:00:58.869277 140278725625664 learner.py:356] Learner params: name : 'loss'
I0930 04:00:58.869325 140278725625664 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I0930 04:00:58.869374 140278725625664 learner.py:356] Learner params: optimizer.beta1 : 0.9
I0930 04:00:58.869423 140278725625664 learner.py:356] Learner params: optimizer.beta2 : 0.997
I0930 04:00:58.869472 140278725625664 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 04:00:58.869535 140278725625664 learner.py:356] Learner params: optimizer.dtype : float32
I0930 04:00:58.869593 140278725625664 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I0930 04:00:58.869643 140278725625664 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I0930 04:00:58.869692 140278725625664 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I0930 04:00:58.869741 140278725625664 learner.py:356] Learner params: optimizer.is_eval : NoneType
I0930 04:00:58.869790 140278725625664 learner.py:356] Learner params: optimizer.is_inference : NoneType
I0930 04:00:58.869838 140278725625664 learner.py:356] Learner params: optimizer.name : 'Adam'
I0930 04:00:58.869887 140278725625664 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I0930 04:00:58.869941 140278725625664 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I0930 04:00:58.869992 140278725625664 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I0930 04:00:58.870040 140278725625664 learner.py:356] Learner params: optimizer.random_seed : NoneType
I0930 04:00:58.870089 140278725625664 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I0930 04:00:58.870137 140278725625664 learner.py:356] Learner params: optimizer.vn.global_vn : False
I0930 04:00:58.870186 140278725625664 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I0930 04:00:58.870234 140278725625664 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I0930 04:00:58.870283 140278725625664 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I0930 04:00:58.870332 140278725625664 learner.py:356] Learner params: params_init.method : 'xavier'
I0930 04:00:58.870380 140278725625664 learner.py:356] Learner params: params_init.scale : 1.000001
I0930 04:00:58.870429 140278725625664 learner.py:356] Learner params: params_init.seed : NoneType
I0930 04:00:58.870478 140278725625664 learner.py:356] Learner params: random_seed : NoneType
I0930 04:00:58.870527 140278725625664 learner.py:356] Learner params: skip_lp_regularization : NoneType
I0930 04:00:58.870576 140278725625664 learner.py:356] Learner params: vn.global_vn : False
I0930 04:00:58.870625 140278725625664 learner.py:356] Learner params: vn.per_step_vn : False
I0930 04:00:58.870674 140278725625664 learner.py:356] Learner params: vn.scale : NoneType
I0930 04:00:58.870724 140278725625664 learner.py:356] Learner params: vn.seed : NoneType
I0930 04:00:58.870773 140278725625664 learner.py:356] Learner params: 
I0930 04:00:59.114064 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I0930 04:00:59.116065 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.135150 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I0930 04:00:59.137084 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.139691 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I0930 04:00:59.141432 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.148436 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I0930 04:00:59.150394 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.153080 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I0930 04:00:59.154747 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.161748 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I0930 04:00:59.163758 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.166364 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I0930 04:00:59.167996 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.175049 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I0930 04:00:59.176969 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.179594 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I0930 04:00:59.181352 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.185028 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I0930 04:00:59.186692 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.190560 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I0930 04:00:59.192219 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.194930 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I0930 04:00:59.196557 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.206519 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.212784 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I0930 04:00:59.214835 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.217442 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I0930 04:00:59.219103 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.221027 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.227360 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I0930 04:00:59.229295 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.231985 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I0930 04:00:59.233658 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.238260 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I0930 04:00:59.239901 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.242648 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I0930 04:00:59.244279 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.264471 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I0930 04:00:59.266422 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.268990 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I0930 04:00:59.270775 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.277836 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I0930 04:00:59.279759 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.282470 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I0930 04:00:59.284116 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.291250 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I0930 04:00:59.293248 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.295858 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I0930 04:00:59.297511 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.305063 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I0930 04:00:59.307020 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.309690 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I0930 04:00:59.311452 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.315127 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I0930 04:00:59.316784 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.320647 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I0930 04:00:59.322323 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.325023 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I0930 04:00:59.326704 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.336227 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.342582 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I0930 04:00:59.344584 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.347221 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I0930 04:00:59.348875 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.350836 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.357187 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I0930 04:00:59.359247 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.362465 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I0930 04:00:59.364121 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.368746 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I0930 04:00:59.370422 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.373164 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I0930 04:00:59.374835 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.394710 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I0930 04:00:59.396641 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.399264 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I0930 04:00:59.401022 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.408043 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I0930 04:00:59.409997 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.413225 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I0930 04:00:59.414903 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.422076 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I0930 04:00:59.424075 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.426699 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I0930 04:00:59.428342 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.435456 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I0930 04:00:59.437396 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.440048 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I0930 04:00:59.441834 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.445545 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I0930 04:00:59.447216 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.451093 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I0930 04:00:59.452755 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.455491 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I0930 04:00:59.457149 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.466650 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.473489 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I0930 04:00:59.475524 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.478164 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I0930 04:00:59.479819 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.481790 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.488121 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I0930 04:00:59.490086 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.492794 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I0930 04:00:59.494504 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.499142 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I0930 04:00:59.500805 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.503581 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I0930 04:00:59.505239 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.525845 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I0930 04:00:59.527779 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.530385 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I0930 04:00:59.532144 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.539188 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I0930 04:00:59.541128 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.543951 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I0930 04:00:59.545629 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.552665 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I0930 04:00:59.554701 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.557292 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I0930 04:00:59.558964 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.566111 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I0930 04:00:59.568060 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.570712 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I0930 04:00:59.572470 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.576186 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I0930 04:00:59.577888 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.581810 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I0930 04:00:59.583485 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.586232 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I0930 04:00:59.587897 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.597937 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.604257 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I0930 04:00:59.606300 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.608906 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I0930 04:00:59.610642 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.612605 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.618927 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I0930 04:00:59.620882 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.623615 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I0930 04:00:59.625288 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.630002 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I0930 04:00:59.631661 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.634453 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I0930 04:00:59.636135 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.656580 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I0930 04:00:59.658554 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.661165 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I0930 04:00:59.662962 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.670025 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I0930 04:00:59.671975 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.674713 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I0930 04:00:59.676380 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.683445 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I0930 04:00:59.685466 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.688105 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I0930 04:00:59.689793 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.697368 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I0930 04:00:59.699350 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.702056 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I0930 04:00:59.703851 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.707562 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I0930 04:00:59.709254 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.713175 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I0930 04:00:59.714882 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.717624 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I0930 04:00:59.719300 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.728888 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.735280 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I0930 04:00:59.737318 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.739958 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I0930 04:00:59.741658 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.743645 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.749980 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I0930 04:00:59.751953 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.755247 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I0930 04:00:59.756933 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.761620 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I0930 04:00:59.763290 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.766072 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I0930 04:00:59.767744 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.787832 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I0930 04:00:59.789966 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.792574 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I0930 04:00:59.794387 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.801472 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I0930 04:00:59.803450 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.806730 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I0930 04:00:59.808403 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.815472 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I0930 04:00:59.817493 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.820132 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I0930 04:00:59.821834 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.828939 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I0930 04:00:59.830929 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.833582 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I0930 04:00:59.835380 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.839102 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I0930 04:00:59.840790 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.844765 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I0930 04:00:59.846467 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.849192 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I0930 04:00:59.850895 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.860499 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.867482 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I0930 04:00:59.869511 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.872170 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I0930 04:00:59.873895 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.875892 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.882378 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I0930 04:00:59.884397 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.887130 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I0930 04:00:59.888817 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.893542 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I0930 04:00:59.895231 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.898009 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I0930 04:00:59.899699 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.920257 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I0930 04:00:59.922243 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.924879 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I0930 04:00:59.926694 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.933743 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I0930 04:00:59.935706 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.938469 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I0930 04:00:59.940152 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.947241 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I0930 04:00:59.949272 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.951938 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I0930 04:00:59.953643 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.960804 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I0930 04:00:59.962790 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.965458 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I0930 04:00:59.967270 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.971026 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I0930 04:00:59.972740 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.976703 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I0930 04:00:59.978412 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:00:59.981149 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I0930 04:00:59.982878 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:00:59.993009 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:00:59.999404 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I0930 04:01:00.001457 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.004108 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I0930 04:01:00.005868 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.007877 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.014265 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I0930 04:01:00.016246 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.019025 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I0930 04:01:00.020726 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.025491 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I0930 04:01:00.027208 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.030015 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I0930 04:01:00.031699 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.052366 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I0930 04:01:00.054368 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.056980 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I0930 04:01:00.058802 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.065917 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I0930 04:01:00.067901 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.070667 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I0930 04:01:00.072368 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.079496 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I0930 04:01:00.081550 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.084196 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I0930 04:01:00.085917 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.093510 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I0930 04:01:00.095521 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.098216 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I0930 04:01:00.100015 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.103768 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I0930 04:01:00.105483 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.109507 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I0930 04:01:00.111224 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.113986 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I0930 04:01:00.115687 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.125320 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.131721 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I0930 04:01:00.133805 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.136475 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I0930 04:01:00.138210 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.140229 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.146601 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I0930 04:01:00.148631 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.151913 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I0930 04:01:00.153633 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.158380 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I0930 04:01:00.160066 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.162883 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I0930 04:01:00.164583 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.237703 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I0930 04:01:00.239735 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.242416 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I0930 04:01:00.244207 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.251276 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I0930 04:01:00.253255 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.256033 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I0930 04:01:00.257758 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.265228 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I0930 04:01:00.267291 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.270015 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I0930 04:01:00.271692 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.278801 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I0930 04:01:00.280773 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.283473 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I0930 04:01:00.285283 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.289073 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I0930 04:01:00.290807 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.294863 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I0930 04:01:00.296552 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.299337 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I0930 04:01:00.301040 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.310724 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.317069 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I0930 04:01:00.319714 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.322404 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I0930 04:01:00.324114 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.326175 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.332537 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I0930 04:01:00.334559 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.337402 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I0930 04:01:00.339142 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.343904 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I0930 04:01:00.345630 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.348419 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I0930 04:01:00.350148 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.370766 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I0930 04:01:00.372762 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.375438 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I0930 04:01:00.377247 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.384331 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I0930 04:01:00.386354 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.389106 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I0930 04:01:00.390844 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.397956 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I0930 04:01:00.400019 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.402704 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I0930 04:01:00.404416 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.411559 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I0930 04:01:00.413570 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.416250 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I0930 04:01:00.418087 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.421880 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I0930 04:01:00.423593 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.427646 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I0930 04:01:00.429335 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.432113 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I0930 04:01:00.433839 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.444081 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.450506 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I0930 04:01:00.452574 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.455270 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I0930 04:01:00.456980 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.459038 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.465410 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I0930 04:01:00.467437 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.470208 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I0930 04:01:00.471918 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.476688 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I0930 04:01:00.478419 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.481205 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I0930 04:01:00.482933 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.503684 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I0930 04:01:00.505686 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.508336 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I0930 04:01:00.510184 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.517353 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I0930 04:01:00.519374 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.522197 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I0930 04:01:00.523904 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.531034 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I0930 04:01:00.533094 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.535809 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I0930 04:01:00.537540 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.544691 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I0930 04:01:00.546709 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.549411 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I0930 04:01:00.551744 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.555583 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I0930 04:01:00.557296 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.561431 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I0930 04:01:00.563162 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.565949 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I0930 04:01:00.567661 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.577432 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.583883 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I0930 04:01:00.585985 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.588649 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I0930 04:01:00.590391 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.592442 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.598869 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I0930 04:01:00.600875 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.603659 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I0930 04:01:00.605375 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.610218 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I0930 04:01:00.611927 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.615239 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I0930 04:01:00.617297 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.637637 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I0930 04:01:00.639657 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.642344 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I0930 04:01:00.644170 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.651316 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I0930 04:01:00.653311 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.656089 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I0930 04:01:00.657831 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.665462 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I0930 04:01:00.667556 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.670264 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I0930 04:01:00.671983 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.679191 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I0930 04:01:00.681208 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.683928 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I0930 04:01:00.685771 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.689854 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I0930 04:01:00.691575 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.695706 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I0930 04:01:00.697418 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.700222 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I0930 04:01:00.701976 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.711739 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.718195 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I0930 04:01:00.720860 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.723587 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I0930 04:01:00.725309 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.727402 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.733810 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I0930 04:01:00.735824 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.738650 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I0930 04:01:00.740379 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.745208 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I0930 04:01:00.746943 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.749796 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I0930 04:01:00.751509 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.772360 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I0930 04:01:00.774416 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.777081 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I0930 04:01:00.778933 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.786069 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I0930 04:01:00.788082 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.790885 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I0930 04:01:00.792615 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.799739 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I0930 04:01:00.801838 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.804516 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I0930 04:01:00.806258 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.813455 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I0930 04:01:00.815570 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.818304 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I0930 04:01:00.820140 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.823997 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I0930 04:01:00.825758 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.829925 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I0930 04:01:00.831648 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.834451 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I0930 04:01:00.836198 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.846583 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.853020 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I0930 04:01:00.855134 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.857851 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I0930 04:01:00.859591 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.861693 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.868083 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I0930 04:01:00.870135 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.872902 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I0930 04:01:00.874665 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.879530 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I0930 04:01:00.881251 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.884137 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I0930 04:01:00.885888 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.906714 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I0930 04:01:00.908722 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.911460 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I0930 04:01:00.913297 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.920447 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I0930 04:01:00.922482 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.925263 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I0930 04:01:00.927078 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.934221 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I0930 04:01:00.936291 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.939007 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I0930 04:01:00.940735 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.947952 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I0930 04:01:00.949985 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.952692 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I0930 04:01:00.955005 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.958906 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I0930 04:01:00.960644 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.964762 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I0930 04:01:00.966513 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.969284 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I0930 04:01:00.971029 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.980794 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:00.987241 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I0930 04:01:00.989329 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:00.992079 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I0930 04:01:00.993837 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:00.995921 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.002358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I0930 04:01:01.004377 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.007184 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I0930 04:01:01.008935 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.013824 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I0930 04:01:01.015569 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.018899 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I0930 04:01:01.020652 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.041044 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I0930 04:01:01.043102 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.045807 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I0930 04:01:01.047644 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.054789 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I0930 04:01:01.056795 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.059607 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I0930 04:01:01.061348 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.068981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I0930 04:01:01.071089 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.073818 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I0930 04:01:01.075553 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.082743 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I0930 04:01:01.084766 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.087508 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I0930 04:01:01.089349 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.093205 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I0930 04:01:01.094962 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.099154 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I0930 04:01:01.100905 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.103785 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I0930 04:01:01.105544 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.115588 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.122076 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I0930 04:01:01.124760 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.127492 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I0930 04:01:01.129244 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.131388 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.137807 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I0930 04:01:01.139831 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.142668 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I0930 04:01:01.144411 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.149287 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I0930 04:01:01.151043 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.153888 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I0930 04:01:01.155620 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.176619 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I0930 04:01:01.178658 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.181374 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I0930 04:01:01.183246 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.190458 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I0930 04:01:01.192477 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.195305 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I0930 04:01:01.197044 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.204204 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I0930 04:01:01.206314 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.209014 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I0930 04:01:01.210777 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.218003 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I0930 04:01:01.220029 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.222792 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I0930 04:01:01.224635 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.228523 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I0930 04:01:01.230315 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.234505 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I0930 04:01:01.236251 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.239077 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I0930 04:01:01.240932 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.252585 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.259089 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I0930 04:01:01.261201 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.263936 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I0930 04:01:01.265722 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.267833 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.274280 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I0930 04:01:01.276334 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.279144 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I0930 04:01:01.280914 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.285836 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I0930 04:01:01.287580 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.290449 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I0930 04:01:01.292210 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.365135 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I0930 04:01:01.367357 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.370134 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I0930 04:01:01.372000 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.379192 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I0930 04:01:01.381231 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.384053 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I0930 04:01:01.385823 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.392949 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I0930 04:01:01.395071 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.397826 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I0930 04:01:01.399578 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.406783 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I0930 04:01:01.408823 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.411603 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I0930 04:01:01.413466 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.417956 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I0930 04:01:01.419718 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.423976 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I0930 04:01:01.425746 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.428568 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I0930 04:01:01.430359 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.440419 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.446972 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I0930 04:01:01.449109 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.451857 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I0930 04:01:01.453641 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.455790 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.462270 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I0930 04:01:01.464333 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.467181 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I0930 04:01:01.468944 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.474297 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I0930 04:01:01.476270 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.479469 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I0930 04:01:01.481412 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.502717 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I0930 04:01:01.504832 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.507577 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I0930 04:01:01.509439 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.516719 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I0930 04:01:01.518788 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.521639 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I0930 04:01:01.523396 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.530656 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I0930 04:01:01.533310 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.536056 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I0930 04:01:01.537832 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.545082 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I0930 04:01:01.547177 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.549972 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I0930 04:01:01.551834 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.555860 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I0930 04:01:01.557630 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.561968 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I0930 04:01:01.563730 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.566587 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I0930 04:01:01.568376 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.578686 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.585656 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I0930 04:01:01.587848 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.590628 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I0930 04:01:01.592390 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.595144 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.601635 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I0930 04:01:01.603706 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.606580 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I0930 04:01:01.608357 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.613356 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I0930 04:01:01.615140 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.618027 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I0930 04:01:01.619779 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.640551 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I0930 04:01:01.642958 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.646107 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I0930 04:01:01.648886 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.656368 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I0930 04:01:01.658518 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.661350 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I0930 04:01:01.663151 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.670367 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I0930 04:01:01.672482 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.675251 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I0930 04:01:01.677041 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.684340 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I0930 04:01:01.686465 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.689273 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I0930 04:01:01.691169 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.695214 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I0930 04:01:01.696984 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.701305 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I0930 04:01:01.703097 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.705971 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I0930 04:01:01.707732 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.718458 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.725043 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I0930 04:01:01.727244 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.730048 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I0930 04:01:01.731826 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.734073 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.740587 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I0930 04:01:01.742712 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.745644 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I0930 04:01:01.747505 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.752873 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I0930 04:01:01.754677 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.757591 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I0930 04:01:01.759359 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.780875 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I0930 04:01:01.783015 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.785782 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I0930 04:01:01.787657 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.795421 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I0930 04:01:01.797753 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.800919 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I0930 04:01:01.802756 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.809984 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I0930 04:01:01.812119 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.814886 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I0930 04:01:01.816649 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.823925 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I0930 04:01:01.826086 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.828865 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I0930 04:01:01.830774 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.835348 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I0930 04:01:01.837133 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.841461 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I0930 04:01:01.843260 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.846130 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I0930 04:01:01.847913 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.858145 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.864961 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I0930 04:01:01.867212 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.870059 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I0930 04:01:01.871847 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.874137 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:01.880697 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I0930 04:01:01.882848 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.885763 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I0930 04:01:01.887561 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.892681 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I0930 04:01:01.894483 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.897365 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I0930 04:01:01.899170 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.920724 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I0930 04:01:01.922893 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.925714 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I0930 04:01:01.927593 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.934777 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I0930 04:01:01.936831 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.939709 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I0930 04:01:01.941481 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.949094 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I0930 04:01:01.952145 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.955269 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I0930 04:01:01.957218 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.964501 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I0930 04:01:01.966607 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.969409 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I0930 04:01:01.971305 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.975358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I0930 04:01:01.977144 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.981477 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I0930 04:01:01.983269 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:01.986154 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I0930 04:01:01.987938 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:01.998078 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.004626 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I0930 04:01:02.006798 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.009589 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I0930 04:01:02.011372 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.014219 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.020836 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I0930 04:01:02.022998 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.025933 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I0930 04:01:02.027747 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.032833 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I0930 04:01:02.034638 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.037558 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I0930 04:01:02.039344 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.060136 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I0930 04:01:02.062228 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.064951 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I0930 04:01:02.067388 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.074585 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I0930 04:01:02.076661 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.079532 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I0930 04:01:02.081327 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.088532 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I0930 04:01:02.090691 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.093439 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I0930 04:01:02.095247 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.102494 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I0930 04:01:02.104558 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.107364 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I0930 04:01:02.109231 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.113186 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I0930 04:01:02.115003 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.119355 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I0930 04:01:02.121130 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.123996 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I0930 04:01:02.125813 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.136405 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.142886 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I0930 04:01:02.145030 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.147862 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I0930 04:01:02.149669 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.151871 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.158345 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I0930 04:01:02.160430 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.163306 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I0930 04:01:02.165106 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.170174 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I0930 04:01:02.171941 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.174847 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I0930 04:01:02.176629 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.197840 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I0930 04:01:02.199920 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.202712 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I0930 04:01:02.204592 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.211801 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I0930 04:01:02.213907 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.216765 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I0930 04:01:02.218577 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.225780 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I0930 04:01:02.227910 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.230695 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I0930 04:01:02.232484 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.239754 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I0930 04:01:02.241861 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.244644 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I0930 04:01:02.246573 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.251037 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I0930 04:01:02.252847 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.257207 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I0930 04:01:02.259021 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.261917 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I0930 04:01:02.263757 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.273841 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.280360 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I0930 04:01:02.282540 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.285302 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I0930 04:01:02.287133 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.289330 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.295832 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I0930 04:01:02.297955 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.300815 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I0930 04:01:02.302630 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.307660 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I0930 04:01:02.309444 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.312358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I0930 04:01:02.314155 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.335385 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I0930 04:01:02.337466 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.340256 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I0930 04:01:02.342193 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.349366 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I0930 04:01:02.351471 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.354348 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I0930 04:01:02.356132 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.363397 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I0930 04:01:02.366051 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.368823 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I0930 04:01:02.370648 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.378005 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I0930 04:01:02.380094 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.382921 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I0930 04:01:02.384825 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.388819 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I0930 04:01:02.390708 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.395046 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I0930 04:01:02.396837 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.399729 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I0930 04:01:02.401556 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.411610 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.418144 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I0930 04:01:02.420317 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.423119 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I0930 04:01:02.424930 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.427793 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.434307 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I0930 04:01:02.436397 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.440155 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I0930 04:01:02.441982 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.447085 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I0930 04:01:02.448937 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.451869 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I0930 04:01:02.453689 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.528285 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I0930 04:01:02.530567 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.533485 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I0930 04:01:02.535309 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.542918 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I0930 04:01:02.545061 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.547865 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I0930 04:01:02.549742 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.557165 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I0930 04:01:02.559366 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.562202 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I0930 04:01:02.564120 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.571366 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I0930 04:01:02.573459 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.576368 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I0930 04:01:02.578202 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.582256 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I0930 04:01:02.584056 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.588509 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I0930 04:01:02.590345 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.593206 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I0930 04:01:02.595028 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.605833 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.612342 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I0930 04:01:02.614535 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.617295 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I0930 04:01:02.619147 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.621375 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.627869 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I0930 04:01:02.629994 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.632853 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I0930 04:01:02.634689 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.639775 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I0930 04:01:02.641596 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.644508 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I0930 04:01:02.646346 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.667613 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I0930 04:01:02.669758 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.672546 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I0930 04:01:02.674487 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.681687 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I0930 04:01:02.683813 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.686712 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I0930 04:01:02.688512 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.695722 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I0930 04:01:02.697884 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.700664 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I0930 04:01:02.702496 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.709772 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I0930 04:01:02.711875 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.714710 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I0930 04:01:02.716632 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.720781 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I0930 04:01:02.722619 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.727036 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I0930 04:01:02.728852 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.732224 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I0930 04:01:02.734068 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.744328 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.750855 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I0930 04:01:02.753029 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.756021 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I0930 04:01:02.757857 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.760111 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.766585 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I0930 04:01:02.768668 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.771556 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I0930 04:01:02.773383 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.778504 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I0930 04:01:02.780321 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.783257 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I0930 04:01:02.785069 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.806378 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I0930 04:01:02.808484 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.811298 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I0930 04:01:02.813235 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.820462 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I0930 04:01:02.822576 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.825462 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I0930 04:01:02.827316 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.834546 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I0930 04:01:02.836704 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.839542 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I0930 04:01:02.841379 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.849056 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I0930 04:01:02.851199 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.854062 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I0930 04:01:02.855973 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.859978 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I0930 04:01:02.861827 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.866219 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I0930 04:01:02.868030 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.870939 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I0930 04:01:02.872746 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.882888 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.889433 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I0930 04:01:02.891615 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.894436 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I0930 04:01:02.896249 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:02.898539 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:02.905585 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I0930 04:01:02.907701 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.910602 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I0930 04:01:02.912419 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.917556 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I0930 04:01:02.919379 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.922343 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I0930 04:01:02.924158 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.944941 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I0930 04:01:02.947078 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.949915 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I0930 04:01:02.951831 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.959617 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I0930 04:01:02.961771 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.964768 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I0930 04:01:02.966604 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.973834 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I0930 04:01:02.975998 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.978809 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I0930 04:01:02.980617 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.987908 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I0930 04:01:02.990024 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.992866 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I0930 04:01:02.994817 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:02.998867 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I0930 04:01:03.000687 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.005125 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I0930 04:01:03.006967 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.009894 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I0930 04:01:03.011712 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:03.022549 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:03.029125 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I0930 04:01:03.031394 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.034272 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I0930 04:01:03.036091 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:03.038440 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:03.044985 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I0930 04:01:03.047120 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.050044 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I0930 04:01:03.051876 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.057071 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I0930 04:01:03.058915 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.061884 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I0930 04:01:03.063704 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.085240 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I0930 04:01:03.087399 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.090215 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I0930 04:01:03.092167 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.099399 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I0930 04:01:03.101548 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.104436 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I0930 04:01:03.106291 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.113509 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I0930 04:01:03.115707 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.118597 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I0930 04:01:03.120421 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.127732 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I0930 04:01:03.129855 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.132706 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I0930 04:01:03.134660 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.138721 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I0930 04:01:03.140572 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.145040 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I0930 04:01:03.146887 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.150273 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I0930 04:01:03.152101 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:03.162378 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:03.168899 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I0930 04:01:03.171109 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.173945 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I0930 04:01:03.175780 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:03.178076 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:03.184587 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I0930 04:01:03.186738 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.189682 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I0930 04:01:03.191525 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.196771 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I0930 04:01:03.198627 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.201596 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I0930 04:01:03.203427 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.224927 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I0930 04:01:03.227065 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.229879 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I0930 04:01:03.231812 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.239100 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I0930 04:01:03.241219 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.244142 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I0930 04:01:03.246002 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.253276 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I0930 04:01:03.255491 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.258336 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I0930 04:01:03.260170 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.267995 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I0930 04:01:03.270145 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.272981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I0930 04:01:03.274921 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.279049 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I0930 04:01:03.280898 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.285393 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I0930 04:01:03.287252 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.290175 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I0930 04:01:03.292007 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:03.302287 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:03.308844 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I0930 04:01:03.311062 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.313970 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I0930 04:01:03.315860 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:03.318407 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:03.326018 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I0930 04:01:03.328242 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.331208 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I0930 04:01:03.333059 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.338316 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I0930 04:01:03.340149 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.343122 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I0930 04:01:03.344959 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.366078 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I0930 04:01:03.368206 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.371057 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I0930 04:01:03.372985 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.380777 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I0930 04:01:03.382923 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.385852 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I0930 04:01:03.387688 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.394971 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I0930 04:01:03.397152 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.399981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I0930 04:01:03.401845 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.409190 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I0930 04:01:03.411356 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.414234 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I0930 04:01:03.416172 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.420273 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I0930 04:01:03.422135 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.426688 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I0930 04:01:03.428516 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:03.431438 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I0930 04:01:03.433294 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:03.444102 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.322493 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I0930 04:01:04.325040 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.328097 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I0930 04:01:04.329970 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.332420 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.339020 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I0930 04:01:04.341175 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.344132 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I0930 04:01:04.346056 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.351393 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I0930 04:01:04.353238 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.356258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I0930 04:01:04.358131 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.381032 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I0930 04:01:04.383371 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.386415 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I0930 04:01:04.388285 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.395704 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I0930 04:01:04.397951 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.400800 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I0930 04:01:04.402701 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.410211 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I0930 04:01:04.412363 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.415232 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I0930 04:01:04.417195 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.424517 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I0930 04:01:04.426697 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.429713 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I0930 04:01:04.431589 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.435776 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I0930 04:01:04.437658 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.442293 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I0930 04:01:04.444160 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.447166 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I0930 04:01:04.449043 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.460358 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.467017 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I0930 04:01:04.469244 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.472132 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I0930 04:01:04.474042 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.476382 140278725625664 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.482961 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I0930 04:01:04.485114 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.488056 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I0930 04:01:04.489968 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.495277 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I0930 04:01:04.497148 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.500155 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I0930 04:01:04.502030 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.507607 140278725625664 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.514330 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I0930 04:01:04.516474 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.517347 140278725625664 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.524471 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I0930 04:01:04.526665 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.527530 140278725625664 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.534056 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I0930 04:01:04.536198 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.537061 140278725625664 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.543606 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I0930 04:01:04.545764 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.546632 140278725625664 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.553121 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I0930 04:01:04.555298 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.556167 140278725625664 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.562681 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I0930 04:01:04.564816 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.565701 140278725625664 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.572155 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I0930 04:01:04.574310 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.575186 140278725625664 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.581712 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I0930 04:01:04.583849 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.584720 140278725625664 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.591170 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I0930 04:01:04.593307 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.594189 140278725625664 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.601310 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I0930 04:01:04.603468 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.604326 140278725625664 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.610758 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I0930 04:01:04.612892 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.613773 140278725625664 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.620257 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I0930 04:01:04.622398 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.623268 140278725625664 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.629735 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I0930 04:01:04.631847 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.632703 140278725625664 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.639200 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I0930 04:01:04.641318 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.642208 140278725625664 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.648602 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I0930 04:01:04.650745 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 04:01:04.651606 140278725625664 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I0930 04:01:04.658094 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I0930 04:01:04.660227 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.663124 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I0930 04:01:04.665063 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.667903 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I0930 04:01:04.669774 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.672698 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I0930 04:01:04.674563 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.677371 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I0930 04:01:04.679236 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.682658 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I0930 04:01:04.684503 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.687350 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I0930 04:01:04.689200 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.692124 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I0930 04:01:04.693971 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.696796 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I0930 04:01:04.698777 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.701618 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I0930 04:01:04.703479 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.706422 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I0930 04:01:04.708262 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.711113 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I0930 04:01:04.712955 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.715883 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I0930 04:01:04.717746 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.720548 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I0930 04:01:04.722457 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.725375 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I0930 04:01:04.727235 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.730071 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I0930 04:01:04.732005 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:04.734878 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I0930 04:01:04.736723 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:05.412843 140278725625664 py_utils.py:1478] === worker 0 ===
I0930 04:01:05.427742 140278725625664 py_utils.py:1468] worker 0: global_step                                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.427833 140278725625664 py_utils.py:1468] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.427892 140278725625664 py_utils.py:1468] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.427945 140278725625664 py_utils.py:1468] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.427993 140278725625664 py_utils.py:1468] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428040 140278725625664 py_utils.py:1468] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428094 140278725625664 py_utils.py:1468] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428142 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428187 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428233 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428278 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428323 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428368 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428414 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428459 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428503 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428548 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428594 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428638 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428684 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428730 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428775 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428821 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428866 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428911 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.428961 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429006 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429052 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429097 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429141 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429186 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429231 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429276 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429320 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429364 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429409 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429453 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429498 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429568 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429617 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429662 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429707 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429752 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429801 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429847 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429892 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429937 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.429982 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430027 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430073 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430118 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430163 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430208 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430253 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430298 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430343 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430388 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430433 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430478 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430523 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430569 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430613 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430662 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430708 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430754 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430799 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430844 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430889 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430934 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.430979 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431024 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431069 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431115 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431160 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431205 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431251 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431296 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431341 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431387 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431432 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431481 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431527 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431572 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431618 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431663 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431708 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431753 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431798 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431843 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431888 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431934 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.431979 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432024 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432070 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432116 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432161 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432206 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432251 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432296 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432343 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432389 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432434 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432479 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432524 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432568 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432613 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432658 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432703 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432752 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432798 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432844 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432888 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432933 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.432978 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433023 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433068 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433113 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433158 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433207 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433253 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433298 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433343 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433388 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433434 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433480 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433538 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433590 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433637 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433682 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433727 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433772 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433817 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433863 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433908 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433953 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.433998 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434049 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434095 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434140 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434185 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434230 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434275 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434320 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434365 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434411 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434456 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434501 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434546 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434592 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434637 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434682 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434727 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434772 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434818 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434863 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434911 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.434957 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435001 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435046 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435091 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435136 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435181 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435225 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435270 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435315 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435359 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435404 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435449 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435494 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435539 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435585 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435630 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435675 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435723 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435768 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435814 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435859 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435904 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435949 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.435994 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436039 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436085 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436130 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436176 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436221 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436266 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436311 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436356 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436402 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436447 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436492 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436537 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436585 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436631 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436676 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436722 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436767 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436812 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436857 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436902 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436948 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.436993 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437037 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437083 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437128 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437173 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437218 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437263 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437309 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437354 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437402 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437448 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437493 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437553 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437602 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437649 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437695 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437740 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437785 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437830 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437875 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437921 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.437967 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438012 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438057 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438102 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438147 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438192 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438238 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438286 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438332 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438377 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438422 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438467 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438512 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438557 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438602 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438647 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438692 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438737 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438782 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438827 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438872 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438917 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.438962 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439007 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439052 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439100 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439146 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439192 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439237 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439282 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439327 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439372 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439417 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439462 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439507 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439552 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439597 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439642 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439687 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439732 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439776 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439821 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439866 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439911 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.439960 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440005 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440050 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440095 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440140 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440185 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440231 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440276 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440321 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440366 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440411 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440456 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440501 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440546 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440590 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440635 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440680 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440725 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440773 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440819 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440864 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440909 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440954 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.440999 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441045 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441090 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441135 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441180 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441225 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441269 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441314 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441359 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441404 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441448 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441494 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441557 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441606 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441655 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441701 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441746 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441792 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441837 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441883 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441927 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.441973 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442017 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442062 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442108 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442153 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442198 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442243 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442288 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442333 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442378 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442424 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442472 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442518 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442563 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442608 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442653 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442698 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442743 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442791 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442837 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442882 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442927 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.442973 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443017 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443062 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443106 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443151 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443196 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443241 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443286 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443335 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443381 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443426 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443471 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443516 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443560 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443604 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443649 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443694 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443739 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443784 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443829 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443874 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443919 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.443964 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444009 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444054 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444099 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444146 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444192 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444237 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444282 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444327 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444371 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444416 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444461 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444506 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444551 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444596 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444640 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444685 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444730 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444774 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444819 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444863 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444908 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.444953 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445001 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445046 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445091 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445136 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445180 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445225 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445270 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445314 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445358 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445403 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445448 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445492 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445552 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445601 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445646 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445691 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445736 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445780 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445825 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445873 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445920 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.445964 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446009 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446053 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446098 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446143 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446187 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446232 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446277 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446321 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446365 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446410 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446454 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446499 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446543 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446588 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446633 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446681 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446726 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446771 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446816 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446861 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446905 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446949 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.446994 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447039 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447083 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447128 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447172 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447217 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447262 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447306 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447351 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447396 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447441 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447486 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447535 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447581 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447626 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447670 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447716 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447761 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447805 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447850 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447895 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447940 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.447985 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448031 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448076 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448120 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448165 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448209 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448254 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448299 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448347 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448393 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448438 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448483 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448528 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448572 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448617 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448662 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448707 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448752 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448797 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448842 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448887 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448932 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.448977 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449022 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449067 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449112 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449157 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449205 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449252 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449296 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449341 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449386 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449431 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449476 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449531 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449583 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449629 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449674 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449719 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449764 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449808 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449853 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449898 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449943 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.449987 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450036 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450081 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450125 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450170 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450216 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450260 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450305 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450350 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450395 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450440 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450485 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450530 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450575 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450620 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450665 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450710 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450755 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450799 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450844 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450892 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450938 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.450982 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451027 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451072 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451117 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451162 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451206 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451251 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451296 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451339 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451384 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451428 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451473 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451518 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451563 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451608 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451653 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451702 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451748 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451792 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451837 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451881 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451926 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.451971 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452016 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452060 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452106 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452151 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452195 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452240 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452284 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452329 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452374 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452419 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452465 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452509 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452558 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452604 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452649 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452694 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452739 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452784 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452831 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452878 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452923 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.452969 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453014 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453059 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453104 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453149 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453195 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453239 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453284 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453329 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453383 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453431 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453476 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453536 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453589 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453635 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453680 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453725 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453770 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453814 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453859 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453904 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453949 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.453994 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454039 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454084 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454128 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454174 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454218 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454269 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454314 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454359 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454404 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454449 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454494 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454539 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454584 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454629 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454674 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454719 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454763 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454808 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454853 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454898 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454942 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.454987 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455031 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455080 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455126 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455171 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455215 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455260 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455306 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455351 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455396 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455441 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455486 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455531 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455576 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455621 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455666 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455710 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455756 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455800 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455845 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455890 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455938 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.455984 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456029 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456073 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456118 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456163 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456207 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456252 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456297 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456341 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456386 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456430 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456475 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456519 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456564 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456609 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456653 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456697 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456746 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456791 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456835 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456880 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456924 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.456968 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457012 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457057 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457102 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457146 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457191 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457237 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457282 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457327 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457371 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457416 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457462 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457507 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457568 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457622 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457668 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457713 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457758 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457802 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457847 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457892 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457937 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.457982 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458027 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458072 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458117 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458162 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458207 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458251 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458296 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458340 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458385 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458430 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458479 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458524 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458570 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458614 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458659 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458703 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458748 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458793 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458838 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458883 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458928 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.458973 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459017 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459062 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459106 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459151 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459195 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459240 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459289 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459335 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459379 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459424 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459468 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459513 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459558 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459603 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459648 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459693 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459738 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459783 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459828 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459874 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459919 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.459964 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460009 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460054 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460100 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460150 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460196 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460241 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460287 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460332 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460377 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460422 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460468 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460513 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460558 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460603 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460649 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460694 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460739 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460784 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460829 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460874 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460919 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.460969 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461015 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461060 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461105 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461150 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461196 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461241 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461286 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461331 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461376 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461421 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461466 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461511 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461573 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461619 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461664 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461709 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461755 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461800 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461851 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461897 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461942 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.461988 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462033 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462078 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462123 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462169 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462214 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462259 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462305 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462350 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462395 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462439 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462484 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462529 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462574 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462618 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462667 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462713 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462758 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462803 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462848 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462896 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462942 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.462987 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463032 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463076 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463121 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463166 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463211 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463256 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463300 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463346 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463391 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463435 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463480 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463530 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463576 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463621 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463666 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463711 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463755 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463800 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463845 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463890 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463934 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.463979 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464024 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464068 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464113 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464158 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464203 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464248 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464293 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464342 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464388 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464433 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464478 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464523 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464568 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464613 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464658 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464702 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464746 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464791 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464835 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464880 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464924 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.464969 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465014 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465059 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465103 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465148 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465197 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465243 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465288 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465333 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465377 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465422 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465466 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465511 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465576 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465623 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465668 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465713 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465758 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465803 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465848 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465893 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465938 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.465983 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466033 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466079 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466124 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466169 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466214 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466260 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466305 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466350 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466394 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466439 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466484 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466529 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466574 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466619 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466663 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466708 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466753 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466798 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466843 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466892 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466938 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.466984 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467029 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467074 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467119 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467164 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467227 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467282 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467328 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467374 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467419 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467463 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467508 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467553 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467598 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467643 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467688 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467738 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467784 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467830 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467875 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467920 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.467964 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468010 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468055 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468100 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468144 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468189 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468234 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468279 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468324 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468368 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468413 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468457 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468502 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468547 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468596 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468642 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468687 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468731 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468777 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468822 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468867 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468912 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.468957 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469002 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469047 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469092 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469137 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469182 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469228 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469273 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469318 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469363 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469413 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469459 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469504 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469567 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469615 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469660 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469706 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469751 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469796 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469841 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469886 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469931 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.469976 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470022 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470067 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470111 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470157 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470202 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470247 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470297 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470343 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470388 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470433 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470478 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470523 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470568 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470613 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470657 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470702 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470747 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470792 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470836 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470880 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470925 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.470969 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471014 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471058 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471103 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471153 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471198 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471244 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471289 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471333 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471379 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471424 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471469 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471513 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471558 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471602 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471647 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471691 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471735 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471780 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471825 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471869 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471913 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.471962 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472008 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472053 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472097 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472141 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472186 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472231 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472276 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472320 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472365 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472410 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472454 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472499 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472544 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472589 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472634 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472679 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472723 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472768 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472816 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472862 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472907 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.472955 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473001 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473045 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473089 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473134 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473178 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473222 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473266 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473311 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473356 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473401 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473445 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473490 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473547 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473597 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473648 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473694 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473740 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473785 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473830 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473875 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473920 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.473965 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474010 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474056 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474100 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474145 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474190 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474235 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474281 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474326 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474370 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474415 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474460 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474510 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474556 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474601 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474646 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474692 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474737 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474782 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474827 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474872 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474917 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.474962 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.475007 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.475052 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.475097 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.475142 140278725625664 py_utils.py:1468] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.475188 140278725625664 py_utils.py:1468] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 04:01:05.475252 140278725625664 py_utils.py:1484] ==========
I0930 04:01:07.754900 140278725625664 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 04:01:09.956239 140278725625664 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:01:12.108181 140278725625664 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:01:14.256472 140278725625664 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:01:16.546580 140278725625664 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W0930 04:01:19.455993 140278725625664 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 04:01:19.876704 140278725625664 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 04:01:21.738255 140278725625664 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 04:01:22.186076 140278725625664 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 04:01:24.051427 140278725625664 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 04:01:24.539441 140278725625664 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 04:01:26.423098 140278725625664 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 04:01:27.308597 140278725625664 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 04:01:29.867511 140278725625664 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:01:32.381459 140278725625664 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:01:36.590813 140278725625664 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:01:38.638974 140278725625664 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I0930 04:01:38.643408 140278725625664 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I0930 04:01:38.734317 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I0930 04:01:38.736181 140278725625664 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:01:38.744049 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I0930 04:01:38.744144 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.744212 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.744271 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.744325 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.744377 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.744428 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.744480 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.744532 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.744583 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.744635 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.744685 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.744735 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.744797 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.744849 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.744899 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.744950 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.744999 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.745049 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.745098 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.745148 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.745196 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.745245 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.745295 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.745344 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.745394 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.745444 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.745494 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.745564 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.745618 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.745668 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.745718 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.745769 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.745818 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.745867 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.745917 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.745966 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.746016 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.746070 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.746121 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.746170 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.746220 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.746269 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.746319 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.746369 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.746418 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.746468 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.746518 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.746567 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.746617 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.746667 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.746716 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.746766 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.746816 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.746865 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.746915 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.746964 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.747014 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.747064 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.747114 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.747165 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.747215 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.747269 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.747320 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.747369 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.747418 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.747469 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.747519 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.747568 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.747618 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.747667 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.747717 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.747766 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.747815 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.747864 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.747914 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.747964 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.748014 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.748064 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.748113 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.748162 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.748210 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.748259 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.748308 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.748358 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.748407 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.748456 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.748510 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.748560 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.748609 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.748658 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.748707 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.748757 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.748806 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.748856 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.748905 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.748954 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.749003 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.749052 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.749101 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.749150 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.749199 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.749248 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.749297 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.749346 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.749395 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.749445 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.749494 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.749560 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.749614 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.749665 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.749714 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.749769 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.749819 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.749869 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.749918 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.749967 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.750016 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.750066 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.750115 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.750164 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.750213 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.750262 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.750312 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.750361 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.750410 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.750459 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.750509 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.750558 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.750612 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.750663 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.750712 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.750761 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.750811 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.750859 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.750909 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.750962 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.751012 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.751062 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.751111 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.751160 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.751209 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.751258 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.751307 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.751358 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.751408 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.751458 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.751507 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.751557 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.751606 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.751656 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.751705 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.751754 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.751804 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.751854 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.751903 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.751951 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.752001 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.752050 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.752099 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.752148 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.752202 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.752253 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.752302 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.752352 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.752402 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.752452 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.752501 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.752551 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.752600 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.752650 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.752699 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.752748 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.752797 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.752846 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.752895 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.752943 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.752992 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.753042 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.753091 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.753140 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.753190 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.753239 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.753288 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.753337 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.753390 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.753440 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.753489 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.753556 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.753611 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.753661 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.753710 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.753762 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.753811 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.753860 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.753910 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.753959 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.754009 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.754058 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.754107 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.754157 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.754206 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.754257 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.754306 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.754355 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.754404 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.754453 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.754503 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.754552 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.754606 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.754657 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.754705 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.754755 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.754804 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.754853 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.754903 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.754951 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.755001 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.755050 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.755099 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.755147 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.755196 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.755245 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.755294 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.755343 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.755391 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.755440 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.755489 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.755538 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.755588 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.755636 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.755686 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.755735 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.755784 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.755840 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.755891 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.755940 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.755989 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.756038 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.756087 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.756136 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.756185 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.756234 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.756283 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.756332 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.756381 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.756431 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.756480 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.756529 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.756578 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.756628 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.756677 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.756726 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.756777 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.756826 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.756875 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.756925 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.756974 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.757027 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.757078 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.757127 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.757176 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.757225 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.757275 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.757324 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.757374 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.757423 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.757472 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.757533 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.757590 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.757640 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.757690 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.757739 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.757788 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.757838 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.757887 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.757936 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.757986 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.758035 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.758085 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.758135 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.758185 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.758234 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.758289 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.758340 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.758390 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.758439 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.758488 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.758538 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.758587 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.758636 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.758686 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.758734 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.758783 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.758831 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.758880 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.758929 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.758978 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.759027 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.759077 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.759125 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.759174 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.759222 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.759272 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.759320 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.759370 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.759418 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.759470 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.759521 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.759570 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.759619 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.759667 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.759716 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.759764 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.759814 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.759863 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.759912 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.759961 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.760010 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.760059 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.760108 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.760158 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.760207 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.760256 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.760304 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.760353 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.760402 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.760451 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.760500 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.760549 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.760598 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.760647 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.760701 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.760752 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.760802 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.760851 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.760901 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.760951 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.760999 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.761049 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.761097 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.761146 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.761194 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.761243 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.761291 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.761340 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.761389 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.761437 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.761486 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.761549 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.761604 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.761654 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.761704 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.761753 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.761802 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.761850 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.761904 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.761953 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.762002 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.762052 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.762102 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.762151 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.762199 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.762248 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.762297 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.762346 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.762395 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.762445 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.762495 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.762544 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.762593 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.762643 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.762692 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.762742 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.762791 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.762840 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.762888 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.762938 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.762988 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.763037 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.763086 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.763144 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.763195 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.763245 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.763295 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.763345 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.763395 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.763446 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.763495 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.763544 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.763593 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.763642 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.763690 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.763739 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.763792 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.763842 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.763891 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.763940 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.763989 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.764039 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.764087 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.764136 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.764186 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.764235 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.764284 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.764337 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.764387 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.764436 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.764484 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.764533 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.764582 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.764631 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.764680 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.764728 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.764777 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.764825 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.764875 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.764924 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.764974 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.765023 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.765071 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.765120 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.765169 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.765218 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.765267 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.765315 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.765364 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.765413 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.765461 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.765509 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.765585 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.765637 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.765687 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.765737 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.765787 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.765836 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.765886 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.765935 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.765985 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.766034 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.766084 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.766133 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.766182 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.766231 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.766280 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.766330 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.766379 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.766428 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.766477 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.766527 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.766576 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.766626 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.766675 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.766724 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.766777 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.766827 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.766876 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.766925 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.766974 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.767023 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.767072 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.767122 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.767171 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.767220 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.767269 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.767319 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.767368 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.767418 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.767466 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.767515 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.767564 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.767613 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.767662 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.767711 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.767760 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.767808 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.767856 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.767905 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.767958 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.768008 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.768057 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.768106 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.768155 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.768204 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.768254 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.768303 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.768352 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.768402 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.768451 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.768501 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.768550 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.768600 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.768650 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.768699 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.768748 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.768798 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.768847 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.768897 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.768946 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.768996 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.769045 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.769095 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.769145 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.769198 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.769249 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.769298 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.769347 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.769397 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.769446 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.769495 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.769561 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.769614 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.769664 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.769714 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.769763 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.769812 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.769862 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.769912 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.769961 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.770011 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.770060 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.770109 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.770159 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.770208 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.770257 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.770306 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.770355 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.770410 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.770461 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 04:01:38.770511 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 04:01:38.770561 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 04:01:38.770612 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 04:01:38.770661 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I0930 04:01:38.770710 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I0930 04:01:38.770761 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 04:01:38.770810 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 04:01:38.770860 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 04:01:38.770910 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 04:01:38.770960 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 04:01:38.771008 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 04:01:38.771059 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 04:01:38.771108 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 04:01:38.771157 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 04:01:38.771207 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I0930 04:01:38.771256 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I0930 04:01:38.771306 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I0930 04:01:38.771356 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I0930 04:01:38.771406 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I0930 04:01:38.771455 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I0930 04:01:38.771504 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I0930 04:01:38.771553 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I0930 04:01:38.771602 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I0930 04:01:38.771651 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I0930 04:01:38.771705 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I0930 04:01:38.771755 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I0930 04:01:38.771804 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I0930 04:01:38.771854 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I0930 04:01:38.771903 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I0930 04:01:38.771953 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I0930 04:01:38.772003 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I0930 04:01:38.772052 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I0930 04:01:38.772101 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I0930 04:01:38.772150 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I0930 04:01:38.772200 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I0930 04:01:38.772249 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I0930 04:01:38.772299 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I0930 04:01:38.772348 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I0930 04:01:38.772398 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I0930 04:01:38.772448 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I0930 04:01:38.772497 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I0930 04:01:38.772546 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I0930 04:01:38.772596 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I0930 04:01:38.772645 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I0930 04:01:38.772695 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I0930 04:01:38.772745 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I0930 04:01:38.772794 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I0930 04:01:38.772843 140278725625664 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I0930 04:01:43.321975 140278725625664 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:01:50.244201 140278725625664 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:01:55.220506 140278725625664 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 04:02:02.029864 140278725625664 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 04:02:12.046152 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I0930 04:02:12.046431 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.046516 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.046593 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.046658 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.046725 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.046787 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.046848 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.046910 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.046975 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.047035 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.047099 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.047159 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.047222 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.047281 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.047344 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.047409 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.047468 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.047528 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.047587 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.047648 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.047708 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.047770 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.047836 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.047896 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.047955 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.048018 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.048077 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.048139 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.048199 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.048266 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.048326 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.048388 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.048447 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.048506 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.048565 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.048624 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.048686 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.048746 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.048808 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.048867 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.048926 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.048986 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.049048 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.049108 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.049175 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.049235 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.049298 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.049356 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.049418 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.049477 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.049553 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.049618 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.049679 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.049741 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.049801 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.049862 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.049921 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.049981 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.050039 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.050106 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.050166 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.050228 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.050287 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.050350 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.050409 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.050470 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.050529 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.050587 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.050644 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.050703 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.050765 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.050824 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.050886 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.050946 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.051010 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.051069 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.051132 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.051191 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.051253 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.051312 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.051374 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.051433 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.051495 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.051553 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.051613 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.051671 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.051729 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.051790 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.051855 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.051919 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.051978 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.052037 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.052096 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.052160 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.052219 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.052281 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.052340 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.052402 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.052460 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.052520 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.052580 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.052639 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.052698 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.052762 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.052825 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.052884 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.052945 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.053003 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.053061 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.053119 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.053182 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.053241 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.053302 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.053360 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.053422 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.053480 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.053562 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.053625 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.053689 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.053748 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.053806 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.053868 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.053927 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.053989 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.054048 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.054105 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.054164 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.054226 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.054285 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.054351 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.054409 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.054471 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.054533 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.054596 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.054655 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.054712 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.054769 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.054828 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.054889 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.054948 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.055009 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.055068 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.055125 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.055183 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.055245 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.055304 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.055365 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.055429 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.055491 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.055550 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.055610 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.055668 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.055726 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.055783 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.055842 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.055903 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.055962 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.056023 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.056082 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.056139 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.056198 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.056260 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.056324 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.056388 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.056447 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.056508 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.056567 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.056628 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.056687 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.056746 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.056804 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.056863 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.056924 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.056982 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.057044 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.057104 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.057162 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.057229 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.057292 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.057352 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.057413 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.057472 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.057548 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.057612 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.057675 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.057733 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.057792 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.057850 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.057908 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.057969 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.058028 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.058094 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.058154 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.058211 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.058270 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.058332 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.058391 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.058454 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.058512 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.058574 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.058632 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.058694 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.058753 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.058811 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.058869 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.058927 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.058992 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.059052 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.059113 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.059173 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.059231 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.059289 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.059352 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.059411 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.059472 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.059531 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.059593 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.059652 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.059713 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.059772 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.059835 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.059894 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.059953 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.060014 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.060073 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.060133 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.060192 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.060249 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.060307 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.060368 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.060426 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.060487 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.060545 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.060606 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.060664 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.060729 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.060787 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.060846 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.060904 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.060962 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.061023 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.061082 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.061142 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.061201 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.061258 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.061317 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.061378 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.061438 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.061499 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.061576 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.061645 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.061705 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.061767 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.061830 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.061888 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.061945 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.062004 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.062066 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.062125 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.062187 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.062246 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.062303 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.062360 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.062422 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.062480 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.062546 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.062606 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.062668 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.062726 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.062787 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.062845 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.062903 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.062961 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.063019 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.063080 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.063139 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.063200 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.063259 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.063317 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.063379 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.063443 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.063502 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.063564 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.063623 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.063684 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.063743 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.063804 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.063863 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.063921 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.063979 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.064038 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.064099 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.064158 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.064230 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.064293 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.064355 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.064414 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.064478 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.064537 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.064598 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.064657 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.064718 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.064777 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.064839 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.064897 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.064955 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.065012 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.065070 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.065136 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.065196 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.065257 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.065316 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.065373 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.065431 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.065493 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.065572 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.065638 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.065697 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.065759 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.065818 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.065879 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.065938 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.065995 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.066057 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.066117 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.066179 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.066237 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.066298 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.066356 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.066414 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.066473 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.066534 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.066593 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.066654 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.066713 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.066774 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.066832 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.066892 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.066955 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.067014 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.067071 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.067129 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.067190 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.067249 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.067310 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.067368 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.067426 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.067484 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.067546 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.067605 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.067666 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.067725 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.067790 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.067850 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.067911 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.067970 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.068028 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.068085 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.068144 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.068204 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.068264 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.068324 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.068383 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.068442 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.068500 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.068563 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.068622 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.068689 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.068748 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.068810 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.068868 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.068928 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.068986 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.069044 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.069101 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.069159 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.069219 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.069277 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.069338 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.069396 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.069454 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.069512 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.069599 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.069660 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.069722 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.069781 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.069842 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.069900 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.069962 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.070021 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.070079 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.070137 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.070197 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.070258 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.070317 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.070379 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.070443 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.070502 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.070561 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.070623 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.070683 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.070744 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.070804 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.070867 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.070925 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.070986 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.071045 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.071102 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.071161 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.071219 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.071280 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.071344 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.071406 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.071465 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.071523 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.071582 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.071644 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.071703 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.071765 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.071824 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.071886 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.071944 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.072006 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.072065 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.072124 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.072187 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.072245 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.072307 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.072365 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.072426 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.072485 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.072543 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.072601 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.072663 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.072722 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.072783 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.072842 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.072903 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.072962 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.073024 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.073088 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.073147 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.073205 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.073264 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.073325 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.073385 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.073446 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.073504 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.073582 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.073642 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.073705 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.073765 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.073827 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.073886 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.073948 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.074015 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.074077 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.074135 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.074193 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.074250 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.074309 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.074371 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.074432 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.074494 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.074553 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.074611 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.074670 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.074732 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.074791 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.074857 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.074917 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.074979 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.075038 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.075099 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.075157 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.075214 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.075271 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.075330 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.075390 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.075449 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.075510 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.075568 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.075626 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.075683 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.075750 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.075809 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.075871 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.075929 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.075991 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.076048 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.076109 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.076166 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.076223 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.076280 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.076339 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.076400 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.076458 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.076518 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.076576 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.076638 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.076696 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.076758 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.076816 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.076877 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.076935 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.076997 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.077056 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.077117 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.077175 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.077232 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.077291 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.077351 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.077414 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.077488 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.077571 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.077635 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.077694 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.077753 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.077817 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.077880 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.077944 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.078002 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.078063 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.078122 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.078182 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.078240 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.078298 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.078356 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 04:02:12.078420 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 04:02:12.078482 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.078543 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 04:02:12.078605 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.078665 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.078725 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 04:02:12.078784 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.078848 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.078908 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.078970 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.079030 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.079093 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.079153 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 04:02:12.079216 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.079281 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.079341 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 04:02:12.079400 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079460 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079518 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079576 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079634 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079692 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079750 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079808 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079866 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079923 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.079982 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.080039 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.080097 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.080155 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.080213 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.080271 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I0930 04:02:12.080329 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080396 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080458 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080521 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080583 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080644 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080705 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080767 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080829 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080891 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.080953 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.081014 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.081076 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.081137 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.081200 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:12.081270 140278725625664 py_utils.py:1976] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 04:02:21.885685 140278725625664 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f9319f446a0>>
I0930 04:02:25.526082 140278725625664 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I0930 04:02:25.529231 140278725625664 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I0930 04:02:25.534029 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I0930 04:02:25.539239 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I0930 04:02:25.544366 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I0930 04:02:25.549372 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I0930 04:02:25.554544 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I0930 04:02:25.559520 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I0930 04:02:25.564654 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I0930 04:02:25.569672 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I0930 04:02:25.574787 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I0930 04:02:25.579797 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I0930 04:02:25.584902 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I0930 04:02:25.589939 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I0930 04:02:25.595578 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I0930 04:02:25.600661 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I0930 04:02:25.604494 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I0930 04:02:25.608232 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I0930 04:02:25.613248 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I0930 04:02:25.618293 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I0930 04:02:25.641170 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I0930 04:02:25.646196 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I0930 04:02:25.651328 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I0930 04:02:25.656442 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I0930 04:02:25.661448 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I0930 04:02:25.666559 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I0930 04:02:25.671546 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I0930 04:02:25.676662 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I0930 04:02:25.681697 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I0930 04:02:25.686792 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I0930 04:02:25.691815 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I0930 04:02:25.696928 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I0930 04:02:25.701969 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I0930 04:02:25.707212 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I0930 04:02:25.712345 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I0930 04:02:25.717351 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I0930 04:02:25.722912 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I0930 04:02:25.727930 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I0930 04:02:25.733026 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I0930 04:02:25.738088 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I0930 04:02:25.743172 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I0930 04:02:25.748199 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I0930 04:02:25.753328 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I0930 04:02:25.758354 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I0930 04:02:25.763460 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I0930 04:02:25.768499 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I0930 04:02:25.773619 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I0930 04:02:25.778757 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I0930 04:02:25.783779 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I0930 04:02:25.788901 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I0930 04:02:25.792727 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I0930 04:02:25.796513 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I0930 04:02:25.801551 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I0930 04:02:25.806587 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I0930 04:02:25.811681 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I0930 04:02:25.816818 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I0930 04:02:25.821853 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I0930 04:02:25.827013 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I0930 04:02:25.832023 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I0930 04:02:25.837663 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I0930 04:02:25.842699 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I0930 04:02:25.847850 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I0930 04:02:25.852880 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I0930 04:02:25.858007 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I0930 04:02:25.863039 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I0930 04:02:25.868206 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I0930 04:02:25.873329 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I0930 04:02:25.878397 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I0930 04:02:25.883551 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I0930 04:02:25.888584 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I0930 04:02:25.893777 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I0930 04:02:25.898814 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I0930 04:02:25.903946 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I0930 04:02:25.908977 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I0930 04:02:25.914148 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I0930 04:02:25.919241 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I0930 04:02:25.924371 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I0930 04:02:25.929502 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I0930 04:02:25.934571 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I0930 04:02:25.939770 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I0930 04:02:25.944782 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I0930 04:02:25.950412 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I0930 04:02:25.955445 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I0930 04:02:25.960587 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I0930 04:02:25.964457 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I0930 04:02:25.968217 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I0930 04:02:25.973280 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I0930 04:02:25.978503 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I0930 04:02:25.983582 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I0930 04:02:25.988718 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I0930 04:02:25.993821 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I0930 04:02:25.998956 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I0930 04:02:26.003986 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I0930 04:02:26.009133 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I0930 04:02:26.014202 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I0930 04:02:26.019344 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I0930 04:02:26.024383 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I0930 04:02:26.029512 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I0930 04:02:26.034685 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I0930 04:02:26.039711 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I0930 04:02:26.044869 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I0930 04:02:26.049941 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I0930 04:02:26.055176 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I0930 04:02:26.060227 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I0930 04:02:26.065822 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I0930 04:02:26.070887 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I0930 04:02:26.076054 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I0930 04:02:26.081176 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I0930 04:02:26.086383 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I0930 04:02:26.091425 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I0930 04:02:26.096579 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I0930 04:02:26.101732 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I0930 04:02:26.106845 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I0930 04:02:26.112018 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I0930 04:02:26.117036 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I0930 04:02:26.122205 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I0930 04:02:26.127245 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I0930 04:02:26.132442 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I0930 04:02:26.136299 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I0930 04:02:26.140069 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I0930 04:02:26.145133 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I0930 04:02:26.150295 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I0930 04:02:26.155343 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I0930 04:02:26.160497 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I0930 04:02:26.165572 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I0930 04:02:26.170725 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I0930 04:02:26.175760 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I0930 04:02:26.181351 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I0930 04:02:26.186424 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I0930 04:02:26.191574 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I0930 04:02:26.196589 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I0930 04:02:26.201749 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I0930 04:02:26.206925 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I0930 04:02:26.212025 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I0930 04:02:26.217188 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I0930 04:02:26.222255 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I0930 04:02:26.227396 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I0930 04:02:26.232463 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I0930 04:02:26.237633 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I0930 04:02:26.242683 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I0930 04:02:26.247827 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I0930 04:02:26.252866 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I0930 04:02:26.258019 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I0930 04:02:26.263058 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I0930 04:02:26.268219 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I0930 04:02:26.273351 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I0930 04:02:26.278444 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I0930 04:02:26.283583 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I0930 04:02:26.288640 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I0930 04:02:26.294261 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I0930 04:02:26.299314 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I0930 04:02:26.304457 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I0930 04:02:26.308335 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I0930 04:02:26.312145 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I0930 04:02:26.317326 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I0930 04:02:26.322524 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I0930 04:02:26.327574 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I0930 04:02:26.332736 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I0930 04:02:26.337825 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I0930 04:02:26.342984 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I0930 04:02:26.348063 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I0930 04:02:26.353224 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I0930 04:02:26.358302 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I0930 04:02:26.363461 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I0930 04:02:26.368534 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I0930 04:02:26.373722 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I0930 04:02:26.378878 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I0930 04:02:26.383965 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I0930 04:02:26.389120 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I0930 04:02:26.394230 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I0930 04:02:26.399506 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I0930 04:02:26.404711 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I0930 04:02:26.410497 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I0930 04:02:26.415564 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I0930 04:02:26.420744 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I0930 04:02:26.425837 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I0930 04:02:26.431000 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I0930 04:02:26.436084 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I0930 04:02:26.441238 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I0930 04:02:26.446447 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I0930 04:02:26.451508 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I0930 04:02:26.456696 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I0930 04:02:26.461766 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I0930 04:02:26.466949 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I0930 04:02:26.472074 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I0930 04:02:26.477229 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I0930 04:02:26.481111 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I0930 04:02:26.484893 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I0930 04:02:26.490026 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I0930 04:02:26.495187 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I0930 04:02:26.500247 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I0930 04:02:26.505438 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I0930 04:02:26.510606 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I0930 04:02:26.515918 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I0930 04:02:26.520983 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I0930 04:02:26.526796 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I0930 04:02:26.531851 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I0930 04:02:26.537021 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I0930 04:02:26.542161 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I0930 04:02:26.547550 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I0930 04:02:26.552956 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I0930 04:02:26.558143 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I0930 04:02:26.563336 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I0930 04:02:26.568397 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I0930 04:02:26.573580 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I0930 04:02:26.578717 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I0930 04:02:26.583882 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I0930 04:02:26.588944 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I0930 04:02:26.594167 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I0930 04:02:26.599336 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I0930 04:02:26.604492 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I0930 04:02:26.609608 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I0930 04:02:26.614781 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I0930 04:02:26.619945 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I0930 04:02:26.625009 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I0930 04:02:26.630212 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I0930 04:02:26.635296 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I0930 04:02:26.641040 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I0930 04:02:26.646149 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I0930 04:02:26.651387 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I0930 04:02:26.655557 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I0930 04:02:26.659425 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I0930 04:02:26.664624 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I0930 04:02:26.669879 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I0930 04:02:26.674941 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I0930 04:02:26.680143 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I0930 04:02:26.685266 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I0930 04:02:26.690517 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I0930 04:02:26.695612 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I0930 04:02:26.700793 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I0930 04:02:26.705917 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I0930 04:02:26.711155 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I0930 04:02:26.716233 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I0930 04:02:26.721430 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I0930 04:02:26.726651 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I0930 04:02:26.731851 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I0930 04:02:26.737027 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I0930 04:02:26.742145 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I0930 04:02:26.747338 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I0930 04:02:26.752417 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I0930 04:02:26.758254 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I0930 04:02:26.763535 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I0930 04:02:26.768888 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I0930 04:02:26.774170 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I0930 04:02:26.779351 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I0930 04:02:26.784453 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I0930 04:02:26.789638 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I0930 04:02:26.794829 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I0930 04:02:26.799904 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I0930 04:02:26.805101 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I0930 04:02:26.810229 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I0930 04:02:26.815420 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I0930 04:02:26.820506 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I0930 04:02:26.825698 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I0930 04:02:26.829581 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I0930 04:02:26.833369 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I0930 04:02:26.838560 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I0930 04:02:26.843767 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I0930 04:02:26.848847 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I0930 04:02:26.854051 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I0930 04:02:26.859131 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I0930 04:02:26.864404 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I0930 04:02:26.869594 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I0930 04:02:26.875554 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I0930 04:02:26.880844 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I0930 04:02:26.886099 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I0930 04:02:26.891227 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I0930 04:02:26.896442 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I0930 04:02:26.901658 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I0930 04:02:26.906790 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I0930 04:02:26.911983 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I0930 04:02:26.917083 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I0930 04:02:26.922274 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I0930 04:02:26.927384 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I0930 04:02:26.932565 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I0930 04:02:26.937660 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I0930 04:02:26.942867 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I0930 04:02:26.947983 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I0930 04:02:26.953171 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I0930 04:02:26.958262 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I0930 04:02:26.963454 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I0930 04:02:26.968734 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I0930 04:02:26.973855 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I0930 04:02:26.979049 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I0930 04:02:26.984130 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I0930 04:02:26.989834 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I0930 04:02:26.994931 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I0930 04:02:27.000085 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I0930 04:02:27.004001 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I0930 04:02:27.007810 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I0930 04:02:27.012912 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I0930 04:02:27.018133 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I0930 04:02:27.023231 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I0930 04:02:27.028408 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I0930 04:02:27.033513 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I0930 04:02:27.038748 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I0930 04:02:27.043817 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I0930 04:02:27.048997 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I0930 04:02:27.054121 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I0930 04:02:27.059302 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I0930 04:02:27.064450 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I0930 04:02:27.069601 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I0930 04:02:27.074896 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I0930 04:02:27.079982 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I0930 04:02:27.085185 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I0930 04:02:27.090329 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I0930 04:02:27.095512 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I0930 04:02:27.100706 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I0930 04:02:27.106407 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I0930 04:02:27.111495 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I0930 04:02:27.116673 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I0930 04:02:27.121783 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I0930 04:02:27.127036 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I0930 04:02:27.132126 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I0930 04:02:27.137304 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I0930 04:02:27.142479 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I0930 04:02:27.147558 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I0930 04:02:27.152750 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I0930 04:02:27.157841 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I0930 04:02:27.163049 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I0930 04:02:27.168127 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I0930 04:02:27.173309 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I0930 04:02:27.177230 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I0930 04:02:27.181037 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I0930 04:02:27.186176 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I0930 04:02:27.191377 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I0930 04:02:27.196473 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I0930 04:02:27.201692 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I0930 04:02:27.206791 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I0930 04:02:27.211982 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I0930 04:02:27.217077 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I0930 04:02:27.222766 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I0930 04:02:27.227936 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I0930 04:02:27.233135 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I0930 04:02:27.238245 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I0930 04:02:27.243424 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I0930 04:02:27.248601 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I0930 04:02:27.253801 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I0930 04:02:27.258970 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I0930 04:02:27.264072 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I0930 04:02:27.269247 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I0930 04:02:27.274373 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I0930 04:02:27.279611 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I0930 04:02:27.284723 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I0930 04:02:27.289923 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I0930 04:02:27.295037 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I0930 04:02:27.300221 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I0930 04:02:27.305335 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I0930 04:02:27.310559 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I0930 04:02:27.315737 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I0930 04:02:27.320815 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I0930 04:02:27.326038 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I0930 04:02:27.331125 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I0930 04:02:27.336779 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I0930 04:02:27.341897 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I0930 04:02:27.347095 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I0930 04:02:27.351003 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I0930 04:02:27.354812 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I0930 04:02:27.359958 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I0930 04:02:27.365155 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I0930 04:02:27.370271 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I0930 04:02:27.375473 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I0930 04:02:27.380581 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I0930 04:02:27.385868 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I0930 04:02:27.390972 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I0930 04:02:27.396179 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I0930 04:02:27.401269 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I0930 04:02:27.406503 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I0930 04:02:27.411661 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I0930 04:02:27.416863 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I0930 04:02:27.422071 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I0930 04:02:27.427207 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I0930 04:02:27.432408 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I0930 04:02:27.437506 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I0930 04:02:27.442726 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I0930 04:02:27.447845 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I0930 04:02:27.453480 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I0930 04:02:27.458590 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I0930 04:02:27.463795 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I0930 04:02:27.468869 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I0930 04:02:27.474098 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I0930 04:02:27.479178 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I0930 04:02:27.484350 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I0930 04:02:27.489571 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I0930 04:02:27.494680 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I0930 04:02:27.499893 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I0930 04:02:27.504980 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I0930 04:02:27.510197 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I0930 04:02:27.515416 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I0930 04:02:27.520603 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I0930 04:02:27.524527 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I0930 04:02:27.528340 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I0930 04:02:27.533472 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I0930 04:02:27.538694 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I0930 04:02:27.543814 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I0930 04:02:27.549025 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I0930 04:02:27.554153 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I0930 04:02:27.559375 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I0930 04:02:27.564471 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I0930 04:02:27.570245 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I0930 04:02:27.575344 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I0930 04:02:27.580524 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I0930 04:02:27.585665 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I0930 04:02:27.590860 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I0930 04:02:27.596085 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I0930 04:02:27.601211 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I0930 04:02:27.606418 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I0930 04:02:27.611545 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I0930 04:02:27.616713 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I0930 04:02:27.621883 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I0930 04:02:27.627083 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I0930 04:02:27.632200 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I0930 04:02:27.637402 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I0930 04:02:27.642522 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I0930 04:02:27.647810 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I0930 04:02:27.652915 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I0930 04:02:27.658139 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I0930 04:02:27.663354 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I0930 04:02:27.668473 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I0930 04:02:27.673807 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I0930 04:02:27.678934 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I0930 04:02:27.684619 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I0930 04:02:27.689739 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I0930 04:02:27.694961 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I0930 04:02:27.698899 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I0930 04:02:27.702742 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I0930 04:02:27.707853 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I0930 04:02:27.713079 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I0930 04:02:27.718219 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I0930 04:02:27.723457 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I0930 04:02:27.728570 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I0930 04:02:27.733809 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I0930 04:02:27.738947 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I0930 04:02:27.744147 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I0930 04:02:27.749265 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I0930 04:02:27.754515 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I0930 04:02:27.759627 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I0930 04:02:27.764855 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I0930 04:02:27.770096 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I0930 04:02:27.775325 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I0930 04:02:27.780558 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I0930 04:02:27.785674 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I0930 04:02:27.790891 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I0930 04:02:27.796017 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I0930 04:02:27.801742 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I0930 04:02:27.806847 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I0930 04:02:27.812090 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I0930 04:02:27.817176 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I0930 04:02:27.822431 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I0930 04:02:27.827578 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I0930 04:02:27.832803 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I0930 04:02:27.838058 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I0930 04:02:27.843184 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I0930 04:02:27.848408 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I0930 04:02:27.853543 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I0930 04:02:27.858779 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I0930 04:02:27.863914 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I0930 04:02:27.869100 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I0930 04:02:27.873053 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I0930 04:02:27.876886 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I0930 04:02:27.882097 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I0930 04:02:27.887325 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I0930 04:02:27.892441 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I0930 04:02:27.897729 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I0930 04:02:27.902838 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I0930 04:02:27.908128 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I0930 04:02:27.913241 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I0930 04:02:27.918981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I0930 04:02:27.924082 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I0930 04:02:27.929283 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I0930 04:02:27.934488 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I0930 04:02:27.939680 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I0930 04:02:27.944944 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I0930 04:02:27.950169 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I0930 04:02:27.955404 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I0930 04:02:27.960546 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I0930 04:02:27.965777 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I0930 04:02:27.970938 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I0930 04:02:27.976166 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I0930 04:02:27.981302 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I0930 04:02:27.986604 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I0930 04:02:27.991723 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I0930 04:02:27.996925 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I0930 04:02:28.002070 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I0930 04:02:28.007280 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I0930 04:02:28.012574 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I0930 04:02:28.017726 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I0930 04:02:28.022932 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I0930 04:02:28.028082 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I0930 04:02:28.033791 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I0930 04:02:28.038949 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I0930 04:02:28.044162 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I0930 04:02:28.048103 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I0930 04:02:28.051959 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I0930 04:02:28.057084 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I0930 04:02:28.062334 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I0930 04:02:28.067550 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I0930 04:02:28.072773 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I0930 04:02:28.077922 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I0930 04:02:28.083170 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I0930 04:02:28.088272 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I0930 04:02:28.093497 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I0930 04:02:28.098659 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I0930 04:02:28.103878 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I0930 04:02:28.108998 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I0930 04:02:28.114260 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I0930 04:02:28.119468 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I0930 04:02:28.124621 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I0930 04:02:28.129884 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I0930 04:02:28.135022 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I0930 04:02:28.140236 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I0930 04:02:28.145457 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I0930 04:02:28.151136 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I0930 04:02:28.156264 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I0930 04:02:28.161484 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I0930 04:02:28.166620 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I0930 04:02:28.171824 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I0930 04:02:28.176954 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I0930 04:02:28.182210 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I0930 04:02:28.187451 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I0930 04:02:28.192593 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I0930 04:02:28.197842 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I0930 04:02:28.202964 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I0930 04:02:28.208200 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I0930 04:02:28.213359 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I0930 04:02:28.218618 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I0930 04:02:28.222566 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I0930 04:02:28.226426 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I0930 04:02:28.231591 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I0930 04:02:28.236810 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I0930 04:02:28.241983 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I0930 04:02:28.247278 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I0930 04:02:28.252410 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I0930 04:02:28.257676 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I0930 04:02:28.262807 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I0930 04:02:28.268490 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I0930 04:02:28.273654 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I0930 04:02:28.278892 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I0930 04:02:28.284031 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I0930 04:02:28.289273 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I0930 04:02:28.294532 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I0930 04:02:28.299726 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I0930 04:02:28.304954 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I0930 04:02:28.310115 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I0930 04:02:28.315401 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I0930 04:02:28.320519 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I0930 04:02:28.325796 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I0930 04:02:28.330961 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I0930 04:02:28.336215 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I0930 04:02:28.341389 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I0930 04:02:28.346604 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I0930 04:02:28.351762 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I0930 04:02:28.356976 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I0930 04:02:28.362236 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I0930 04:02:28.367370 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I0930 04:02:28.372621 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I0930 04:02:28.377800 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I0930 04:02:28.383496 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I0930 04:02:28.388628 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I0930 04:02:28.393897 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I0930 04:02:28.397857 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I0930 04:02:28.401727 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I0930 04:02:28.406891 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I0930 04:02:28.412145 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I0930 04:02:28.417283 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I0930 04:02:28.422563 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I0930 04:02:28.427675 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I0930 04:02:28.432921 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I0930 04:02:28.438089 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I0930 04:02:28.443358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I0930 04:02:28.448506 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I0930 04:02:28.453750 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I0930 04:02:28.458918 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I0930 04:02:28.464162 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I0930 04:02:28.469397 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I0930 04:02:28.474574 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I0930 04:02:28.479809 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I0930 04:02:28.484963 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I0930 04:02:28.490213 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I0930 04:02:28.495355 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I0930 04:02:28.501036 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I0930 04:02:28.506201 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I0930 04:02:28.511445 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I0930 04:02:28.516697 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I0930 04:02:28.521958 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I0930 04:02:28.527097 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I0930 04:02:28.532358 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I0930 04:02:28.537624 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I0930 04:02:28.542760 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I0930 04:02:28.547995 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I0930 04:02:28.553147 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I0930 04:02:28.558425 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I0930 04:02:28.563570 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I0930 04:02:28.568812 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I0930 04:02:28.572778 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I0930 04:02:28.576658 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I0930 04:02:28.581834 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I0930 04:02:28.587082 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I0930 04:02:28.592265 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I0930 04:02:28.597488 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I0930 04:02:28.602686 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I0930 04:02:28.607923 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I0930 04:02:28.613061 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I0930 04:02:28.618777 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I0930 04:02:28.623962 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I0930 04:02:28.629200 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I0930 04:02:28.634363 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I0930 04:02:28.639602 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I0930 04:02:28.644908 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I0930 04:02:28.650099 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I0930 04:02:28.655331 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I0930 04:02:28.660478 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I0930 04:02:28.665740 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I0930 04:02:28.670932 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I0930 04:02:28.676182 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I0930 04:02:28.681333 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I0930 04:02:28.686604 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I0930 04:02:28.691753 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I0930 04:02:28.696990 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I0930 04:02:28.702188 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I0930 04:02:28.707406 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I0930 04:02:28.712666 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I0930 04:02:28.717847 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I0930 04:02:28.723098 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I0930 04:02:28.728256 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I0930 04:02:28.733965 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I0930 04:02:28.739125 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I0930 04:02:28.744385 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I0930 04:02:28.748372 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I0930 04:02:28.752263 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I0930 04:02:28.757468 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I0930 04:02:28.762758 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I0930 04:02:28.767904 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I0930 04:02:28.773193 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I0930 04:02:28.778357 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I0930 04:02:28.783633 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I0930 04:02:28.788810 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I0930 04:02:28.794083 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I0930 04:02:28.799256 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I0930 04:02:28.804516 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I0930 04:02:28.809699 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I0930 04:02:28.815009 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I0930 04:02:28.820280 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I0930 04:02:28.825514 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I0930 04:02:28.830785 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I0930 04:02:28.835956 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I0930 04:02:28.841187 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I0930 04:02:28.846372 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I0930 04:02:28.852156 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I0930 04:02:28.857339 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I0930 04:02:28.862666 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I0930 04:02:28.867834 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I0930 04:02:28.873054 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I0930 04:02:28.878258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I0930 04:02:28.883518 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I0930 04:02:28.888784 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I0930 04:02:28.894019 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I0930 04:02:28.899261 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I0930 04:02:28.904517 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I0930 04:02:28.909787 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I0930 04:02:28.914966 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I0930 04:02:28.920255 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I0930 04:02:28.924244 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I0930 04:02:28.928104 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I0930 04:02:28.933360 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I0930 04:02:28.938646 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I0930 04:02:28.943784 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I0930 04:02:28.949025 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I0930 04:02:28.954215 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I0930 04:02:28.959487 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I0930 04:02:28.964653 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I0930 04:02:28.970376 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I0930 04:02:28.975532 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I0930 04:02:28.980777 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I0930 04:02:28.986165 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I0930 04:02:28.991435 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I0930 04:02:28.996694 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I0930 04:02:29.001901 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I0930 04:02:29.007154 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I0930 04:02:29.012327 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I0930 04:02:29.017588 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I0930 04:02:29.022782 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I0930 04:02:29.028055 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I0930 04:02:29.033221 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I0930 04:02:29.038516 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I0930 04:02:29.043708 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I0930 04:02:29.049006 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I0930 04:02:29.054202 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I0930 04:02:29.059497 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I0930 04:02:29.064783 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I0930 04:02:29.069964 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I0930 04:02:29.075236 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I0930 04:02:29.080407 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I0930 04:02:29.086165 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I0930 04:02:29.091374 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I0930 04:02:29.096621 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I0930 04:02:29.100591 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I0930 04:02:29.104472 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I0930 04:02:29.109681 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I0930 04:02:29.114937 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I0930 04:02:29.120116 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I0930 04:02:29.125384 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I0930 04:02:29.130575 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I0930 04:02:29.135850 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I0930 04:02:29.141005 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I0930 04:02:29.146303 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I0930 04:02:29.151447 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I0930 04:02:29.156739 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I0930 04:02:29.161923 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I0930 04:02:29.167187 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I0930 04:02:29.172442 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I0930 04:02:29.177679 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I0930 04:02:29.182934 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I0930 04:02:29.188078 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I0930 04:02:29.193385 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I0930 04:02:29.198734 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I0930 04:02:29.204445 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I0930 04:02:29.209636 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I0930 04:02:29.214912 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I0930 04:02:29.220131 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I0930 04:02:29.225382 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I0930 04:02:29.230605 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I0930 04:02:29.235852 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I0930 04:02:29.241132 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I0930 04:02:29.246347 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I0930 04:02:29.251619 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I0930 04:02:29.256814 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I0930 04:02:29.262152 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I0930 04:02:29.267311 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I0930 04:02:29.272579 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I0930 04:02:29.276581 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I0930 04:02:29.280467 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I0930 04:02:29.285688 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I0930 04:02:29.290972 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I0930 04:02:29.296131 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I0930 04:02:29.301415 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I0930 04:02:29.306639 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I0930 04:02:29.311929 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I0930 04:02:29.317119 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I0930 04:02:29.322925 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I0930 04:02:29.328209 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I0930 04:02:29.333494 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I0930 04:02:29.338728 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I0930 04:02:29.343985 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I0930 04:02:29.349279 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I0930 04:02:29.354519 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I0930 04:02:29.359827 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I0930 04:02:29.364973 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I0930 04:02:29.370276 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I0930 04:02:29.375450 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I0930 04:02:29.380754 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I0930 04:02:29.385937 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I0930 04:02:29.391202 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I0930 04:02:29.396390 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I0930 04:02:29.401684 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I0930 04:02:29.406877 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I0930 04:02:29.412147 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I0930 04:02:29.417399 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I0930 04:02:29.422609 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I0930 04:02:29.427901 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I0930 04:02:29.433077 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I0930 04:02:29.438844 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I0930 04:02:29.444036 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I0930 04:02:29.449317 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I0930 04:02:29.453310 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I0930 04:02:29.457222 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I0930 04:02:29.462445 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I0930 04:02:29.467751 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I0930 04:02:29.472924 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I0930 04:02:29.478236 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I0930 04:02:29.483407 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I0930 04:02:29.488709 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I0930 04:02:29.493923 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I0930 04:02:29.499192 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I0930 04:02:29.504426 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I0930 04:02:29.509732 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I0930 04:02:29.515012 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I0930 04:02:29.520295 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I0930 04:02:29.525596 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I0930 04:02:29.530820 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I0930 04:02:29.536127 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I0930 04:02:29.541333 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I0930 04:02:29.546640 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I0930 04:02:29.551830 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I0930 04:02:29.557542 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I0930 04:02:29.562757 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I0930 04:02:29.568054 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I0930 04:02:29.573259 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I0930 04:02:29.578616 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I0930 04:02:29.583884 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I0930 04:02:29.589155 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I0930 04:02:29.594516 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I0930 04:02:29.599696 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I0930 04:02:29.604969 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I0930 04:02:29.610171 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I0930 04:02:29.615474 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I0930 04:02:29.620750 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I0930 04:02:29.626090 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I0930 04:02:29.630088 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I0930 04:02:29.634020 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I0930 04:02:29.639204 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I0930 04:02:29.644500 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I0930 04:02:29.649697 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I0930 04:02:29.654977 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I0930 04:02:29.660202 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I0930 04:02:29.665539 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I0930 04:02:29.670736 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I0930 04:02:29.676536 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I0930 04:02:29.681753 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I0930 04:02:29.687044 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I0930 04:02:29.692241 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I0930 04:02:29.697539 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I0930 04:02:29.702848 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I0930 04:02:29.708050 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I0930 04:02:29.713340 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I0930 04:02:29.718557 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I0930 04:02:29.723868 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I0930 04:02:29.729031 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I0930 04:02:29.734341 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I0930 04:02:29.739537 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I0930 04:02:29.744839 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I0930 04:02:29.750058 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I0930 04:02:29.755374 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I0930 04:02:29.760590 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I0930 04:02:29.765891 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I0930 04:02:29.771152 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I0930 04:02:29.776351 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I0930 04:02:29.781676 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I0930 04:02:29.786872 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I0930 04:02:29.792630 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I0930 04:02:29.797839 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I0930 04:02:29.803155 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I0930 04:02:29.807183 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I0930 04:02:29.811100 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I0930 04:02:29.816311 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I0930 04:02:29.821630 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I0930 04:02:29.826827 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I0930 04:02:29.832190 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I0930 04:02:29.837389 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I0930 04:02:29.842727 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I0930 04:02:29.847932 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I0930 04:02:29.853219 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I0930 04:02:29.858461 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I0930 04:02:29.863745 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I0930 04:02:29.868967 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I0930 04:02:29.874306 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I0930 04:02:29.879609 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I0930 04:02:29.884843 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I0930 04:02:29.890195 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I0930 04:02:29.895402 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I0930 04:02:29.900706 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I0930 04:02:29.905926 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I0930 04:02:29.911724 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I0930 04:02:29.916908 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I0930 04:02:29.922224 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I0930 04:02:29.927441 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I0930 04:02:29.932698 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I0930 04:02:29.937949 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I0930 04:02:29.943229 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I0930 04:02:29.948543 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I0930 04:02:29.953752 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I0930 04:02:29.959079 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I0930 04:02:29.964277 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I0930 04:02:29.969648 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I0930 04:02:29.974856 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I0930 04:02:29.980168 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I0930 04:02:29.984195 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I0930 04:02:29.988142 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I0930 04:02:29.993371 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I0930 04:02:29.998754 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I0930 04:02:30.003947 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I0930 04:02:30.009252 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I0930 04:02:30.014471 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I0930 04:02:30.019818 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I0930 04:02:30.025011 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I0930 04:02:30.030783 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I0930 04:02:30.035986 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I0930 04:02:30.041300 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I0930 04:02:30.046532 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I0930 04:02:30.051847 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I0930 04:02:30.057161 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I0930 04:02:30.062426 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I0930 04:02:30.067753 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I0930 04:02:30.072968 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I0930 04:02:30.078282 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I0930 04:02:30.083490 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I0930 04:02:30.088804 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I0930 04:02:30.094038 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I0930 04:02:30.099485 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I0930 04:02:30.104681 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I0930 04:02:30.110011 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I0930 04:02:30.115225 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I0930 04:02:30.120564 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I0930 04:02:30.125885 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I0930 04:02:30.131098 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I0930 04:02:30.136419 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I0930 04:02:30.141685 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I0930 04:02:30.147465 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I0930 04:02:30.152728 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I0930 04:02:30.158073 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I0930 04:02:30.162096 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I0930 04:02:30.166045 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I0930 04:02:30.171272 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I0930 04:02:30.176582 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I0930 04:02:30.181940 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I0930 04:02:30.187258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I0930 04:02:30.192469 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I0930 04:02:30.197796 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I0930 04:02:30.203024 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I0930 04:02:30.208339 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I0930 04:02:30.213569 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I0930 04:02:30.218918 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I0930 04:02:30.224117 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I0930 04:02:30.229411 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I0930 04:02:30.234784 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I0930 04:02:30.240041 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I0930 04:02:30.245348 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I0930 04:02:30.250637 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I0930 04:02:30.256049 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I0930 04:02:30.261288 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I0930 04:02:30.267107 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I0930 04:02:30.272318 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I0930 04:02:30.277673 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I0930 04:02:30.282915 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I0930 04:02:30.288219 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I0930 04:02:30.293458 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I0930 04:02:30.298804 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I0930 04:02:30.304120 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I0930 04:02:30.309328 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I0930 04:02:30.314694 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I0930 04:02:30.319910 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I0930 04:02:30.325218 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I0930 04:02:30.330446 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I0930 04:02:30.335748 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I0930 04:02:30.339776 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I0930 04:02:30.343731 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I0930 04:02:30.348973 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I0930 04:02:30.354334 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I0930 04:02:30.359514 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I0930 04:02:30.364844 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I0930 04:02:30.370086 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I0930 04:02:30.375403 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I0930 04:02:30.380608 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I0930 04:02:30.386403 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I0930 04:02:30.391630 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I0930 04:02:30.396950 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I0930 04:02:30.402189 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I0930 04:02:30.407512 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I0930 04:02:30.412835 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I0930 04:02:30.418149 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I0930 04:02:30.423480 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I0930 04:02:30.428696 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I0930 04:02:30.434050 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I0930 04:02:30.439254 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I0930 04:02:30.444648 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I0930 04:02:30.449909 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I0930 04:02:30.455235 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I0930 04:02:30.460447 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I0930 04:02:30.465770 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I0930 04:02:30.471063 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I0930 04:02:30.476386 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I0930 04:02:30.481735 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I0930 04:02:30.486966 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I0930 04:02:30.492299 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I0930 04:02:30.497546 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I0930 04:02:30.503337 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I0930 04:02:30.508542 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I0930 04:02:30.513993 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I0930 04:02:30.518091 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I0930 04:02:30.522061 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I0930 04:02:30.527295 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I0930 04:02:30.532665 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I0930 04:02:30.537901 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I0930 04:02:30.543217 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I0930 04:02:30.548436 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I0930 04:02:30.553872 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I0930 04:02:30.559108 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I0930 04:02:30.564432 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I0930 04:02:30.569683 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I0930 04:02:30.574997 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I0930 04:02:30.580221 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I0930 04:02:30.585564 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I0930 04:02:30.590895 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I0930 04:02:30.596147 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I0930 04:02:30.601474 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I0930 04:02:30.606712 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I0930 04:02:30.612033 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I0930 04:02:30.617258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I0930 04:02:30.623057 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I0930 04:02:30.628305 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I0930 04:02:30.633658 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I0930 04:02:30.638893 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I0930 04:02:30.644204 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I0930 04:02:30.649444 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I0930 04:02:30.654783 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I0930 04:02:30.660200 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I0930 04:02:30.665425 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I0930 04:02:30.670792 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I0930 04:02:30.675975 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I0930 04:02:30.681352 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I0930 04:02:30.686613 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I0930 04:02:30.691965 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I0930 04:02:30.696020 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I0930 04:02:30.699998 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I0930 04:02:30.705234 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I0930 04:02:30.710592 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I0930 04:02:30.715848 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I0930 04:02:30.721174 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I0930 04:02:30.726447 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I0930 04:02:30.731762 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I0930 04:02:30.736981 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I0930 04:02:30.742811 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I0930 04:02:30.748030 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I0930 04:02:30.753374 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I0930 04:02:30.758606 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I0930 04:02:30.763943 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I0930 04:02:30.769272 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I0930 04:02:30.774595 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I0930 04:02:30.779922 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I0930 04:02:30.785153 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I0930 04:02:30.790535 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I0930 04:02:30.795793 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I0930 04:02:30.801130 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I0930 04:02:30.806379 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I0930 04:02:30.811711 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I0930 04:02:30.816954 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I0930 04:02:30.822337 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I0930 04:02:30.827584 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I0930 04:02:30.832920 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I0930 04:02:30.838273 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I0930 04:02:30.843537 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I0930 04:02:30.848965 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I0930 04:02:30.854210 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I0930 04:02:30.860019 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I0930 04:02:30.865279 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I0930 04:02:30.870630 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I0930 04:02:30.874688 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I0930 04:02:30.878646 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I0930 04:02:30.883887 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I0930 04:02:30.889229 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I0930 04:02:30.894531 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I0930 04:02:30.899841 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I0930 04:02:30.905071 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I0930 04:02:30.910423 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I0930 04:02:30.915652 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I0930 04:02:30.920992 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I0930 04:02:30.926258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I0930 04:02:30.931584 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I0930 04:02:30.936821 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I0930 04:02:30.942189 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I0930 04:02:30.947523 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I0930 04:02:30.952809 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I0930 04:02:30.958248 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I0930 04:02:30.963514 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I0930 04:02:30.968879 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I0930 04:02:30.974191 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I0930 04:02:30.980098 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I0930 04:02:30.985342 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I0930 04:02:30.990701 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I0930 04:02:30.995971 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I0930 04:02:31.001327 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I0930 04:02:31.006661 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I0930 04:02:31.011986 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I0930 04:02:31.017345 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I0930 04:02:31.022627 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I0930 04:02:31.028004 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I0930 04:02:31.033252 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I0930 04:02:31.038628 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I0930 04:02:31.043884 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I0930 04:02:31.049257 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I0930 04:02:31.053318 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I0930 04:02:31.057307 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I0930 04:02:31.062612 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I0930 04:02:31.067978 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I0930 04:02:31.073245 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I0930 04:02:31.078619 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I0930 04:02:31.083876 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I0930 04:02:31.089292 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I0930 04:02:31.094570 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I0930 04:02:31.100393 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I0930 04:02:31.105679 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I0930 04:02:31.111055 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I0930 04:02:31.116295 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I0930 04:02:31.121693 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I0930 04:02:31.127051 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I0930 04:02:31.132330 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I0930 04:02:31.137728 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I0930 04:02:31.142987 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I0930 04:02:31.148327 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I0930 04:02:31.153632 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I0930 04:02:31.158964 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I0930 04:02:31.164263 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I0930 04:02:31.169644 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I0930 04:02:31.174922 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I0930 04:02:31.180250 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I0930 04:02:31.185485 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I0930 04:02:31.190850 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I0930 04:02:31.196186 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I0930 04:02:31.201437 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I0930 04:02:31.206812 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I0930 04:02:31.212078 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I0930 04:02:31.217896 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I0930 04:02:31.223121 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I0930 04:02:31.228504 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I0930 04:02:31.233781 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I0930 04:02:31.239142 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I0930 04:02:31.244404 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I0930 04:02:31.249755 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I0930 04:02:31.255035 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I0930 04:02:31.260374 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I0930 04:02:31.265770 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I0930 04:02:31.271043 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I0930 04:02:31.276400 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I0930 04:02:31.281685 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I0930 04:02:31.287050 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I0930 04:02:31.292290 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I0930 04:02:31.297670 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I0930 04:02:31.302920 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I0930 04:02:31.308284 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I0930 04:02:31.313604 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I0930 04:02:31.318965 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I0930 04:02:31.324201 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I0930 04:02:31.329580 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I0930 04:02:31.335394 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I0930 04:02:31.340665 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I0930 04:02:31.346090 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I0930 04:02:31.351372 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I0930 04:02:31.356731 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I0930 04:02:31.362029 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I0930 04:02:31.367435 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I0930 04:02:31.372697 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I0930 04:02:31.378093 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I0930 04:02:31.383341 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I0930 04:02:31.388720 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I0930 04:02:31.394160 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I0930 04:02:31.399419 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I0930 04:02:31.404795 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I0930 04:02:31.410079 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I0930 04:02:31.415471 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I0930 04:02:31.420737 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I0930 04:02:31.426148 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I0930 04:02:31.431435 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I0930 04:02:31.436821 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I0930 04:02:31.442129 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I0930 04:02:31.447544 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I0930 04:02:31.453317 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I0930 04:02:31.458627 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I0930 04:02:31.464011 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I0930 04:02:31.469258 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I0930 04:02:31.474668 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I0930 04:02:31.479928 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I0930 04:02:31.485317 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I0930 04:02:31.490630 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I0930 04:02:31.495998 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I0930 04:02:31.501286 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I0930 04:02:31.506664 140278725625664 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I0930 04:02:32.265741 140278725625664 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I0930 04:02:32.268027 140278725625664 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I0930 04:02:32.472225 140278725625664 trainer.py:400] Trainer number of enqueue ops: 0
I0930 04:02:32.472438 140278725625664 trainer.py:409] AttributeError. Expected for single task models.
I0930 04:02:52.374259 140278725625664 trainer.py:1576] Starting runners
I0930 04:02:52.374764 140269265082112 base_runner.py:167] controller started.
I0930 04:02:52.375182 140269256689408 base_runner.py:167] trainer started.
I0930 04:02:52.375319 140278725625664 trainer.py:1595] Waiting for runners to finish...
I0930 04:03:05.366374 140269256689408 trainer.py:465] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 04:03:06.370370 140269256689408 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 1.50 seconds before retrying.
I0930 04:03:06.371555 140269256689408 trainer.py:465] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 04:03:07.874041 140269256689408 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 2.29 seconds before retrying.
I0930 04:03:07.875221 140269256689408 trainer.py:465] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 04:03:10.167948 140269256689408 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 3.54 seconds before retrying.
I0930 04:03:10.169117 140269256689408 trainer.py:465] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 04:03:12.725120 140269265082112 checkpointer.py:133] Uninitialized var list: [b'global_step', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var', b'1bwds_wpm_level_lm/total_samples/var', b'beta1_power', b'beta2_power', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1', b'total_nan_gradients/var'] 
I0930 04:03:12.728242 140269265082112 checkpointer.py:88] Load from checkpoint /tmp/mnist/log/train/ckpt-00001837.
INFO:tensorflow:Restoring parameters from /tmp/mnist/log/train/ckpt-00001837
I0930 04:03:12.729329 140269265082112 saver.py:1284] Restoring parameters from /tmp/mnist/log/train/ckpt-00001837
I0930 04:03:13.713790 140269256689408 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 5.38 seconds before retrying.
I0930 04:03:13.715321 140269256689408 trainer.py:465] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
2019-09-30 04:03:18.235898: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
E0930 04:03:18.645202 140269265082112 base_runner.py:212] controller done (fatal error): <class 'tensorflow.python.framework.errors_impl.NotFoundError'>
I0930 04:03:18.646462 140269265082112 base_runner.py:106] controller exception: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

From /job:local/replica:0/task:0:
Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
	 [[node save/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save/RestoreV2':
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1823, in <module>
    tf.app.run(main)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1819, in main
    RunnerManager(FLAGS.model).Start()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1811, in Start
    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1559, in CreateRunners
    trial)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1507, in _CreateRunner
    return self.Controller(cfg, *common_args)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 268, in __init__
    self._model)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 281, in _CreateCheckpointer
    return checkpointer.Checkpointer(train_dir, model)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 70, in __init__
    self._saver = self._GetSaver()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 83, in _GetSaver
    write_version=tf.train.SaverDef.V2)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 502, in _build_internal
    restore_sequentially, reshape)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 381, in _AddShardedRestoreOps
    name="restore_shard"))
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
    op_def=op_def)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


E0930 04:03:18.648569 140269265082112 base_runner.py:219] Traceback (most recent call last):
E0930 04:03:18.648653 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1365, in _do_call
E0930 04:03:18.648715 140269265082112 base_runner.py:219]     return fn(*args)
E0930 04:03:18.648766 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1350, in _run_fn
E0930 04:03:18.648815 140269265082112 base_runner.py:219]     target_list, run_metadata)
E0930 04:03:18.648861 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1443, in _call_tf_sessionrun
E0930 04:03:18.648908 140269265082112 base_runner.py:219]     run_metadata)
E0930 04:03:18.648953 140269265082112 base_runner.py:219] tensorflow.python.framework.errors_impl.NotFoundError: From /job:local/replica:0/task:0:
E0930 04:03:18.648998 140269265082112 base_runner.py:219] Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
E0930 04:03:18.649043 140269265082112 base_runner.py:219] 	 [[{{node save/RestoreV2}}]]
E0930 04:03:18.649089 140269265082112 base_runner.py:219] 
E0930 04:03:18.649134 140269265082112 base_runner.py:219] During handling of the above exception, another exception occurred:
E0930 04:03:18.649179 140269265082112 base_runner.py:219] 
E0930 04:03:18.649223 140269265082112 base_runner.py:219] Traceback (most recent call last):
E0930 04:03:18.649267 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 1290, in restore
E0930 04:03:18.649318 140269265082112 base_runner.py:219]     {self.saver_def.filename_tensor_name: save_path})
E0930 04:03:18.649363 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
E0930 04:03:18.649408 140269265082112 base_runner.py:219]     run_metadata_ptr)
E0930 04:03:18.649452 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
E0930 04:03:18.649496 140269265082112 base_runner.py:219]     feed_dict_tensor, options, run_metadata)
E0930 04:03:18.649568 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
E0930 04:03:18.649629 140269265082112 base_runner.py:219]     run_metadata)
E0930 04:03:18.649674 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
E0930 04:03:18.649728 140269265082112 base_runner.py:219]     raise type(e)(node_def, op, message)
E0930 04:03:18.649775 140269265082112 base_runner.py:219] tensorflow.python.framework.errors_impl.NotFoundError: From /job:local/replica:0/task:0:
E0930 04:03:18.649821 140269265082112 base_runner.py:219] Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
E0930 04:03:18.649866 140269265082112 base_runner.py:219] 	 [[node save/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
E0930 04:03:18.649911 140269265082112 base_runner.py:219] 
E0930 04:03:18.649956 140269265082112 base_runner.py:219] Original stack trace for 'save/RestoreV2':
E0930 04:03:18.650001 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1823, in <module>
E0930 04:03:18.650045 140269265082112 base_runner.py:219]     tf.app.run(main)
E0930 04:03:18.650090 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
E0930 04:03:18.650135 140269265082112 base_runner.py:219]     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
E0930 04:03:18.650181 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
E0930 04:03:18.650227 140269265082112 base_runner.py:219]     _run_main(main, args)
E0930 04:03:18.650272 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
E0930 04:03:18.650317 140269265082112 base_runner.py:219]     sys.exit(main(argv))
E0930 04:03:18.650362 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1819, in main
E0930 04:03:18.650407 140269265082112 base_runner.py:219]     RunnerManager(FLAGS.model).Start()
E0930 04:03:18.650452 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1811, in Start
E0930 04:03:18.650497 140269265082112 base_runner.py:219]     self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
E0930 04:03:18.650542 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1559, in CreateRunners
E0930 04:03:18.650587 140269265082112 base_runner.py:219]     trial)
E0930 04:03:18.650632 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1507, in _CreateRunner
E0930 04:03:18.650677 140269265082112 base_runner.py:219]     return self.Controller(cfg, *common_args)
E0930 04:03:18.650722 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 268, in __init__
E0930 04:03:18.650766 140269265082112 base_runner.py:219]     self._model)
E0930 04:03:18.650810 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 281, in _CreateCheckpointer
E0930 04:03:18.650855 140269265082112 base_runner.py:219]     return checkpointer.Checkpointer(train_dir, model)
E0930 04:03:18.650901 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 70, in __init__
E0930 04:03:18.650946 140269265082112 base_runner.py:219]     self._saver = self._GetSaver()
E0930 04:03:18.650991 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 83, in _GetSaver
E0930 04:03:18.651036 140269265082112 base_runner.py:219]     write_version=tf.train.SaverDef.V2)
E0930 04:03:18.651080 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
E0930 04:03:18.651130 140269265082112 base_runner.py:219]     self.build()
E0930 04:03:18.651175 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 840, in build
E0930 04:03:18.651220 140269265082112 base_runner.py:219]     self._build(self._filename, build_save=True, build_restore=True)
E0930 04:03:18.651265 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 878, in _build
E0930 04:03:18.651310 140269265082112 base_runner.py:219]     build_restore=build_restore)
E0930 04:03:18.651354 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 502, in _build_internal
E0930 04:03:18.651399 140269265082112 base_runner.py:219]     restore_sequentially, reshape)
E0930 04:03:18.651443 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 381, in _AddShardedRestoreOps
E0930 04:03:18.651488 140269265082112 base_runner.py:219]     name="restore_shard"))
E0930 04:03:18.651531 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
E0930 04:03:18.651575 140269265082112 base_runner.py:219]     restore_sequentially)
E0930 04:03:18.651619 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
E0930 04:03:18.651664 140269265082112 base_runner.py:219]     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
E0930 04:03:18.651708 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
E0930 04:03:18.651752 140269265082112 base_runner.py:219]     name=name)
E0930 04:03:18.651796 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
E0930 04:03:18.651840 140269265082112 base_runner.py:219]     op_def=op_def)
E0930 04:03:18.651884 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
E0930 04:03:18.651927 140269265082112 base_runner.py:219]     return func(*args, **kwargs)
E0930 04:03:18.651972 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
E0930 04:03:18.652016 140269265082112 base_runner.py:219]     attrs, op_def, compute_device)
E0930 04:03:18.652060 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
E0930 04:03:18.652104 140269265082112 base_runner.py:219]     op_def=op_def)
E0930 04:03:18.652148 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
E0930 04:03:18.652192 140269265082112 base_runner.py:219]     self._traceback = tf_stack.extract_stack()
E0930 04:03:18.652236 140269265082112 base_runner.py:219] 
E0930 04:03:18.652280 140269265082112 base_runner.py:219] 
E0930 04:03:18.652324 140269265082112 base_runner.py:219] During handling of the above exception, another exception occurred:
E0930 04:03:18.652369 140269265082112 base_runner.py:219] 
E0930 04:03:18.652413 140269265082112 base_runner.py:219] Traceback (most recent call last):
E0930 04:03:18.652457 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 1300, in restore
E0930 04:03:18.652501 140269265082112 base_runner.py:219]     names_to_keys = object_graph_key_mapping(save_path)
E0930 04:03:18.652546 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 1618, in object_graph_key_mapping
E0930 04:03:18.652594 140269265082112 base_runner.py:219]     object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY)
E0930 04:03:18.652640 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/pywrap_tensorflow_internal.py", line 915, in get_tensor
E0930 04:03:18.652684 140269265082112 base_runner.py:219]     return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str))
E0930 04:03:18.652728 140269265082112 base_runner.py:219] tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint
E0930 04:03:18.652772 140269265082112 base_runner.py:219] 
E0930 04:03:18.652816 140269265082112 base_runner.py:219] During handling of the above exception, another exception occurred:
E0930 04:03:18.652859 140269265082112 base_runner.py:219] 
E0930 04:03:18.652903 140269265082112 base_runner.py:219] Traceback (most recent call last):
E0930 04:03:18.652947 140269265082112 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
E0930 04:03:18.652991 140269265082112 base_runner.py:219]     loop_func(*loop_args)
E0930 04:03:18.653035 140269265082112 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 318, in _Loop
E0930 04:03:18.653079 140269265082112 base_runner.py:219]     self.checkpointer.RestoreIfNeeded(sess)
E0930 04:03:18.653123 140269265082112 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 134, in RestoreIfNeeded
E0930 04:03:18.653167 140269265082112 base_runner.py:219]     if self._Restore(sess):
E0930 04:03:18.653211 140269265082112 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 119, in _Restore
E0930 04:03:18.653255 140269265082112 base_runner.py:219]     self.RestoreFromPath(sess, path)
E0930 04:03:18.653300 140269265082112 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 89, in RestoreFromPath
E0930 04:03:18.653343 140269265082112 base_runner.py:219]     self._saver.restore(sess, checkpoint_path)
E0930 04:03:18.653387 140269265082112 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 1306, in restore
E0930 04:03:18.653432 140269265082112 base_runner.py:219]     err, "a Variable name or other graph key that is missing")
E0930 04:03:18.653476 140269265082112 base_runner.py:219] tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
E0930 04:03:18.653539 140269265082112 base_runner.py:219] 
E0930 04:03:18.653591 140269265082112 base_runner.py:219] From /job:local/replica:0/task:0:
E0930 04:03:18.653636 140269265082112 base_runner.py:219] Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
E0930 04:03:18.653681 140269265082112 base_runner.py:219] 	 [[node save/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
E0930 04:03:18.653725 140269265082112 base_runner.py:219] 
E0930 04:03:18.653769 140269265082112 base_runner.py:219] Original stack trace for 'save/RestoreV2':
E0930 04:03:18.653813 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1823, in <module>
E0930 04:03:18.653857 140269265082112 base_runner.py:219]     tf.app.run(main)
E0930 04:03:18.653901 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
E0930 04:03:18.653945 140269265082112 base_runner.py:219]     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
E0930 04:03:18.653994 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
E0930 04:03:18.654039 140269265082112 base_runner.py:219]     _run_main(main, args)
E0930 04:03:18.654083 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
E0930 04:03:18.654127 140269265082112 base_runner.py:219]     sys.exit(main(argv))
E0930 04:03:18.654170 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1819, in main
E0930 04:03:18.654214 140269265082112 base_runner.py:219]     RunnerManager(FLAGS.model).Start()
E0930 04:03:18.654258 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1811, in Start
E0930 04:03:18.654302 140269265082112 base_runner.py:219]     self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
E0930 04:03:18.654346 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1559, in CreateRunners
E0930 04:03:18.654390 140269265082112 base_runner.py:219]     trial)
E0930 04:03:18.654434 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1507, in _CreateRunner
E0930 04:03:18.654478 140269265082112 base_runner.py:219]     return self.Controller(cfg, *common_args)
E0930 04:03:18.654522 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 268, in __init__
E0930 04:03:18.654565 140269265082112 base_runner.py:219]     self._model)
E0930 04:03:18.654609 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 281, in _CreateCheckpointer
E0930 04:03:18.654653 140269265082112 base_runner.py:219]     return checkpointer.Checkpointer(train_dir, model)
E0930 04:03:18.654696 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 70, in __init__
E0930 04:03:18.654741 140269265082112 base_runner.py:219]     self._saver = self._GetSaver()
E0930 04:03:18.654784 140269265082112 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 83, in _GetSaver
E0930 04:03:18.654829 140269265082112 base_runner.py:219]     write_version=tf.train.SaverDef.V2)
E0930 04:03:18.654873 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
E0930 04:03:18.654917 140269265082112 base_runner.py:219]     self.build()
E0930 04:03:18.654961 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 840, in build
E0930 04:03:18.655005 140269265082112 base_runner.py:219]     self._build(self._filename, build_save=True, build_restore=True)
E0930 04:03:18.655057 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 878, in _build
E0930 04:03:18.655109 140269265082112 base_runner.py:219]     build_restore=build_restore)
E0930 04:03:18.655153 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 502, in _build_internal
E0930 04:03:18.655197 140269265082112 base_runner.py:219]     restore_sequentially, reshape)
E0930 04:03:18.655241 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 381, in _AddShardedRestoreOps
E0930 04:03:18.655284 140269265082112 base_runner.py:219]     name="restore_shard"))
E0930 04:03:18.655328 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
E0930 04:03:18.655377 140269265082112 base_runner.py:219]     restore_sequentially)
E0930 04:03:18.655422 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
E0930 04:03:18.655467 140269265082112 base_runner.py:219]     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
E0930 04:03:18.655510 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
E0930 04:03:18.655555 140269265082112 base_runner.py:219]     name=name)
E0930 04:03:18.655599 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
E0930 04:03:18.655642 140269265082112 base_runner.py:219]     op_def=op_def)
E0930 04:03:18.655686 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
E0930 04:03:18.655741 140269265082112 base_runner.py:219]     return func(*args, **kwargs)
E0930 04:03:18.655785 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
E0930 04:03:18.655829 140269265082112 base_runner.py:219]     attrs, op_def, compute_device)
E0930 04:03:18.655873 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
E0930 04:03:18.655917 140269265082112 base_runner.py:219]     op_def=op_def)
E0930 04:03:18.655961 140269265082112 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
E0930 04:03:18.656005 140269265082112 base_runner.py:219]     self._traceback = tf_stack.extract_stack()
E0930 04:03:18.656049 140269265082112 base_runner.py:219] 
E0930 04:03:18.656092 140269265082112 base_runner.py:219] 
I0930 04:03:19.094094 140269256689408 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 8.09 seconds before retrying.
I0930 04:03:19.095241 140269256689408 trainer.py:465] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 04:03:27.191959 140269256689408 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1359, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1384, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 12.43 seconds before retrying.
I0930 04:03:27.193106 140269256689408 trainer.py:465] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
