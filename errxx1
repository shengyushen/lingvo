WARNING:tensorflow:

  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.

  Please upgrade your code to TensorFlow 2.0:
    * https://www.tensorflow.org/beta/guide/migration_guide

  Or install the latest stable TensorFlow 1.X release:
    * `pip install -U "tensorflow==1.*"`

  Otherwise your code may be broken by the change.

  
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0930 09:16:03.758502 140009111922496 model_imports.py:47] Importing lingvo.tasks.asr.params
I0930 09:16:03.789175 140009111922496 model_registry.py:124] Registering models from module: lingvo.tasks.asr.params.librispeech
I0930 09:16:03.795227 140009111922496 model_imports.py:47] Importing lingvo.tasks.car.params
I0930 09:16:03.825001 140009111922496 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.kitti
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

W0930 09:16:03.836333 140009111922496 module_wrapper.py:137] From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/metrics/ops/py_metrics_ops.py:24: The name tf.resource_loader.get_path_to_datafile is deprecated. Please use tf.compat.v1.resource_loader.get_path_to_datafile instead.

I0930 09:16:03.854232 140009111922496 model_registry.py:124] Registering models from module: lingvo.tasks.car.params.waymo
I0930 09:16:03.859766 140009111922496 model_imports.py:47] Importing lingvo.tasks.image.params
I0930 09:16:03.862557 140009111922496 model_registry.py:124] Registering models from module: lingvo.tasks.image.params.mnist
I0930 09:16:03.862683 140009111922496 model_imports.py:47] Importing lingvo.tasks.lm.params
I0930 09:16:03.864928 140009111922496 model_registry.py:124] Registering models from module: lingvo.tasks.lm.params.one_billion_wds
I0930 09:16:03.868257 140009111922496 model_imports.py:47] Importing lingvo.tasks.mt.params
I0930 09:16:03.874613 140009111922496 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmt14_en_de
I0930 09:16:03.881779 140009111922496 model_registry.py:124] Registering models from module: lingvo.tasks.mt.params.wmtm16_en_de
I0930 09:16:03.882858 140009111922496 model_imports.py:47] Importing lingvo.tasks.punctuator.params
I0930 09:16:03.885167 140009111922496 model_registry.py:124] Registering models from module: lingvo.tasks.punctuator.params.codelab
2019-09-30 09:16:03.885673: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-09-30 09:16:03.892318: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300060000 Hz
2019-09-30 09:16:03.892555: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7086ed0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-09-30 09:16:03.892574: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-09-30 09:16:03.894903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-09-30 09:16:03.967229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 09:16:03.968040: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x714e8c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-09-30 09:16:03.968063: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2019-09-30 09:16:03.968256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 09:16:03.968967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2019-09-30 09:16:03.969235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-30 09:16:03.970656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2019-09-30 09:16:03.971830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2019-09-30 09:16:03.972113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2019-09-30 09:16:03.973717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2019-09-30 09:16:03.974940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2019-09-30 09:16:03.978746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-09-30 09:16:03.978860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 09:16:03.979631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 09:16:03.980317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-09-30 09:16:03.980356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2019-09-30 09:16:03.981922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-09-30 09:16:03.981938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-09-30 09:16:03.981945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-09-30 09:16:03.982064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 09:16:03.982845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-09-30 09:16:03.983559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:local/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
2019-09-30 09:16:03.985842: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job local -> {0 -> localhost:35020}
2019-09-30 09:16:03.987161: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:35020
I0930 09:16:03.987538 140009111922496 trainer.py:1502] Job controller start
I0930 09:16:04.021966 140009111922496 base_runner.py:57] ============================================================
I0930 09:16:04.042757 140009111922496 base_runner.py:59] allow_implicit_capture : NoneType
I0930 09:16:04.043057 140009111922496 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I0930 09:16:04.043141 140009111922496 base_runner.py:59] cluster.add_summary : NoneType
I0930 09:16:04.043215 140009111922496 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I0930 09:16:04.043285 140009111922496 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I0930 09:16:04.043373 140009111922496 base_runner.py:59] cluster.controller.devices_per_split : 1
I0930 09:16:04.043436 140009111922496 base_runner.py:59] cluster.controller.gpus_per_replica : 1
I0930 09:16:04.043508 140009111922496 base_runner.py:59] cluster.controller.name : '/job:local'
I0930 09:16:04.043575 140009111922496 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I0930 09:16:04.043642 140009111922496 base_runner.py:59] cluster.controller.replicas : 1
I0930 09:16:04.043713 140009111922496 base_runner.py:59] cluster.controller.targets : ''
I0930 09:16:04.043778 140009111922496 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I0930 09:16:04.043850 140009111922496 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I0930 09:16:04.043915 140009111922496 base_runner.py:59] cluster.decoder.devices_per_split : 1
I0930 09:16:04.043985 140009111922496 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I0930 09:16:04.044051 140009111922496 base_runner.py:59] cluster.decoder.name : '/job:local'
I0930 09:16:04.044119 140009111922496 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I0930 09:16:04.044183 140009111922496 base_runner.py:59] cluster.decoder.replicas : 1
I0930 09:16:04.044253 140009111922496 base_runner.py:59] cluster.decoder.targets : ''
I0930 09:16:04.044319 140009111922496 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I0930 09:16:04.044386 140009111922496 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I0930 09:16:04.044454 140009111922496 base_runner.py:59] cluster.evaler.devices_per_split : 1
I0930 09:16:04.044517 140009111922496 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I0930 09:16:04.044593 140009111922496 base_runner.py:59] cluster.evaler.name : '/job:local'
I0930 09:16:04.044652 140009111922496 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I0930 09:16:04.044724 140009111922496 base_runner.py:59] cluster.evaler.replicas : 1
I0930 09:16:04.044787 140009111922496 base_runner.py:59] cluster.evaler.targets : ''
I0930 09:16:04.044854 140009111922496 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I0930 09:16:04.044919 140009111922496 base_runner.py:59] cluster.input.cpus_per_replica : 1
I0930 09:16:04.044984 140009111922496 base_runner.py:59] cluster.input.devices_per_split : 1
I0930 09:16:04.045059 140009111922496 base_runner.py:59] cluster.input.gpus_per_replica : 0
I0930 09:16:04.045132 140009111922496 base_runner.py:59] cluster.input.name : '/job:local'
I0930 09:16:04.045211 140009111922496 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I0930 09:16:04.045272 140009111922496 base_runner.py:59] cluster.input.replicas : 0
I0930 09:16:04.045348 140009111922496 base_runner.py:59] cluster.input.targets : ''
I0930 09:16:04.045423 140009111922496 base_runner.py:59] cluster.input.tpus_per_replica : 0
I0930 09:16:04.045511 140009111922496 base_runner.py:59] cluster.job : 'controller'
I0930 09:16:04.045595 140009111922496 base_runner.py:59] cluster.logdir : ''
I0930 09:16:04.045665 140009111922496 base_runner.py:59] cluster.mode : 'sync'
I0930 09:16:04.045731 140009111922496 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I0930 09:16:04.045794 140009111922496 base_runner.py:59] cluster.ps.devices_per_split : 1
I0930 09:16:04.045873 140009111922496 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I0930 09:16:04.045935 140009111922496 base_runner.py:59] cluster.ps.name : '/job:local'
I0930 09:16:04.046009 140009111922496 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I0930 09:16:04.046068 140009111922496 base_runner.py:59] cluster.ps.replicas : 1
I0930 09:16:04.046153 140009111922496 base_runner.py:59] cluster.ps.targets : ''
I0930 09:16:04.046230 140009111922496 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I0930 09:16:04.046350 140009111922496 base_runner.py:59] cluster.task : 0
I0930 09:16:04.046438 140009111922496 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I0930 09:16:04.046517 140009111922496 base_runner.py:59] cluster.worker.devices_per_split : 1
I0930 09:16:04.046610 140009111922496 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I0930 09:16:04.046694 140009111922496 base_runner.py:59] cluster.worker.name : '/job:local'
I0930 09:16:04.046783 140009111922496 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I0930 09:16:04.046870 140009111922496 base_runner.py:59] cluster.worker.replicas : 1
I0930 09:16:04.046949 140009111922496 base_runner.py:59] cluster.worker.targets : ''
I0930 09:16:04.047036 140009111922496 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I0930 09:16:04.047115 140009111922496 base_runner.py:59] dtype : float32
I0930 09:16:04.047196 140009111922496 base_runner.py:59] fprop_dtype : NoneType
I0930 09:16:04.047282 140009111922496 base_runner.py:59] inference_driver_name : NoneType
I0930 09:16:04.047368 140009111922496 base_runner.py:59] input.allow_implicit_capture : NoneType
I0930 09:16:04.047456 140009111922496 base_runner.py:59] input.bucket_adjust_every_n : 0
I0930 09:16:04.047540 140009111922496 base_runner.py:59] input.bucket_batch_limit : [32]
I0930 09:16:04.047613 140009111922496 base_runner.py:59] input.bucket_upper_bound : [1024]
I0930 09:16:04.047680 140009111922496 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I0930 09:16:04.047746 140009111922496 base_runner.py:59] input.dtype : float32
I0930 09:16:04.047818 140009111922496 base_runner.py:59] input.file_buffer_size : 10000000
I0930 09:16:04.047902 140009111922496 base_runner.py:59] input.file_datasource : NoneType
I0930 09:16:04.047988 140009111922496 base_runner.py:59] input.file_parallelism : 10
I0930 09:16:04.048067 140009111922496 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I0930 09:16:04.048156 140009111922496 base_runner.py:59] input.file_random_seed : 301
I0930 09:16:04.048235 140009111922496 base_runner.py:59] input.fixed_input_shape : True
I0930 09:16:04.048325 140009111922496 base_runner.py:59] input.flush_every_n : 0
I0930 09:16:04.048405 140009111922496 base_runner.py:59] input.fprop_dtype : NoneType
I0930 09:16:04.048485 140009111922496 base_runner.py:59] input.inference_driver_name : NoneType
I0930 09:16:04.048569 140009111922496 base_runner.py:59] input.is_eval : NoneType
I0930 09:16:04.048651 140009111922496 base_runner.py:59] input.is_inference : NoneType
I0930 09:16:04.048738 140009111922496 base_runner.py:59] input.name : '1bwds_train_set'
I0930 09:16:04.048815 140009111922496 base_runner.py:59] input.num_batcher_threads : 16
I0930 09:16:04.048901 140009111922496 base_runner.py:59] input.num_samples : 0
I0930 09:16:04.048980 140009111922496 base_runner.py:59] input.pad_to_max_seq_length : False
I0930 09:16:04.049066 140009111922496 base_runner.py:59] input.params_init.method : 'xavier'
I0930 09:16:04.049143 140009111922496 base_runner.py:59] input.params_init.scale : 1.000001
I0930 09:16:04.049228 140009111922496 base_runner.py:59] input.params_init.seed : NoneType
I0930 09:16:04.049307 140009111922496 base_runner.py:59] input.random_seed : NoneType
I0930 09:16:04.049387 140009111922496 base_runner.py:59] input.remote.max_inflights_per_target : 32
I0930 09:16:04.049472 140009111922496 base_runner.py:59] input.remote.shardable_batch : False
I0930 09:16:04.049560 140009111922496 base_runner.py:59] input.require_sequential_order : False
I0930 09:16:04.049647 140009111922496 base_runner.py:59] input.skip_lp_regularization : NoneType
I0930 09:16:04.049726 140009111922496 base_runner.py:59] input.source_max_length : NoneType
I0930 09:16:04.049813 140009111922496 base_runner.py:59] input.target_max_length : 1024
I0930 09:16:04.049891 140009111922496 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I0930 09:16:04.049979 140009111922496 base_runner.py:59] input.tokenizer.append_eos : True
I0930 09:16:04.050065 140009111922496 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I0930 09:16:04.050151 140009111922496 base_runner.py:59] input.tokenizer.dtype : float32
I0930 09:16:04.050244 140009111922496 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I0930 09:16:04.050351 140009111922496 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I0930 09:16:04.050431 140009111922496 base_runner.py:59] input.tokenizer.is_eval : NoneType
I0930 09:16:04.050517 140009111922496 base_runner.py:59] input.tokenizer.is_inference : NoneType
I0930 09:16:04.050594 140009111922496 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I0930 09:16:04.050681 140009111922496 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I0930 09:16:04.050761 140009111922496 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I0930 09:16:04.050847 140009111922496 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I0930 09:16:04.050924 140009111922496 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I0930 09:16:04.051005 140009111922496 base_runner.py:59] input.tokenizer.random_seed : NoneType
I0930 09:16:04.051088 140009111922496 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I0930 09:16:04.051174 140009111922496 base_runner.py:59] input.tokenizer.target_eos_id : 2
I0930 09:16:04.051260 140009111922496 base_runner.py:59] input.tokenizer.target_sos_id : 1
I0930 09:16:04.051344 140009111922496 base_runner.py:59] input.tokenizer.target_unk_id : 0
I0930 09:16:04.051431 140009111922496 base_runner.py:59] input.tokenizer.vn.global_vn : False
I0930 09:16:04.051514 140009111922496 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I0930 09:16:04.051601 140009111922496 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I0930 09:16:04.051685 140009111922496 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I0930 09:16:04.051749 140009111922496 base_runner.py:59] input.tokenizer.vocab_size : 32000
I0930 09:16:04.051821 140009111922496 base_runner.py:59] input.tokenizer_dict : {}
I0930 09:16:04.051882 140009111922496 base_runner.py:59] input.tpu_infeed_parallelism : 1
I0930 09:16:04.051958 140009111922496 base_runner.py:59] input.use_chaining : False
I0930 09:16:04.052021 140009111922496 base_runner.py:59] input.use_per_host_infeed : False
I0930 09:16:04.052094 140009111922496 base_runner.py:59] input.use_within_batch_mixing : False
I0930 09:16:04.052161 140009111922496 base_runner.py:59] input.vn.global_vn : False
I0930 09:16:04.052230 140009111922496 base_runner.py:59] input.vn.per_step_vn : False
I0930 09:16:04.052300 140009111922496 base_runner.py:59] input.vn.scale : NoneType
I0930 09:16:04.052365 140009111922496 base_runner.py:59] input.vn.seed : NoneType
I0930 09:16:04.052448 140009111922496 base_runner.py:59] is_eval : NoneType
I0930 09:16:04.052533 140009111922496 base_runner.py:59] is_inference : NoneType
I0930 09:16:04.052608 140009111922496 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:187'
I0930 09:16:04.052674 140009111922496 base_runner.py:59] name : ''
I0930 09:16:04.052742 140009111922496 base_runner.py:59] params_init.method : 'xavier'
I0930 09:16:04.052798 140009111922496 base_runner.py:59] params_init.scale : 1.000001
I0930 09:16:04.052882 140009111922496 base_runner.py:59] params_init.seed : NoneType
I0930 09:16:04.052961 140009111922496 base_runner.py:59] random_seed : NoneType
I0930 09:16:04.053046 140009111922496 base_runner.py:59] skip_lp_regularization : NoneType
I0930 09:16:04.053131 140009111922496 base_runner.py:59] task.allow_implicit_capture : NoneType
I0930 09:16:04.053210 140009111922496 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I0930 09:16:04.053263 140009111922496 base_runner.py:59] task.decoder : NoneType
I0930 09:16:04.053331 140009111922496 base_runner.py:59] task.dtype : float32
I0930 09:16:04.053399 140009111922496 base_runner.py:59] task.encoder : NoneType
I0930 09:16:04.053465 140009111922496 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I0930 09:16:04.053541 140009111922496 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I0930 09:16:04.053608 140009111922496 base_runner.py:59] task.eval.samples_per_summary : 0
I0930 09:16:04.053691 140009111922496 base_runner.py:59] task.eval.start_decoder_after : 0
I0930 09:16:04.053771 140009111922496 base_runner.py:59] task.eval.start_eval_after : 0
I0930 09:16:04.053858 140009111922496 base_runner.py:59] task.fprop_dtype : NoneType
I0930 09:16:04.053934 140009111922496 base_runner.py:59] task.inference_driver_name : NoneType
I0930 09:16:04.054021 140009111922496 base_runner.py:59] task.input : NoneType
I0930 09:16:04.054105 140009111922496 base_runner.py:59] task.is_eval : NoneType
I0930 09:16:04.054175 140009111922496 base_runner.py:59] task.is_inference : NoneType
I0930 09:16:04.054242 140009111922496 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I0930 09:16:04.054321 140009111922496 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I0930 09:16:04.054389 140009111922496 base_runner.py:59] task.lm.dtype : float32
I0930 09:16:04.054461 140009111922496 base_runner.py:59] task.lm.fprop_dtype : NoneType
I0930 09:16:04.054547 140009111922496 base_runner.py:59] task.lm.inference_driver_name : NoneType
I0930 09:16:04.054632 140009111922496 base_runner.py:59] task.lm.is_eval : NoneType
I0930 09:16:04.054702 140009111922496 base_runner.py:59] task.lm.is_inference : NoneType
I0930 09:16:04.054764 140009111922496 base_runner.py:59] task.lm.name : 'transformerlm'
I0930 09:16:04.054832 140009111922496 base_runner.py:59] task.lm.params_init.method : 'xavier'
I0930 09:16:04.054905 140009111922496 base_runner.py:59] task.lm.params_init.scale : 1.000001
I0930 09:16:04.054968 140009111922496 base_runner.py:59] task.lm.params_init.seed : NoneType
I0930 09:16:04.055045 140009111922496 base_runner.py:59] task.lm.random_seed : NoneType
I0930 09:16:04.055108 140009111922496 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I0930 09:16:04.055186 140009111922496 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I0930 09:16:04.055240 140009111922496 base_runner.py:59] task.lm.stack.batch_dim : 1
I0930 09:16:04.055325 140009111922496 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I0930 09:16:04.055409 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.055480 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 09:16:04.055548 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I0930 09:16:04.055617 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I0930 09:16:04.055683 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I0930 09:16:04.055769 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I0930 09:16:04.055853 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I0930 09:16:04.055917 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I0930 09:16:04.055974 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I0930 09:16:04.056045 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I0930 09:16:04.056122 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I0930 09:16:04.056174 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.056249 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:16:04.056319 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I0930 09:16:04.056388 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I0930 09:16:04.056456 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:16:04.056524 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:16:04.056606 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I0930 09:16:04.056668 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I0930 09:16:04.056746 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I0930 09:16:04.056808 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I0930 09:16:04.056881 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:16:04.056948 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:16:04.057016 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:16:04.057084 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I0930 09:16:04.057169 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.057253 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I0930 09:16:04.057322 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:16:04.057385 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I0930 09:16:04.057447 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I0930 09:16:04.057525 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I0930 09:16:04.057586 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I0930 09:16:04.057661 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I0930 09:16:04.057730 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I0930 09:16:04.057799 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I0930 09:16:04.057866 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I0930 09:16:04.057936 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I0930 09:16:04.058008 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I0930 09:16:04.058073 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I0930 09:16:04.058139 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.058208 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I0930 09:16:04.058289 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 09:16:04.058363 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.058419 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 09:16:04.058498 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 09:16:04.058551 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.058624 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 09:16:04.058686 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 09:16:04.058752 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 09:16:04.058821 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 09:16:04.058886 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 09:16:04.058955 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 09:16:04.059027 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 09:16:04.059097 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I0930 09:16:04.059162 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 09:16:04.059233 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 09:16:04.059286 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 09:16:04.059361 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 09:16:04.059422 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 09:16:04.059489 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 09:16:04.059558 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 09:16:04.059632 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 09:16:04.059695 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 09:16:04.059761 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 09:16:04.059830 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 09:16:04.059904 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 09:16:04.059967 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 09:16:04.060030 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 09:16:04.060102 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 09:16:04.060164 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 09:16:04.060236 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 09:16:04.060294 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 09:16:04.060371 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 09:16:04.060428 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 09:16:04.060501 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 09:16:04.060568 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 09:16:04.060640 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 09:16:04.060693 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 09:16:04.060765 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 09:16:04.060828 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 09:16:04.060912 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 09:16:04.060981 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 09:16:04.061053 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 09:16:04.061114 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 09:16:04.061186 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 09:16:04.061239 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 09:16:04.061309 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 09:16:04.061373 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 09:16:04.061435 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 09:16:04.061507 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 09:16:04.061562 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 09:16:04.061640 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 09:16:04.061693 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 09:16:04.061763 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 09:16:04.061832 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 09:16:04.061908 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 09:16:04.061977 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 09:16:04.062046 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 09:16:04.062106 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 09:16:04.062178 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.062232 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 09:16:04.062324 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 09:16:04.062379 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 09:16:04.062452 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 09:16:04.062519 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 09:16:04.062596 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 09:16:04.062651 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 09:16:04.062723 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I0930 09:16:04.062783 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I0930 09:16:04.062850 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 09:16:04.062921 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 09:16:04.062981 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 09:16:04.063065 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 09:16:04.063148 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I0930 09:16:04.063234 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.063318 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:16:04.063404 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 09:16:04.063487 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 09:16:04.063565 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:16:04.063634 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:16:04.063698 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 09:16:04.063765 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 09:16:04.063838 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 09:16:04.063907 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 09:16:04.063976 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:16:04.064049 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:16:04.064116 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:16:04.064193 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 09:16:04.064249 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.064321 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 09:16:04.064381 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:16:04.064448 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 09:16:04.064517 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 09:16:04.064594 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 09:16:04.064651 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I0930 09:16:04.064719 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I0930 09:16:04.064787 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I0930 09:16:04.064852 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 09:16:04.064921 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 09:16:04.064992 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 09:16:04.065061 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 09:16:04.065133 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 09:16:04.065196 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.065274 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:16:04.065334 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 09:16:04.065412 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 09:16:04.065479 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 09:16:04.065552 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 09:16:04.065611 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 09:16:04.065682 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 09:16:04.065739 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 09:16:04.065818 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 09:16:04.065873 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 09:16:04.065948 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:16:04.066015 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 09:16:04.066091 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 09:16:04.066158 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 09:16:04.066227 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 09:16:04.066308 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.066379 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 09:16:04.066439 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 09:16:04.066514 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 09:16:04.066582 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 09:16:04.066651 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.066708 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I0930 09:16:04.066783 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 09:16:04.066835 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 09:16:04.066913 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 09:16:04.066982 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 09:16:04.067052 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I0930 09:16:04.067109 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 09:16:04.067181 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.067244 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 09:16:04.067321 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 09:16:04.067379 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 09:16:04.067429 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.067476 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 09:16:04.067531 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 09:16:04.067606 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 09:16:04.067661 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 09:16:04.067734 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:16:04.067804 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 09:16:04.067877 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 09:16:04.067932 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 09:16:04.068006 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 09:16:04.068067 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 09:16:04.068138 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 09:16:04.068206 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 09:16:04.068273 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 09:16:04.068344 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 09:16:04.068402 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 09:16:04.068480 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 09:16:04.068547 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 09:16:04.068616 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 09:16:04.068675 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 09:16:04.068749 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 09:16:04.068817 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 09:16:04.068885 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 09:16:04.068954 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 09:16:04.069021 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 09:16:04.069087 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 09:16:04.069156 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 09:16:04.069216 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 09:16:04.069294 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 09:16:04.069349 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 09:16:04.069423 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 09:16:04.069484 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 09:16:04.069550 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 09:16:04.069619 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 09:16:04.069678 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 09:16:04.069752 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 09:16:04.069812 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 09:16:04.069885 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 09:16:04.069939 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 09:16:04.070012 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 09:16:04.070079 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 09:16:04.070158 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 09:16:04.070211 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 09:16:04.070312 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 09:16:04.070372 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 09:16:04.070445 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 09:16:04.070513 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 09:16:04.070586 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 09:16:04.070645 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 09:16:04.070718 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 09:16:04.070775 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 09:16:04.070845 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 09:16:04.070915 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 09:16:04.070987 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 09:16:04.071045 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 09:16:04.071110 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 09:16:04.071178 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 09:16:04.071245 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 09:16:04.071316 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 09:16:04.071379 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 09:16:04.071450 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 09:16:04.071507 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 09:16:04.071580 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 09:16:04.071639 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 09:16:04.071707 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 09:16:04.071775 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.071854 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 09:16:04.071909 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 09:16:04.071981 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 09:16:04.072049 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 09:16:04.072128 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 09:16:04.072181 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 09:16:04.072252 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I0930 09:16:04.072317 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 09:16:04.072378 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 09:16:04.072450 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 09:16:04.072511 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 09:16:04.072584 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.072638 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:16:04.072712 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 09:16:04.072775 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 09:16:04.072842 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:16:04.072910 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:16:04.072968 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 09:16:04.073047 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 09:16:04.073117 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 09:16:04.073184 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 09:16:04.073261 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:16:04.073325 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:16:04.073397 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:16:04.073462 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 09:16:04.073525 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.073596 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 09:16:04.073655 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:16:04.073726 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 09:16:04.073780 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 09:16:04.073847 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I0930 09:16:04.073916 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 09:16:04.073982 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 09:16:04.074054 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 09:16:04.074106 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 09:16:04.074184 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 09:16:04.074240 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 09:16:04.074330 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 09:16:04.074388 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 09:16:04.074459 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.074524 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 09:16:04.074586 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 09:16:04.074656 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 09:16:04.074719 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 09:16:04.074790 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 09:16:04.074843 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 09:16:04.074918 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 09:16:04.074986 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 09:16:04.075061 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 09:16:04.075120 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 09:16:04.075192 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 09:16:04.075247 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 09:16:04.075319 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 09:16:04.075389 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 09:16:04.075466 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 09:16:04.075519 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 09:16:04.075595 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 09:16:04.075655 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 09:16:04.075721 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.075791 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 09:16:04.075864 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 09:16:04.075923 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 09:16:04.075989 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 09:16:04.076058 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 09:16:04.076114 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 09:16:04.076190 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.076247 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:16:04.076319 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 09:16:04.076379 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 09:16:04.076446 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 09:16:04.076513 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 09:16:04.076578 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 09:16:04.076646 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 09:16:04.076707 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 09:16:04.076780 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 09:16:04.076835 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 09:16:04.076909 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:16:04.076969 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 09:16:04.077037 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 09:16:04.077108 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 09:16:04.077181 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 09:16:04.077249 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.077322 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 09:16:04.077380 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 09:16:04.077457 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 09:16:04.077523 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 09:16:04.077596 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.077663 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 09:16:04.077730 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 09:16:04.077796 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 09:16:04.077864 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 09:16:04.077923 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I0930 09:16:04.077996 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I0930 09:16:04.078051 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I0930 09:16:04.078123 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I0930 09:16:04.078185 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I0930 09:16:04.078253 140009111922496 base_runner.py:59] task.lm.stack.dtype : float32
I0930 09:16:04.078340 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I0930 09:16:04.078417 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.078485 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I0930 09:16:04.078557 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I0930 09:16:04.078620 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I0930 09:16:04.078691 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.078746 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:16:04.078819 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I0930 09:16:04.078880 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I0930 09:16:04.078947 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 09:16:04.079015 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 09:16:04.079079 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I0930 09:16:04.079148 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I0930 09:16:04.079216 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I0930 09:16:04.079284 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I0930 09:16:04.079342 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I0930 09:16:04.079418 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:16:04.079486 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 09:16:04.079555 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 09:16:04.079608 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I0930 09:16:04.079686 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I0930 09:16:04.079761 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.079831 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I0930 09:16:04.079886 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I0930 09:16:04.079964 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I0930 09:16:04.080019 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I0930 09:16:04.080092 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I0930 09:16:04.080152 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I0930 09:16:04.080220 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I0930 09:16:04.080289 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I0930 09:16:04.080353 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I0930 09:16:04.080424 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I0930 09:16:04.080481 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I0930 09:16:04.080553 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I0930 09:16:04.080612 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I0930 09:16:04.080678 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I0930 09:16:04.080746 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I0930 09:16:04.080812 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I0930 09:16:04.080880 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I0930 09:16:04.080940 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I0930 09:16:04.081013 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I0930 09:16:04.081067 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I0930 09:16:04.081139 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I0930 09:16:04.081207 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I0930 09:16:04.081285 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I0930 09:16:04.081336 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I0930 09:16:04.081408 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I0930 09:16:04.081471 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I0930 09:16:04.081538 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I0930 09:16:04.081605 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I0930 09:16:04.081669 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I0930 09:16:04.081739 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I0930 09:16:04.081808 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I0930 09:16:04.081877 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I0930 09:16:04.081944 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I0930 09:16:04.082013 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I0930 09:16:04.082079 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I0930 09:16:04.082152 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I0930 09:16:04.082212 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I0930 09:16:04.082306 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I0930 09:16:04.082384 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I0930 09:16:04.082454 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I0930 09:16:04.082527 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I0930 09:16:04.082581 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.082653 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I0930 09:16:04.082721 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I0930 09:16:04.082792 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I0930 09:16:04.082851 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I0930 09:16:04.082918 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I0930 09:16:04.082986 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I0930 09:16:04.083052 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I0930 09:16:04.083121 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I0930 09:16:04.083183 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I0930 09:16:04.083256 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I0930 09:16:04.083309 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I0930 09:16:04.083381 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I0930 09:16:04.083445 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I0930 09:16:04.083512 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I0930 09:16:04.083580 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I0930 09:16:04.083644 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I0930 09:16:04.083715 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I0930 09:16:04.083772 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I0930 09:16:04.083844 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I0930 09:16:04.083911 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I0930 09:16:04.083985 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I0930 09:16:04.084052 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I0930 09:16:04.084122 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I0930 09:16:04.084189 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I0930 09:16:04.084257 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I0930 09:16:04.084325 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I0930 09:16:04.084393 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I0930 09:16:04.084457 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I0930 09:16:04.084527 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.084584 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 09:16:04.084664 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I0930 09:16:04.084731 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I0930 09:16:04.084805 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I0930 09:16:04.084865 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I0930 09:16:04.084939 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I0930 09:16:04.085005 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I0930 09:16:04.085076 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I0930 09:16:04.085140 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I0930 09:16:04.085211 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I0930 09:16:04.085269 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.085342 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:16:04.085400 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I0930 09:16:04.085467 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I0930 09:16:04.085534 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:16:04.085600 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:16:04.085668 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I0930 09:16:04.085736 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I0930 09:16:04.085805 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I0930 09:16:04.085875 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I0930 09:16:04.085940 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:16:04.086002 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:16:04.086074 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:16:04.086143 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I0930 09:16:04.086212 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.086288 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I0930 09:16:04.086361 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:16:04.086427 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I0930 09:16:04.086496 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I0930 09:16:04.086556 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I0930 09:16:04.086629 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I0930 09:16:04.086684 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I0930 09:16:04.086754 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I0930 09:16:04.086818 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I0930 09:16:04.086881 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I0930 09:16:04.086952 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I0930 09:16:04.087025 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I0930 09:16:04.087089 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I0930 09:16:04.087160 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.087230 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I0930 09:16:04.087291 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 09:16:04.087364 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.087425 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 09:16:04.087498 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 09:16:04.087553 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.087627 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 09:16:04.087689 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 09:16:04.087756 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 09:16:04.087826 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 09:16:04.087893 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 09:16:04.087962 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 09:16:04.088016 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 09:16:04.088095 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I0930 09:16:04.088148 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 09:16:04.088224 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 09:16:04.088285 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 09:16:04.088351 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 09:16:04.088424 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 09:16:04.088499 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 09:16:04.088563 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 09:16:04.088636 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 09:16:04.088704 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 09:16:04.088778 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 09:16:04.088845 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 09:16:04.088914 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 09:16:04.088968 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 09:16:04.089046 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 09:16:04.089113 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 09:16:04.089187 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 09:16:04.089253 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 09:16:04.089322 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 09:16:04.089389 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 09:16:04.089458 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 09:16:04.089525 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 09:16:04.089593 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 09:16:04.089653 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 09:16:04.089727 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 09:16:04.089785 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 09:16:04.089857 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 09:16:04.089914 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 09:16:04.089983 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 09:16:04.090049 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 09:16:04.090116 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 09:16:04.090184 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 09:16:04.090247 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 09:16:04.090333 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 09:16:04.090395 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 09:16:04.090468 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 09:16:04.090522 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 09:16:04.090593 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 09:16:04.090656 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 09:16:04.090722 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 09:16:04.090791 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 09:16:04.090862 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 09:16:04.090926 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 09:16:04.090994 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 09:16:04.091063 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 09:16:04.091145 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 09:16:04.091205 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.091272 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 09:16:04.091339 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 09:16:04.091411 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 09:16:04.091473 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 09:16:04.091536 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 09:16:04.091607 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 09:16:04.091683 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 09:16:04.091743 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I0930 09:16:04.091811 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I0930 09:16:04.091880 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 09:16:04.091940 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 09:16:04.092014 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 09:16:04.092079 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 09:16:04.092149 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I0930 09:16:04.092205 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.092277 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:16:04.092335 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 09:16:04.092401 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 09:16:04.092468 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:16:04.092527 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:16:04.092601 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 09:16:04.092660 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 09:16:04.092731 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 09:16:04.092787 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 09:16:04.092858 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:16:04.092922 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:16:04.092989 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:16:04.093058 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 09:16:04.093120 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.093191 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 09:16:04.093264 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:16:04.093333 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 09:16:04.093402 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 09:16:04.093470 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 09:16:04.093530 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I0930 09:16:04.093604 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I0930 09:16:04.093666 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I0930 09:16:04.093738 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 09:16:04.093805 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 09:16:04.093876 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 09:16:04.093938 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 09:16:04.094009 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 09:16:04.094076 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.094144 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:16:04.094211 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 09:16:04.094300 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 09:16:04.094374 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 09:16:04.094438 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 09:16:04.094505 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 09:16:04.094573 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 09:16:04.094639 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 09:16:04.094707 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 09:16:04.094759 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 09:16:04.094838 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:16:04.094900 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 09:16:04.094972 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 09:16:04.095027 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 09:16:04.095099 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 09:16:04.095161 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.095227 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 09:16:04.095295 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 09:16:04.095373 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 09:16:04.095436 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 09:16:04.095503 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.095572 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I0930 09:16:04.095639 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 09:16:04.095707 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 09:16:04.095767 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 09:16:04.095839 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 09:16:04.095894 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I0930 09:16:04.095965 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 09:16:04.096029 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.096091 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 09:16:04.096162 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 09:16:04.096225 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 09:16:04.096296 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.096348 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 09:16:04.096423 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 09:16:04.096491 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 09:16:04.096565 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 09:16:04.096632 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:16:04.096700 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 09:16:04.096760 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 09:16:04.096833 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 09:16:04.096901 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 09:16:04.096970 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 09:16:04.097035 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 09:16:04.097103 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 09:16:04.097162 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 09:16:04.097234 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 09:16:04.097289 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 09:16:04.097369 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 09:16:04.097437 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 09:16:04.097509 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 09:16:04.097576 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 09:16:04.097644 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 09:16:04.097709 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 09:16:04.097778 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 09:16:04.097830 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 09:16:04.097906 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 09:16:04.097962 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 09:16:04.098031 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 09:16:04.098100 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 09:16:04.098171 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 09:16:04.098237 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 09:16:04.098323 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 09:16:04.098390 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 09:16:04.098459 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 09:16:04.098529 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 09:16:04.098598 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 09:16:04.098659 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 09:16:04.098732 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 09:16:04.098792 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 09:16:04.098864 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 09:16:04.098920 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 09:16:04.098992 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 09:16:04.099056 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 09:16:04.099119 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 09:16:04.099190 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 09:16:04.099254 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 09:16:04.099328 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 09:16:04.099387 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 09:16:04.099459 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 09:16:04.099515 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 09:16:04.099585 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 09:16:04.099652 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 09:16:04.099731 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 09:16:04.099786 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 09:16:04.099856 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 09:16:04.099925 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 09:16:04.099996 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 09:16:04.100054 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 09:16:04.100119 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 09:16:04.100186 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 09:16:04.100262 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 09:16:04.100330 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 09:16:04.100402 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 09:16:04.100463 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 09:16:04.100535 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 09:16:04.100603 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 09:16:04.100671 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.100730 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 09:16:04.100801 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 09:16:04.100867 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 09:16:04.100940 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 09:16:04.100999 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 09:16:04.101071 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 09:16:04.101126 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I0930 09:16:04.101196 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 09:16:04.101261 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 09:16:04.101330 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 09:16:04.101399 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 09:16:04.101452 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.101531 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:16:04.101585 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 09:16:04.101658 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 09:16:04.101722 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:16:04.101787 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:16:04.101856 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 09:16:04.101920 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 09:16:04.101989 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 09:16:04.102047 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 09:16:04.102120 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:16:04.102189 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:16:04.102272 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:16:04.102337 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 09:16:04.102409 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.102462 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 09:16:04.102532 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:16:04.102597 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 09:16:04.102664 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 09:16:04.102733 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I0930 09:16:04.102802 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 09:16:04.102870 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 09:16:04.102949 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 09:16:04.103004 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 09:16:04.103074 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 09:16:04.103147 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 09:16:04.103224 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 09:16:04.103291 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 09:16:04.103360 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.103422 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 09:16:04.103497 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 09:16:04.103556 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 09:16:04.103628 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 09:16:04.103687 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 09:16:04.103754 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 09:16:04.103821 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 09:16:04.103886 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 09:16:04.103955 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 09:16:04.104022 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 09:16:04.104090 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 09:16:04.104156 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 09:16:04.104225 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 09:16:04.104285 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 09:16:04.104356 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 09:16:04.104412 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 09:16:04.104480 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 09:16:04.104547 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 09:16:04.104613 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.104682 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 09:16:04.104734 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 09:16:04.104810 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 09:16:04.104893 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 09:16:04.104976 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 09:16:04.105062 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 09:16:04.105146 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.105214 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:16:04.105271 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 09:16:04.105339 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 09:16:04.105413 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 09:16:04.105482 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 09:16:04.105560 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 09:16:04.105628 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 09:16:04.105696 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 09:16:04.105756 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 09:16:04.105830 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 09:16:04.105898 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:16:04.105966 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 09:16:04.106018 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 09:16:04.106095 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 09:16:04.106151 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 09:16:04.106222 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.106307 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 09:16:04.106383 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 09:16:04.106443 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 09:16:04.106510 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 09:16:04.106577 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.106638 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 09:16:04.106711 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 09:16:04.106768 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 09:16:04.106845 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 09:16:04.106903 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I0930 09:16:04.106975 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I0930 09:16:04.107043 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I0930 09:16:04.107114 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I0930 09:16:04.107182 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I0930 09:16:04.107250 140009111922496 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I0930 09:16:04.107313 140009111922496 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I0930 09:16:04.107385 140009111922496 base_runner.py:59] task.lm.stack.is_eval : NoneType
I0930 09:16:04.107442 140009111922496 base_runner.py:59] task.lm.stack.is_inference : NoneType
I0930 09:16:04.107515 140009111922496 base_runner.py:59] task.lm.stack.is_transparent : False
I0930 09:16:04.107572 140009111922496 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I0930 09:16:04.107639 140009111922496 base_runner.py:59] task.lm.stack.model_dim : 2048
I0930 09:16:04.107708 140009111922496 base_runner.py:59] task.lm.stack.name : ''
I0930 09:16:04.107787 140009111922496 base_runner.py:59] task.lm.stack.normalize_encoder : False
I0930 09:16:04.107846 140009111922496 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I0930 09:16:04.107923 140009111922496 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I0930 09:16:04.107982 140009111922496 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I0930 09:16:04.108048 140009111922496 base_runner.py:59] task.lm.stack.packed_input : False
I0930 09:16:04.108116 140009111922496 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I0930 09:16:04.108177 140009111922496 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I0930 09:16:04.108250 140009111922496 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I0930 09:16:04.108307 140009111922496 base_runner.py:59] task.lm.stack.random_seed : NoneType
I0930 09:16:04.108384 140009111922496 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I0930 09:16:04.108442 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.108513 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I0930 09:16:04.108576 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I0930 09:16:04.108644 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I0930 09:16:04.108712 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I0930 09:16:04.108770 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I0930 09:16:04.108844 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I0930 09:16:04.108902 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I0930 09:16:04.108974 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I0930 09:16:04.109030 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I0930 09:16:04.109100 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I0930 09:16:04.109167 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I0930 09:16:04.109246 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I0930 09:16:04.109300 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I0930 09:16:04.109373 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I0930 09:16:04.109435 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I0930 09:16:04.109501 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I0930 09:16:04.109569 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I0930 09:16:04.109626 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I0930 09:16:04.109700 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I0930 09:16:04.109769 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I0930 09:16:04.109836 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.109889 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I0930 09:16:04.109968 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I0930 09:16:04.110022 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I0930 09:16:04.110094 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I0930 09:16:04.110162 140009111922496 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I0930 09:16:04.110233 140009111922496 base_runner.py:59] task.lm.stack.state_dtype : float32
I0930 09:16:04.110306 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I0930 09:16:04.110386 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.110448 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I0930 09:16:04.110522 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 09:16:04.110582 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I0930 09:16:04.110649 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I0930 09:16:04.110717 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I0930 09:16:04.110794 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 09:16:04.110851 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 09:16:04.110918 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I0930 09:16:04.110985 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I0930 09:16:04.111043 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I0930 09:16:04.111115 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I0930 09:16:04.111172 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I0930 09:16:04.111243 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:16:04.111310 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 09:16:04.111382 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 09:16:04.111435 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I0930 09:16:04.111507 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I0930 09:16:04.111576 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.111648 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I0930 09:16:04.111715 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I0930 09:16:04.111783 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I0930 09:16:04.111849 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I0930 09:16:04.111917 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I0930 09:16:04.111990 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I0930 09:16:04.112053 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I0930 09:16:04.112119 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I0930 09:16:04.112187 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I0930 09:16:04.112259 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I0930 09:16:04.112323 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I0930 09:16:04.112389 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I0930 09:16:04.112457 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I0930 09:16:04.112508 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I0930 09:16:04.112591 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I0930 09:16:04.112646 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I0930 09:16:04.112717 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I0930 09:16:04.112780 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I0930 09:16:04.112845 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I0930 09:16:04.112914 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I0930 09:16:04.112974 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I0930 09:16:04.113048 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I0930 09:16:04.113114 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I0930 09:16:04.113183 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I0930 09:16:04.113247 140009111922496 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I0930 09:16:04.113317 140009111922496 base_runner.py:59] task.lm.stack.vn.global_vn : False
I0930 09:16:04.113368 140009111922496 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I0930 09:16:04.113444 140009111922496 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I0930 09:16:04.113503 140009111922496 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I0930 09:16:04.113570 140009111922496 base_runner.py:59] task.lm.vn.global_vn : False
I0930 09:16:04.113638 140009111922496 base_runner.py:59] task.lm.vn.per_step_vn : False
I0930 09:16:04.113698 140009111922496 base_runner.py:59] task.lm.vn.scale : NoneType
I0930 09:16:04.113771 140009111922496 base_runner.py:59] task.lm.vn.seed : NoneType
I0930 09:16:04.113838 140009111922496 base_runner.py:59] task.lm.vocab_size : 32000
I0930 09:16:04.113906 140009111922496 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I0930 09:16:04.113976 140009111922496 base_runner.py:59] task.online_encoder : NoneType
I0930 09:16:04.114040 140009111922496 base_runner.py:59] task.params_init.method : 'xavier'
I0930 09:16:04.114107 140009111922496 base_runner.py:59] task.params_init.scale : 1.000001
I0930 09:16:04.114176 140009111922496 base_runner.py:59] task.params_init.seed : NoneType
I0930 09:16:04.114250 140009111922496 base_runner.py:59] task.random_seed : NoneType
I0930 09:16:04.114323 140009111922496 base_runner.py:59] task.skip_lp_regularization : NoneType
I0930 09:16:04.114396 140009111922496 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I0930 09:16:04.114454 140009111922496 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I0930 09:16:04.114526 140009111922496 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I0930 09:16:04.114585 140009111922496 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I0930 09:16:04.114651 140009111922496 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I0930 09:16:04.114717 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I0930 09:16:04.114787 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I0930 09:16:04.114851 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I0930 09:16:04.114918 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I0930 09:16:04.114983 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I0930 09:16:04.115048 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I0930 09:16:04.115113 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I0930 09:16:04.115183 140009111922496 base_runner.py:59] task.train.early_stop.min_steps : 0
I0930 09:16:04.115242 140009111922496 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I0930 09:16:04.115312 140009111922496 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I0930 09:16:04.115370 140009111922496 base_runner.py:59] task.train.early_stop.verbose : True
I0930 09:16:04.115435 140009111922496 base_runner.py:59] task.train.early_stop.window : 0
I0930 09:16:04.115494 140009111922496 base_runner.py:59] task.train.ema_decay : 0.0
I0930 09:16:04.115557 140009111922496 base_runner.py:59] task.train.enqueue_max_steps : -1
I0930 09:16:04.115622 140009111922496 base_runner.py:59] task.train.gate_gradients : False
I0930 09:16:04.115683 140009111922496 base_runner.py:59] task.train.grad_aggregation_method : 1
I0930 09:16:04.115749 140009111922496 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I0930 09:16:04.115811 140009111922496 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I0930 09:16:04.115876 140009111922496 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I0930 09:16:04.115940 140009111922496 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I0930 09:16:04.116006 140009111922496 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I0930 09:16:04.116071 140009111922496 base_runner.py:59] task.train.learner : NoneType
I0930 09:16:04.116139 140009111922496 base_runner.py:59] task.train.learning_rate : 0.5
I0930 09:16:04.116204 140009111922496 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I0930 09:16:04.116270 140009111922496 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 09:16:04.116349 140009111922496 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I0930 09:16:04.116405 140009111922496 base_runner.py:59] task.train.lr_schedule.dtype : float32
I0930 09:16:04.116472 140009111922496 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I0930 09:16:04.116540 140009111922496 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I0930 09:16:04.116607 140009111922496 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I0930 09:16:04.116659 140009111922496 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I0930 09:16:04.116736 140009111922496 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I0930 09:16:04.116791 140009111922496 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I0930 09:16:04.116859 140009111922496 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I0930 09:16:04.116926 140009111922496 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I0930 09:16:04.116988 140009111922496 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I0930 09:16:04.117060 140009111922496 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I0930 09:16:04.117121 140009111922496 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I0930 09:16:04.117193 140009111922496 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I0930 09:16:04.117247 140009111922496 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I0930 09:16:04.117325 140009111922496 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I0930 09:16:04.117380 140009111922496 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I0930 09:16:04.117452 140009111922496 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I0930 09:16:04.117515 140009111922496 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I0930 09:16:04.117580 140009111922496 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I0930 09:16:04.117648 140009111922496 base_runner.py:59] task.train.max_steps : 4000000
I0930 09:16:04.117714 140009111922496 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I0930 09:16:04.117784 140009111922496 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I0930 09:16:04.117856 140009111922496 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I0930 09:16:04.117925 140009111922496 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 09:16:04.117986 140009111922496 base_runner.py:59] task.train.optimizer.dtype : float32
I0930 09:16:04.118058 140009111922496 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I0930 09:16:04.118129 140009111922496 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I0930 09:16:04.118195 140009111922496 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I0930 09:16:04.118282 140009111922496 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I0930 09:16:04.118350 140009111922496 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I0930 09:16:04.118428 140009111922496 base_runner.py:59] task.train.optimizer.name : 'Adam'
I0930 09:16:04.118487 140009111922496 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I0930 09:16:04.118558 140009111922496 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I0930 09:16:04.118627 140009111922496 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I0930 09:16:04.118706 140009111922496 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I0930 09:16:04.118775 140009111922496 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I0930 09:16:04.118845 140009111922496 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I0930 09:16:04.118898 140009111922496 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I0930 09:16:04.118973 140009111922496 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I0930 09:16:04.119042 140009111922496 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I0930 09:16:04.119113 140009111922496 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I0930 09:16:04.119174 140009111922496 base_runner.py:59] task.train.save_interval_seconds : 600
I0930 09:16:04.119248 140009111922496 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I0930 09:16:04.119315 140009111922496 base_runner.py:59] task.train.save_max_to_keep : 100
I0930 09:16:04.119384 140009111922496 base_runner.py:59] task.train.start_up_delay_steps : 200
I0930 09:16:04.119446 140009111922496 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I0930 09:16:04.119517 140009111922496 base_runner.py:59] task.train.summary_interval_steps : 100
I0930 09:16:04.119585 140009111922496 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I0930 09:16:04.119653 140009111922496 base_runner.py:59] task.train.vn_start_step : 20000
I0930 09:16:04.119714 140009111922496 base_runner.py:59] task.train.vn_std : 0.0
I0930 09:16:04.119786 140009111922496 base_runner.py:59] task.vn.global_vn : False
I0930 09:16:04.119841 140009111922496 base_runner.py:59] task.vn.per_step_vn : False
I0930 09:16:04.119913 140009111922496 base_runner.py:59] task.vn.scale : NoneType
I0930 09:16:04.119976 140009111922496 base_runner.py:59] task.vn.seed : NoneType
I0930 09:16:04.120041 140009111922496 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I0930 09:16:04.120110 140009111922496 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I0930 09:16:04.120171 140009111922496 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I0930 09:16:04.120243 140009111922496 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I0930 09:16:04.120296 140009111922496 base_runner.py:59] train.early_stop.metric_history.minimize : True
I0930 09:16:04.120368 140009111922496 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I0930 09:16:04.120431 140009111922496 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I0930 09:16:04.120498 140009111922496 base_runner.py:59] train.early_stop.min_steps : 0
I0930 09:16:04.120566 140009111922496 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I0930 09:16:04.120648 140009111922496 base_runner.py:59] train.early_stop.tolerance : 0.0
I0930 09:16:04.120707 140009111922496 base_runner.py:59] train.early_stop.verbose : True
I0930 09:16:04.120777 140009111922496 base_runner.py:59] train.early_stop.window : 0
I0930 09:16:04.120843 140009111922496 base_runner.py:59] train.ema_decay : 0.0
I0930 09:16:04.120905 140009111922496 base_runner.py:59] train.enqueue_max_steps : -1
I0930 09:16:04.120977 140009111922496 base_runner.py:59] train.init_from_checkpoint_rules : {}
I0930 09:16:04.121039 140009111922496 base_runner.py:59] train.max_steps : 4000000
I0930 09:16:04.121112 140009111922496 base_runner.py:59] train.save_interval_seconds : 600
I0930 09:16:04.121179 140009111922496 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I0930 09:16:04.121249 140009111922496 base_runner.py:59] train.save_max_to_keep : 100
I0930 09:16:04.121321 140009111922496 base_runner.py:59] train.start_up_delay_steps : 200
I0930 09:16:04.121389 140009111922496 base_runner.py:59] train.summary_interval_steps : 100
I0930 09:16:04.121467 140009111922496 base_runner.py:59] train.tpu_steps_per_loop : 100
I0930 09:16:04.121527 140009111922496 base_runner.py:59] vn.global_vn : False
I0930 09:16:04.121599 140009111922496 base_runner.py:59] vn.per_step_vn : False
I0930 09:16:04.121665 140009111922496 base_runner.py:59] vn.scale : NoneType
I0930 09:16:04.121737 140009111922496 base_runner.py:59] vn.seed : NoneType
I0930 09:16:04.121790 140009111922496 base_runner.py:59] 
I0930 09:16:04.121950 140009111922496 base_runner.py:60] ============================================================
I0930 09:16:04.126333 140009111922496 base_runner.py:106] Starting ...
I0930 09:16:04.132948 140009111922496 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I0930 09:16:04.153618 140009111922496 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I0930 09:16:04.167433 140009111922496 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I0930 09:16:04.183488 140009111922496 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I0930 09:16:04.187345 140009111922496 base_input_generator.py:624] bucket_batch_limit [32]
I0930 09:16:04.244717 140009111922496 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I0930 09:16:04.244854 140009111922496 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I0930 09:16:04.244920 140009111922496 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I0930 09:16:04.244977 140009111922496 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I0930 09:16:04.245030 140009111922496 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I0930 09:16:04.245083 140009111922496 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I0930 09:16:04.245198 140009111922496 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I0930 09:16:04.245253 140009111922496 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I0930 09:16:04.245306 140009111922496 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I0930 09:16:04.245356 140009111922496 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I0930 09:16:04.245405 140009111922496 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I0930 09:16:04.245453 140009111922496 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I0930 09:16:04.245503 140009111922496 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I0930 09:16:04.245554 140009111922496 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I0930 09:16:04.245603 140009111922496 learner.py:351] Ignoring legacy param learner=None for optimization program
I0930 09:16:04.245686 140009111922496 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I0930 09:16:04.245739 140009111922496 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I0930 09:16:04.246177 140009111922496 learner.py:356] Learner params: allow_implicit_capture : NoneType
I0930 09:16:04.246273 140009111922496 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I0930 09:16:04.246343 140009111922496 learner.py:356] Learner params: bprop_variable_filter : NoneType
I0930 09:16:04.246399 140009111922496 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I0930 09:16:04.246452 140009111922496 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I0930 09:16:04.246503 140009111922496 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I0930 09:16:04.246554 140009111922496 learner.py:356] Learner params: colocate_gradients_with_ops : True
I0930 09:16:04.246604 140009111922496 learner.py:356] Learner params: dtype : float32
I0930 09:16:04.246654 140009111922496 learner.py:356] Learner params: fprop_dtype : NoneType
I0930 09:16:04.246705 140009111922496 learner.py:356] Learner params: gate_gradients : False
I0930 09:16:04.246754 140009111922496 learner.py:356] Learner params: grad_aggregation_method : 1
I0930 09:16:04.246803 140009111922496 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I0930 09:16:04.246853 140009111922496 learner.py:356] Learner params: grad_norm_tracker : NoneType
I0930 09:16:04.246911 140009111922496 learner.py:356] Learner params: inference_driver_name : NoneType
I0930 09:16:04.246962 140009111922496 learner.py:356] Learner params: is_eval : NoneType
I0930 09:16:04.247011 140009111922496 learner.py:356] Learner params: is_inference : NoneType
I0930 09:16:04.247060 140009111922496 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I0930 09:16:04.247109 140009111922496 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I0930 09:16:04.247158 140009111922496 learner.py:356] Learner params: learning_rate : 0.5
I0930 09:16:04.247207 140009111922496 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I0930 09:16:04.247256 140009111922496 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 09:16:04.247305 140009111922496 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I0930 09:16:04.247354 140009111922496 learner.py:356] Learner params: lr_schedule.dtype : float32
I0930 09:16:04.247402 140009111922496 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I0930 09:16:04.247451 140009111922496 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I0930 09:16:04.247499 140009111922496 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I0930 09:16:04.247547 140009111922496 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I0930 09:16:04.247596 140009111922496 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I0930 09:16:04.247645 140009111922496 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I0930 09:16:04.247693 140009111922496 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I0930 09:16:04.247742 140009111922496 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I0930 09:16:04.247791 140009111922496 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I0930 09:16:04.247839 140009111922496 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I0930 09:16:04.247887 140009111922496 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I0930 09:16:04.247936 140009111922496 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I0930 09:16:04.247984 140009111922496 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I0930 09:16:04.248033 140009111922496 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I0930 09:16:04.248081 140009111922496 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I0930 09:16:04.248130 140009111922496 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I0930 09:16:04.248178 140009111922496 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I0930 09:16:04.248227 140009111922496 learner.py:356] Learner params: name : 'loss'
I0930 09:16:04.248275 140009111922496 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I0930 09:16:04.248324 140009111922496 learner.py:356] Learner params: optimizer.beta1 : 0.9
I0930 09:16:04.248372 140009111922496 learner.py:356] Learner params: optimizer.beta2 : 0.997
I0930 09:16:04.248421 140009111922496 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 09:16:04.248470 140009111922496 learner.py:356] Learner params: optimizer.dtype : float32
I0930 09:16:04.248518 140009111922496 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I0930 09:16:04.248567 140009111922496 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I0930 09:16:04.248617 140009111922496 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I0930 09:16:04.248666 140009111922496 learner.py:356] Learner params: optimizer.is_eval : NoneType
I0930 09:16:04.248715 140009111922496 learner.py:356] Learner params: optimizer.is_inference : NoneType
I0930 09:16:04.248769 140009111922496 learner.py:356] Learner params: optimizer.name : 'Adam'
I0930 09:16:04.248818 140009111922496 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I0930 09:16:04.248872 140009111922496 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I0930 09:16:04.248923 140009111922496 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I0930 09:16:04.248971 140009111922496 learner.py:356] Learner params: optimizer.random_seed : NoneType
I0930 09:16:04.249021 140009111922496 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I0930 09:16:04.249070 140009111922496 learner.py:356] Learner params: optimizer.vn.global_vn : False
I0930 09:16:04.249120 140009111922496 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I0930 09:16:04.249168 140009111922496 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I0930 09:16:04.249217 140009111922496 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I0930 09:16:04.249267 140009111922496 learner.py:356] Learner params: params_init.method : 'xavier'
I0930 09:16:04.249316 140009111922496 learner.py:356] Learner params: params_init.scale : 1.000001
I0930 09:16:04.249365 140009111922496 learner.py:356] Learner params: params_init.seed : NoneType
I0930 09:16:04.249414 140009111922496 learner.py:356] Learner params: random_seed : NoneType
I0930 09:16:04.249463 140009111922496 learner.py:356] Learner params: skip_lp_regularization : NoneType
I0930 09:16:04.249512 140009111922496 learner.py:356] Learner params: vn.global_vn : False
I0930 09:16:04.249561 140009111922496 learner.py:356] Learner params: vn.per_step_vn : False
I0930 09:16:04.249610 140009111922496 learner.py:356] Learner params: vn.scale : NoneType
I0930 09:16:04.249660 140009111922496 learner.py:356] Learner params: vn.seed : NoneType
I0930 09:16:04.249709 140009111922496 learner.py:356] Learner params: 
I0930 09:16:04.601733 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I0930 09:16:04.603626 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.622716 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I0930 09:16:04.624642 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.627265 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I0930 09:16:04.628853 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.635638 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I0930 09:16:04.637507 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.640120 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I0930 09:16:04.641709 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.648527 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I0930 09:16:04.650437 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.652941 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I0930 09:16:04.654539 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.661363 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I0930 09:16:04.663255 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.665881 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I0930 09:16:04.667494 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.671553 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I0930 09:16:04.673147 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.676943 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I0930 09:16:04.678551 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.681152 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I0930 09:16:04.682768 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:04.692237 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:04.698364 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I0930 09:16:04.700311 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.702901 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I0930 09:16:04.704485 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:04.706411 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:04.712526 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I0930 09:16:04.714415 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.717003 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I0930 09:16:04.718650 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.723225 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I0930 09:16:04.724807 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.727454 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I0930 09:16:04.729033 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.749286 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I0930 09:16:04.751206 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.753752 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I0930 09:16:04.755488 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.762280 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I0930 09:16:04.764158 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.766780 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I0930 09:16:04.768379 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.775216 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I0930 09:16:04.777656 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.780211 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I0930 09:16:04.781821 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.788658 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I0930 09:16:04.790566 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.793120 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I0930 09:16:04.794846 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.798482 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I0930 09:16:04.800083 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.803922 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I0930 09:16:04.805512 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.808137 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I0930 09:16:04.809757 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:04.819279 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:04.825429 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I0930 09:16:04.827396 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.829916 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I0930 09:16:04.831534 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:04.833953 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:04.840151 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I0930 09:16:04.842038 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.844665 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I0930 09:16:04.846284 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.850851 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I0930 09:16:04.852442 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.855109 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I0930 09:16:04.856695 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.876430 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I0930 09:16:04.878323 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.880877 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I0930 09:16:04.883098 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.889884 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I0930 09:16:04.891800 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.894453 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I0930 09:16:04.896074 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.903040 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I0930 09:16:04.904990 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.907536 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I0930 09:16:04.909131 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.916050 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I0930 09:16:04.917948 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.920557 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I0930 09:16:04.922284 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.925938 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I0930 09:16:04.927578 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.931448 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I0930 09:16:04.933064 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.935722 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I0930 09:16:04.937332 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:04.947513 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:04.953667 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I0930 09:16:04.955647 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.958171 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I0930 09:16:04.959796 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:04.961731 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:04.967874 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I0930 09:16:04.969747 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.972390 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I0930 09:16:04.974000 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.978632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I0930 09:16:04.980237 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:04.982917 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I0930 09:16:04.984523 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.120102 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I0930 09:16:05.122105 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.124797 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I0930 09:16:05.126452 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.133304 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I0930 09:16:05.135321 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.137968 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I0930 09:16:05.139612 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.146567 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I0930 09:16:05.148486 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.151041 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I0930 09:16:05.152778 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.159700 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I0930 09:16:05.161610 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.164330 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I0930 09:16:05.165961 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.169609 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I0930 09:16:05.171272 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.175182 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I0930 09:16:05.177473 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.180025 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I0930 09:16:05.181642 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.191284 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.197470 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I0930 09:16:05.199414 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.201964 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I0930 09:16:05.203607 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.205548 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.211686 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I0930 09:16:05.213640 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.216206 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I0930 09:16:05.217840 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.222572 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I0930 09:16:05.224371 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.226907 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I0930 09:16:05.228526 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.249002 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I0930 09:16:05.250938 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.253484 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I0930 09:16:05.255239 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.262069 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I0930 09:16:05.263997 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.266637 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I0930 09:16:05.268262 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.275151 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I0930 09:16:05.277139 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.279713 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I0930 09:16:05.281337 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.288651 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I0930 09:16:05.290578 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.293249 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I0930 09:16:05.294914 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.298564 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I0930 09:16:05.300215 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.304095 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I0930 09:16:05.305717 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.308398 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I0930 09:16:05.310021 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.319784 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.326081 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I0930 09:16:05.328126 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.330730 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I0930 09:16:05.332359 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.334385 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.341256 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I0930 09:16:05.343421 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.346354 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I0930 09:16:05.348008 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.352835 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I0930 09:16:05.354504 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.357198 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I0930 09:16:05.358860 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.379414 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I0930 09:16:05.381649 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.384358 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I0930 09:16:05.386128 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.393948 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I0930 09:16:05.395983 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.398735 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I0930 09:16:05.400367 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.407312 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I0930 09:16:05.409387 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.412061 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I0930 09:16:05.413695 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.420771 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I0930 09:16:05.422738 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.425354 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I0930 09:16:05.427136 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.430877 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I0930 09:16:05.432526 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.436479 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I0930 09:16:05.438120 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.440824 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I0930 09:16:05.442509 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.452987 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.459615 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I0930 09:16:05.461646 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.464272 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I0930 09:16:05.465949 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.468053 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.474312 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I0930 09:16:05.476271 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.478960 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I0930 09:16:05.480593 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.485384 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I0930 09:16:05.487035 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.489731 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I0930 09:16:05.491387 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.512171 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I0930 09:16:05.514143 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.516728 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I0930 09:16:05.518509 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.525382 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I0930 09:16:05.527330 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.529986 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I0930 09:16:05.531641 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.538511 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I0930 09:16:05.540497 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.543085 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I0930 09:16:05.544713 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.551742 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I0930 09:16:05.553657 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.556264 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I0930 09:16:05.557993 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.561782 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I0930 09:16:05.563449 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.567418 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I0930 09:16:05.569052 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.572234 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I0930 09:16:05.573885 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.583754 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.589952 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I0930 09:16:05.591976 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.594580 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I0930 09:16:05.596239 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.598238 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.604412 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I0930 09:16:05.606372 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.609024 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I0930 09:16:05.610702 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.615451 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I0930 09:16:05.617085 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.619830 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I0930 09:16:05.621469 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.642136 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I0930 09:16:05.644132 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.646715 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I0930 09:16:05.648455 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.655358 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I0930 09:16:05.657299 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.659990 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I0930 09:16:05.661630 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.668561 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I0930 09:16:05.670565 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.673135 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I0930 09:16:05.674798 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.682178 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I0930 09:16:05.684142 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.686772 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I0930 09:16:05.688531 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.692266 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I0930 09:16:05.693912 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.697902 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I0930 09:16:05.699559 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.702216 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I0930 09:16:05.703882 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.713633 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.719980 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I0930 09:16:05.721988 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.724597 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I0930 09:16:05.726276 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.728303 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.735073 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I0930 09:16:05.737008 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.739690 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I0930 09:16:05.741336 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.746066 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I0930 09:16:05.747857 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.750572 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I0930 09:16:05.752215 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.825502 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I0930 09:16:05.827550 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.830221 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I0930 09:16:05.831897 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.838814 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I0930 09:16:05.841251 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.843827 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I0930 09:16:05.845490 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.852411 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I0930 09:16:05.854350 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.856986 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I0930 09:16:05.858779 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.865664 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I0930 09:16:05.867625 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.870355 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I0930 09:16:05.872013 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.875720 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I0930 09:16:05.877391 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.881376 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I0930 09:16:05.883063 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:05.885743 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I0930 09:16:05.887414 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:05.897172 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:05.904080 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I0930 09:16:06.036091 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.039067 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I0930 09:16:06.040779 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.042888 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.049163 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I0930 09:16:06.051165 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.053847 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I0930 09:16:06.055538 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.060346 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I0930 09:16:06.061995 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.064752 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I0930 09:16:06.066439 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.087063 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I0930 09:16:06.089038 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.091752 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I0930 09:16:06.093412 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.100399 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I0930 09:16:06.102410 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.105010 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I0930 09:16:06.106698 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.113632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I0930 09:16:06.115605 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.118189 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I0930 09:16:06.119978 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.126904 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I0930 09:16:06.128858 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.131593 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I0930 09:16:06.133255 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.137003 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I0930 09:16:06.138689 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.142701 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I0930 09:16:06.144353 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.147050 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I0930 09:16:06.148715 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.159070 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.165282 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I0930 09:16:06.167312 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.169892 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I0930 09:16:06.171587 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.173602 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.179792 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I0930 09:16:06.181748 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.184435 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I0930 09:16:06.186083 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.190880 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I0930 09:16:06.192530 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.195265 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I0930 09:16:06.196924 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.217692 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I0930 09:16:06.219698 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.222358 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I0930 09:16:06.224125 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.231074 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I0930 09:16:06.233047 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.235789 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I0930 09:16:06.237441 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.244378 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I0930 09:16:06.246455 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.249046 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I0930 09:16:06.250743 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.257711 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I0930 09:16:06.259704 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.262339 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I0930 09:16:06.264631 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.268440 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I0930 09:16:06.270105 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.274135 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I0930 09:16:06.275816 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.278538 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I0930 09:16:06.280210 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.289960 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.296257 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I0930 09:16:06.298307 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.300902 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I0930 09:16:06.302586 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.304616 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.310867 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I0930 09:16:06.312798 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.315526 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I0930 09:16:06.317186 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.322015 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I0930 09:16:06.323693 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.326941 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I0930 09:16:06.328931 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.348947 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I0930 09:16:06.350940 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.353521 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I0930 09:16:06.355328 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.362179 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I0930 09:16:06.364151 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.366920 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I0930 09:16:06.368600 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.376020 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I0930 09:16:06.378027 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.380647 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I0930 09:16:06.382333 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.389280 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I0930 09:16:06.391278 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.393918 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I0930 09:16:06.395707 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.399559 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I0930 09:16:06.401241 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.405280 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I0930 09:16:06.406963 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.409644 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I0930 09:16:06.411349 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.421086 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.427381 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I0930 09:16:06.429970 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.432622 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I0930 09:16:06.434324 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.436383 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.442589 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I0930 09:16:06.444548 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.447236 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I0930 09:16:06.448914 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.453757 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I0930 09:16:06.455432 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.458144 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I0930 09:16:06.459836 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.480430 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I0930 09:16:06.482412 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.485014 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I0930 09:16:06.486815 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.493797 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I0930 09:16:06.495778 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.498499 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I0930 09:16:06.500166 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.507095 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I0930 09:16:06.509116 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.511754 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I0930 09:16:06.513426 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.520401 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I0930 09:16:06.522417 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.525059 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I0930 09:16:06.526872 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.530667 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I0930 09:16:06.532343 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.536410 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I0930 09:16:06.538069 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.540810 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I0930 09:16:06.542516 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.553158 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.559469 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I0930 09:16:06.561538 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.564183 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I0930 09:16:06.565852 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.567974 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.574187 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I0930 09:16:06.576176 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.578920 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I0930 09:16:06.580598 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.585498 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I0930 09:16:06.587188 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.589927 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I0930 09:16:06.591639 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.612551 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I0930 09:16:06.614568 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.617187 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I0930 09:16:06.619001 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.625947 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I0930 09:16:06.627947 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.630683 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I0930 09:16:06.632359 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.639287 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I0930 09:16:06.641310 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.643954 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I0930 09:16:06.645620 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.652614 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I0930 09:16:06.654598 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.657246 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I0930 09:16:06.659524 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.663430 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I0930 09:16:06.665118 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.669186 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I0930 09:16:06.670872 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.673578 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I0930 09:16:06.675280 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.685148 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.691422 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I0930 09:16:06.693455 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.696095 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I0930 09:16:06.697789 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.699885 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.706107 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I0930 09:16:06.708113 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.710841 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I0930 09:16:06.712524 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.717357 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I0930 09:16:06.719066 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.722338 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I0930 09:16:06.724022 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.744134 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I0930 09:16:06.746092 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.748809 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I0930 09:16:06.750613 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.757537 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I0930 09:16:06.759531 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.762271 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I0930 09:16:06.763952 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.771392 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I0930 09:16:06.773438 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.776100 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I0930 09:16:06.777775 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.784768 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I0930 09:16:06.786764 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.789402 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I0930 09:16:06.791214 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.795017 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I0930 09:16:06.796719 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.800827 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I0930 09:16:06.802542 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.805263 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I0930 09:16:06.806993 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.816929 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.823309 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I0930 09:16:06.826007 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.828726 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I0930 09:16:06.830455 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.832570 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.838891 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I0930 09:16:06.840894 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.843662 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I0930 09:16:06.845375 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.850310 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I0930 09:16:06.852015 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.854794 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I0930 09:16:06.856489 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.877389 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I0930 09:16:06.879412 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.882043 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I0930 09:16:06.883863 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.890912 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I0930 09:16:06.892900 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.895649 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I0930 09:16:06.897339 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.904306 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I0930 09:16:06.906377 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.909028 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I0930 09:16:06.910746 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.917762 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I0930 09:16:06.919782 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.922501 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I0930 09:16:06.924306 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.928156 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I0930 09:16:06.929866 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.934001 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I0930 09:16:06.935703 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.938453 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I0930 09:16:06.940165 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.950690 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.957020 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I0930 09:16:06.959087 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.961721 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I0930 09:16:06.963459 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:06.965571 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:06.971813 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I0930 09:16:06.973802 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.976536 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I0930 09:16:06.978232 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.983150 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I0930 09:16:06.984832 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:06.987644 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I0930 09:16:06.989351 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.207954 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I0930 09:16:07.210066 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.212766 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I0930 09:16:07.214619 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.221606 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I0930 09:16:07.223692 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.226498 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I0930 09:16:07.228197 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.235176 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I0930 09:16:07.237241 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.239927 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I0930 09:16:07.241626 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.248648 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I0930 09:16:07.250670 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.253345 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I0930 09:16:07.255189 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.259566 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I0930 09:16:07.261278 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.265454 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I0930 09:16:07.267169 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.269893 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I0930 09:16:07.271615 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.281520 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.287822 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I0930 09:16:07.289878 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.292551 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I0930 09:16:07.294250 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.296382 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.302631 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I0930 09:16:07.304623 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.307447 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I0930 09:16:07.309156 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.314095 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I0930 09:16:07.315804 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.318579 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I0930 09:16:07.320272 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.340929 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I0930 09:16:07.342952 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.345595 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I0930 09:16:07.347428 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.354382 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I0930 09:16:07.356381 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.359147 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I0930 09:16:07.360846 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.367803 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I0930 09:16:07.370367 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.373019 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I0930 09:16:07.374731 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.381747 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I0930 09:16:07.383760 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.386462 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I0930 09:16:07.388256 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.392113 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I0930 09:16:07.393823 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.397989 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I0930 09:16:07.399717 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.402477 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I0930 09:16:07.404197 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.414076 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.420410 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I0930 09:16:07.422501 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.425235 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I0930 09:16:07.426972 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.429685 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.435979 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I0930 09:16:07.437986 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.440754 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I0930 09:16:07.442487 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.447481 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I0930 09:16:07.449178 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.451967 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I0930 09:16:07.453686 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.474297 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I0930 09:16:07.476338 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.479029 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I0930 09:16:07.481427 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.488442 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I0930 09:16:07.490471 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.493229 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I0930 09:16:07.494980 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.501912 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I0930 09:16:07.504008 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.506699 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I0930 09:16:07.508415 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.515499 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I0930 09:16:07.517516 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.520241 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I0930 09:16:07.522065 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.526027 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I0930 09:16:07.527855 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.532091 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I0930 09:16:07.533803 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.536584 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I0930 09:16:07.538330 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.549013 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.555413 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I0930 09:16:07.557503 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.560192 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I0930 09:16:07.561915 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.564176 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.570467 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I0930 09:16:07.572474 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.575248 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I0930 09:16:07.576967 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.581956 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I0930 09:16:07.583674 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.586520 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I0930 09:16:07.588236 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.609121 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I0930 09:16:07.611178 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.613821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I0930 09:16:07.615677 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.622692 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I0930 09:16:07.624740 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.627529 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I0930 09:16:07.629252 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.636289 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I0930 09:16:07.638446 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.641143 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I0930 09:16:07.642869 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.649901 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I0930 09:16:07.651924 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.654646 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I0930 09:16:07.656469 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.660894 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I0930 09:16:07.662638 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.667072 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I0930 09:16:07.668797 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.671611 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I0930 09:16:07.673350 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.683572 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.690099 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I0930 09:16:07.692230 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.694958 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I0930 09:16:07.696690 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.698880 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.705163 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I0930 09:16:07.707204 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.709971 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I0930 09:16:07.711728 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.716741 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I0930 09:16:07.718470 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.721250 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I0930 09:16:07.723003 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.744056 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I0930 09:16:07.746081 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.748892 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I0930 09:16:07.750746 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.757722 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I0930 09:16:07.759749 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.762527 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I0930 09:16:07.764246 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.771416 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I0930 09:16:07.773961 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.776631 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I0930 09:16:07.778397 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.785369 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I0930 09:16:07.787393 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.790091 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I0930 09:16:07.791935 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.795796 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I0930 09:16:07.797537 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.801756 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I0930 09:16:07.803494 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.806253 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I0930 09:16:07.808014 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.817999 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.824333 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I0930 09:16:07.826476 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.829171 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I0930 09:16:07.830929 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.833743 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.840115 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I0930 09:16:07.842171 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.844990 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I0930 09:16:07.846761 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.851844 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I0930 09:16:07.853557 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.856375 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I0930 09:16:07.858132 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.878729 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I0930 09:16:07.880754 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.883456 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I0930 09:16:07.885810 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.892826 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I0930 09:16:07.894867 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.897646 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I0930 09:16:07.899403 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.906360 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I0930 09:16:07.908446 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.911170 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I0930 09:16:07.912895 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.919975 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I0930 09:16:07.921992 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.924759 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I0930 09:16:07.926604 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.930529 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I0930 09:16:07.932276 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.936523 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I0930 09:16:07.938247 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.941031 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I0930 09:16:07.942795 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.953472 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.959883 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I0930 09:16:07.961967 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.964692 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I0930 09:16:07.966509 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:07.968697 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:07.974992 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I0930 09:16:07.977019 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.979815 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I0930 09:16:07.981555 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.986633 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I0930 09:16:07.988365 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:07.991193 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I0930 09:16:07.992933 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.014152 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I0930 09:16:08.016225 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.018946 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I0930 09:16:08.020774 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.027799 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I0930 09:16:08.029833 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.032642 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I0930 09:16:08.034389 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.041351 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I0930 09:16:08.043467 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.046161 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I0930 09:16:08.047908 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.054957 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I0930 09:16:08.056976 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.059790 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I0930 09:16:08.061628 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.065994 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I0930 09:16:08.067782 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.072054 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I0930 09:16:08.073776 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.076567 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I0930 09:16:08.078340 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.088456 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.094801 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I0930 09:16:08.096900 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.099620 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I0930 09:16:08.101359 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.103581 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.109829 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I0930 09:16:08.112004 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.114814 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I0930 09:16:08.116543 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.121575 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I0930 09:16:08.123331 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.126198 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I0930 09:16:08.127953 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.148930 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I0930 09:16:08.151060 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.153854 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I0930 09:16:08.155833 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.162905 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I0930 09:16:08.165017 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.167942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I0930 09:16:08.169693 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.176755 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I0930 09:16:08.179480 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.182198 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I0930 09:16:08.183964 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.191079 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I0930 09:16:08.193123 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.195882 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I0930 09:16:08.197730 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.201721 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I0930 09:16:08.203489 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.207790 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I0930 09:16:08.209549 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.212358 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I0930 09:16:08.214101 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.224219 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.230723 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I0930 09:16:08.232906 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.235731 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I0930 09:16:08.237483 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.240425 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.246818 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I0930 09:16:08.248901 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.252639 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I0930 09:16:08.254445 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.259637 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I0930 09:16:08.261387 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.264221 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I0930 09:16:08.265978 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.339159 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I0930 09:16:08.341339 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.344171 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I0930 09:16:08.345923 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.517297 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I0930 09:16:08.519620 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.522541 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I0930 09:16:08.524402 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.531561 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I0930 09:16:08.533619 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.536472 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I0930 09:16:08.538210 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.545258 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I0930 09:16:08.547398 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.550149 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I0930 09:16:08.551920 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.555840 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I0930 09:16:08.557702 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.562088 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I0930 09:16:08.563867 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.566677 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I0930 09:16:08.568437 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.579250 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.585689 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I0930 09:16:08.587775 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.590525 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I0930 09:16:08.592286 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.594534 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.600854 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I0930 09:16:08.603003 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.605726 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I0930 09:16:08.607501 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.612611 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I0930 09:16:08.614557 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.617225 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I0930 09:16:08.619015 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.640202 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I0930 09:16:08.642253 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.645076 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I0930 09:16:08.647032 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.654013 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I0930 09:16:08.656075 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.658905 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I0930 09:16:08.660654 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.667695 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I0930 09:16:08.669723 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.672470 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I0930 09:16:08.674206 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.681277 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I0930 09:16:08.683346 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.686198 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I0930 09:16:08.687965 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.691936 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I0930 09:16:08.693704 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.698034 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I0930 09:16:08.699795 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.703075 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I0930 09:16:08.704837 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.714979 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.721356 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I0930 09:16:08.723491 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.726217 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I0930 09:16:08.728031 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.730253 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.736658 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I0930 09:16:08.738723 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.741518 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I0930 09:16:08.743304 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.748454 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I0930 09:16:08.750210 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.753076 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I0930 09:16:08.754862 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.775879 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I0930 09:16:08.777937 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.780688 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I0930 09:16:08.782564 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.789542 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I0930 09:16:08.791647 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.794532 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I0930 09:16:08.796270 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.803273 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I0930 09:16:08.805384 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.808145 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I0930 09:16:08.809900 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.817463 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I0930 09:16:08.819533 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.822311 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I0930 09:16:08.824181 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.828196 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I0930 09:16:08.829972 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.834364 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I0930 09:16:08.836117 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.838963 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I0930 09:16:08.840735 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.850916 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.857254 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I0930 09:16:08.859394 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.862119 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I0930 09:16:08.863902 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.866134 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.873004 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I0930 09:16:08.875078 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.877885 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I0930 09:16:08.879704 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.884807 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I0930 09:16:08.886591 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.889428 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I0930 09:16:08.891211 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.911819 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I0930 09:16:08.913909 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.916656 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I0930 09:16:08.918552 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.926152 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I0930 09:16:08.928259 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.931344 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I0930 09:16:08.933108 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.940112 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I0930 09:16:08.942215 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.944964 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I0930 09:16:08.946737 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.953748 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I0930 09:16:08.955832 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.958611 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I0930 09:16:08.960481 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.964492 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I0930 09:16:08.966248 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.970634 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I0930 09:16:08.972398 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.975234 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I0930 09:16:08.976997 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:08.987789 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:08.994148 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I0930 09:16:08.996294 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:08.999063 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I0930 09:16:09.000838 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.003107 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.009479 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I0930 09:16:09.011547 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.014389 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I0930 09:16:09.016159 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.021301 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I0930 09:16:09.023092 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.025941 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I0930 09:16:09.027780 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.049308 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I0930 09:16:09.051409 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.054129 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I0930 09:16:09.056018 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.063057 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I0930 09:16:09.065116 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.067952 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I0930 09:16:09.069726 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.076761 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I0930 09:16:09.078909 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.081642 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I0930 09:16:09.083433 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.090535 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I0930 09:16:09.092605 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.095391 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I0930 09:16:09.097265 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.101289 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I0930 09:16:09.103097 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.107545 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I0930 09:16:09.109612 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.112974 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I0930 09:16:09.114811 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.125132 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.131645 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I0930 09:16:09.133779 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.136551 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I0930 09:16:09.138351 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.140614 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.146944 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I0930 09:16:09.149008 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.151867 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I0930 09:16:09.153682 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.158853 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I0930 09:16:09.160635 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.163504 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I0930 09:16:09.165280 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.186455 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I0930 09:16:09.188522 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.191265 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I0930 09:16:09.193138 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.200191 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I0930 09:16:09.202339 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.205181 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I0930 09:16:09.206973 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.213962 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I0930 09:16:09.216116 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.218901 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I0930 09:16:09.220670 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.228225 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I0930 09:16:09.230319 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.233097 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I0930 09:16:09.234990 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.239048 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I0930 09:16:09.240846 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.245285 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I0930 09:16:09.247070 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.249873 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I0930 09:16:09.251683 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.261883 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.268594 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I0930 09:16:09.270752 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.273516 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I0930 09:16:09.275323 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.277607 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.284549 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I0930 09:16:09.286656 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.289511 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I0930 09:16:09.291323 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.296487 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I0930 09:16:09.298287 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.301137 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I0930 09:16:09.302954 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.323586 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I0930 09:16:09.325669 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.328468 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I0930 09:16:09.330363 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.337954 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I0930 09:16:09.340063 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.342911 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I0930 09:16:09.344691 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.351731 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I0930 09:16:09.353873 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.356642 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I0930 09:16:09.358453 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.365525 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I0930 09:16:09.367620 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.370402 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I0930 09:16:09.372292 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.376333 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I0930 09:16:09.378110 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.382574 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I0930 09:16:09.384344 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.387248 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I0930 09:16:09.389037 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.399821 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.406235 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I0930 09:16:09.408418 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.411190 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I0930 09:16:09.412983 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.415302 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.421665 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I0930 09:16:09.423775 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.426648 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I0930 09:16:09.428473 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.433676 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I0930 09:16:09.435511 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.438414 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I0930 09:16:09.440211 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.461696 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I0930 09:16:09.463857 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.466630 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I0930 09:16:09.468521 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.475590 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I0930 09:16:09.477681 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.480556 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I0930 09:16:09.482473 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.489794 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I0930 09:16:09.491974 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.494756 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I0930 09:16:09.496546 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.503661 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I0930 09:16:09.505760 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.508582 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I0930 09:16:09.510485 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.514531 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I0930 09:16:09.516337 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.520797 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I0930 09:16:09.522610 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.525926 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I0930 09:16:09.527748 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.538087 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.544455 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I0930 09:16:09.546627 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.549407 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I0930 09:16:09.551234 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.553612 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.559988 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I0930 09:16:09.562077 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.564929 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I0930 09:16:09.566756 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.571961 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I0930 09:16:09.573794 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.576683 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I0930 09:16:09.578507 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.583928 140009111922496 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.590204 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I0930 09:16:09.592304 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.593132 140009111922496 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.599891 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I0930 09:16:09.601973 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.602832 140009111922496 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.609045 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I0930 09:16:09.611181 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.612017 140009111922496 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.618384 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I0930 09:16:09.620539 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.621495 140009111922496 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.627942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I0930 09:16:09.630053 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.630943 140009111922496 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.637213 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I0930 09:16:09.639325 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.640169 140009111922496 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.646433 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I0930 09:16:09.648511 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.649339 140009111922496 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.655644 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I0930 09:16:09.657707 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.658571 140009111922496 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.664794 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I0930 09:16:09.666923 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.667771 140009111922496 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.674730 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I0930 09:16:09.676846 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.677699 140009111922496 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.684034 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I0930 09:16:09.686144 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.687026 140009111922496 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.693326 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I0930 09:16:09.695444 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.696292 140009111922496 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.702589 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I0930 09:16:09.704652 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.705494 140009111922496 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.711857 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I0930 09:16:09.713954 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.714831 140009111922496 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.721071 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I0930 09:16:09.723188 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:16:09.724030 140009111922496 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I0930 09:16:09.730367 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I0930 09:16:09.732430 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.735269 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I0930 09:16:09.737183 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.739994 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I0930 09:16:09.741839 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.745308 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I0930 09:16:09.747262 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.750029 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I0930 09:16:09.751822 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.754752 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I0930 09:16:09.756535 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.759305 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I0930 09:16:09.761089 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.763945 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I0930 09:16:09.765715 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.768462 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I0930 09:16:09.770381 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.773121 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I0930 09:16:09.774953 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.777784 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I0930 09:16:09.779595 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.782350 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I0930 09:16:09.784112 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.786967 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I0930 09:16:09.788768 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.791515 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I0930 09:16:09.793315 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.796175 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I0930 09:16:09.797956 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.800698 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I0930 09:16:09.802604 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:09.805338 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I0930 09:16:09.807160 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:10.641444 140009111922496 py_utils.py:1484] === worker 0 ===
I0930 09:16:10.656506 140009111922496 py_utils.py:1474] worker 0: global_step                                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.656606 140009111922496 py_utils.py:1474] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.656666 140009111922496 py_utils.py:1474] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.656719 140009111922496 py_utils.py:1474] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.656769 140009111922496 py_utils.py:1474] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.656817 140009111922496 py_utils.py:1474] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.656873 140009111922496 py_utils.py:1474] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.656921 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.656967 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657013 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657059 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657105 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657151 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657197 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657243 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657290 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657335 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657380 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657425 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657470 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657516 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657561 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657605 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657650 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657695 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657744 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657791 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657835 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657881 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657925 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.657971 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658016 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658061 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658106 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658151 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658196 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658241 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658319 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658373 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658421 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658467 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658512 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658558 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658608 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658655 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658700 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658746 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658792 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658837 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658883 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658928 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.658974 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659019 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659065 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659111 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659156 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659202 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659247 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659292 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659338 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659383 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659428 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659478 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659524 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659569 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659614 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659660 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659704 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659750 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659795 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659840 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659885 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659930 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.659976 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660021 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660067 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660113 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660158 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660204 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660249 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660298 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660345 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660390 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660436 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660481 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660526 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660572 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660617 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660662 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660708 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660753 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660797 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660842 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660888 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660933 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.660978 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661024 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661068 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661113 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661162 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661208 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661254 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661299 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661344 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661389 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661434 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661479 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661523 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661568 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661613 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661657 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661708 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661753 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661799 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661844 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661889 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661934 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.661983 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662030 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662076 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662121 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662167 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662211 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662270 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662323 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662369 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662415 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662461 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662507 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662552 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662597 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662643 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662688 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662734 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662779 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662825 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662875 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662921 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.662967 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663013 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663058 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663103 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663148 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663192 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663237 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663283 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663327 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663372 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663417 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663462 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663506 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663551 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663595 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663640 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663689 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663735 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663780 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663824 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663868 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663913 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.663958 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664003 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664048 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664093 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664137 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664182 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664227 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664271 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664315 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664360 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664405 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664450 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664495 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664545 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664591 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664636 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664681 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664726 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664771 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664816 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664860 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664905 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664949 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.664993 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665037 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665082 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665126 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665171 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665216 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665261 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665305 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665354 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665400 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665445 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665489 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665534 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665579 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665623 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665669 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665714 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665758 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665802 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665847 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665892 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665936 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.665981 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666026 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666070 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666115 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666159 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666208 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666253 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666316 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666363 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666408 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666453 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666497 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666543 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666587 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666632 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666676 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666721 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666765 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666809 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666854 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666899 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666943 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.666988 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667036 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667081 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667126 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667171 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667215 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667259 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667304 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667348 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667392 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667437 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667481 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667525 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667569 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667614 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667658 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667703 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667748 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667792 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667837 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667886 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667932 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.667976 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668021 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668066 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668111 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668156 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668201 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668246 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668291 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668336 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668381 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668426 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668469 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668514 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668558 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668603 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668648 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668691 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668741 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668786 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668831 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668875 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668920 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.668965 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669010 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669055 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669099 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669144 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669189 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669234 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669279 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669324 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669369 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669414 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669459 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669504 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669553 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669599 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669644 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669689 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669734 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669779 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669824 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669869 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669914 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.669959 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670003 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670048 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670093 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670137 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670182 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670226 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670292 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670341 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670387 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670437 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670484 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670529 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670574 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670619 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670664 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670709 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670754 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670799 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670845 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670889 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670934 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.670980 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671024 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671069 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671114 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671159 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671204 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671252 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671298 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671343 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671388 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671433 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671478 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671523 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671567 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671612 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671657 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671701 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671750 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671796 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671840 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671885 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671930 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.671975 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672020 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672065 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672114 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672160 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672205 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672251 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672296 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672341 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672386 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672432 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672477 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672526 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672573 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672619 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672664 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672710 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672755 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672799 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672844 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672888 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672937 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.672983 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673029 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673074 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673119 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673164 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673209 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673254 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673299 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673344 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673388 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673434 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673479 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673524 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673568 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673614 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673658 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673704 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673749 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673798 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673844 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673889 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673934 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.673979 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674025 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674070 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674115 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674160 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674205 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674250 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674314 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674361 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674407 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674452 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674498 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674543 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674588 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674637 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674683 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674728 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674773 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674818 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674863 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674908 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674953 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.674998 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675043 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675088 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675133 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675178 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675224 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675269 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675314 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675359 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675404 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675449 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675498 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675544 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675590 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675636 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675681 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675726 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675772 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675816 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675860 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675906 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675951 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.675996 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676041 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676085 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676131 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676176 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676221 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676266 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676315 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676360 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676405 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676450 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676496 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676541 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676585 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676630 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676675 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676720 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676765 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676810 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676855 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676900 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676945 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.676990 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677035 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677081 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677125 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677175 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677220 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677266 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677310 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677355 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677401 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677445 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677490 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677535 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677579 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677624 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677668 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677712 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677757 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677801 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677845 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677889 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677934 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.677983 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678029 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678074 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678119 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678164 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678208 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678253 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678316 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678362 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678407 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678452 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678497 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678542 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678587 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678632 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678676 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678720 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678765 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678810 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678860 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678906 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678951 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.678996 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679041 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679086 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679131 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679176 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679221 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679267 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679312 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679357 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679403 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679448 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679493 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679538 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679583 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679628 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679678 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679724 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679769 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679814 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679859 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679904 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679949 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.679995 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680041 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680087 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680132 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680178 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680223 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680268 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680313 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680358 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680403 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680448 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680493 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680543 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680589 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680634 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680679 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680723 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680768 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680814 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680858 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680904 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680949 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.680994 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681039 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681084 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681129 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681174 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681220 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681265 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681310 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681355 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681404 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681450 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681495 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681540 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681586 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681630 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681675 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681720 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681767 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681814 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681859 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681904 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681948 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.681993 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682038 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682082 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682127 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682172 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682221 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682284 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682334 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682380 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682425 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682471 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682516 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682561 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682607 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682652 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682697 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682742 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682787 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682832 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682876 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682922 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.682966 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683012 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683057 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683106 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683152 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683197 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683242 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683287 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683332 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683377 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683422 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683467 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683512 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683557 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683602 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683647 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683692 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683737 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683782 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683827 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683872 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683921 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.683966 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684011 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684056 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684100 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684144 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684189 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684233 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684278 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684322 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684366 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684411 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684455 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684500 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684545 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684590 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684635 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684679 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684723 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684772 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684818 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684862 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684907 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684952 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.684997 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685042 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685087 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685132 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685177 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685222 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685267 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685312 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685357 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685402 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685447 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685492 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685536 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685585 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685631 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685676 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685720 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685764 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685808 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685852 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685897 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685941 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.685985 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686030 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686074 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686118 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686162 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686207 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686251 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686314 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686360 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686405 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686454 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686500 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686544 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686589 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686634 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686679 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686723 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686768 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686812 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686857 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686901 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686946 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.686990 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687035 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687080 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687125 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687170 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687214 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687263 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687308 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687353 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687398 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687443 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687488 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687534 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687579 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687623 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687668 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687713 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687758 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687803 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687849 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687894 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687939 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.687984 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688030 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688075 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688124 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688170 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688215 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688261 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688306 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688351 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688396 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688441 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688486 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688531 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688577 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688622 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688668 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688713 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688758 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688803 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688848 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688893 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688942 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.688988 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689033 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689078 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689123 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689167 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689212 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689257 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689301 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689346 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689390 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689435 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689479 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689524 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689569 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689614 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689659 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689704 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689749 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689797 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689844 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689889 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689933 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.689978 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690023 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690068 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690114 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690159 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690204 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690249 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690310 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690357 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690402 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690447 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690491 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690536 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690581 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690631 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690677 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690722 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690767 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690813 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690858 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690904 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690949 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.690994 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691040 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691084 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691129 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691174 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691220 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691265 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691310 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691355 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691401 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691446 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691495 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691541 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691586 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691632 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691676 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691722 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691767 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691812 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691861 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691907 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691952 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.691996 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692042 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692086 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692132 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692178 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692223 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692267 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692317 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692363 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692408 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692453 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692498 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692543 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692589 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692634 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692679 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692724 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692770 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692815 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692860 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692905 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692950 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.692996 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693041 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693086 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693131 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693180 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693226 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693270 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693315 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693360 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693405 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693450 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693495 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693540 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693585 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693630 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693675 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693720 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693765 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693810 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693855 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693900 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693945 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.693990 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694038 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694084 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694129 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694174 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694219 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694280 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694331 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694377 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694422 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694468 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694513 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694559 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694604 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694650 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694695 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694740 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694785 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694830 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694880 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694926 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.694971 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695016 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695061 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695106 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695151 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695195 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695240 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695285 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695329 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695374 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695419 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695464 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695508 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695554 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695598 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695643 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695688 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695738 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695783 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695828 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695874 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695919 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.695964 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696009 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696055 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696100 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696146 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696191 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696236 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696281 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696326 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696372 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696416 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696462 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696508 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696558 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696605 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696650 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696695 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696740 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696786 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696831 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696876 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696921 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.696967 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697012 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697057 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697101 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697147 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697191 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697237 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697282 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697327 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697372 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697422 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697468 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697514 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697559 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697604 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697650 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697695 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697741 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697786 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697830 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697875 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697921 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.697966 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698011 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698056 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698101 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698146 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698191 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698240 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698302 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698349 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698395 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698440 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698485 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698530 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698575 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698620 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698665 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698710 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698754 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698799 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698843 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698888 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698933 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.698977 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699022 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699067 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699116 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699162 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699208 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699253 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699298 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699343 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699388 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699434 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699479 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699524 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699568 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699613 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699659 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699704 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699749 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699795 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699839 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699884 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699934 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.699980 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700025 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700070 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700116 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700161 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700206 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700252 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700298 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700343 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700388 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700433 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700477 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700523 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700568 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700613 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700658 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700703 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700748 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700797 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700842 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700888 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700933 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.700977 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701022 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701067 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701112 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701157 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701202 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701247 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701292 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701337 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701382 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701426 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701472 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701517 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701562 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701611 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701657 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701702 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701747 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701792 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701836 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701884 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701931 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.701977 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702021 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702066 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702111 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702157 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702202 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702247 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702310 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702357 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702402 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702448 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702497 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702544 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702594 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702641 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702685 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702731 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702776 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702821 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702867 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702912 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.702957 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703002 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703047 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703092 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703137 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703182 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703228 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703273 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703322 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703368 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703413 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703458 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703503 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703548 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703593 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703638 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703683 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703727 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703772 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703817 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703861 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703906 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703952 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.703997 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.704042 140009111922496 py_utils.py:1474] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:16:10.704107 140009111922496 py_utils.py:1490] ==========
I0930 09:16:13.106904 140009111922496 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 09:16:15.286535 140009111922496 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:16:17.647145 140009111922496 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:16:19.760848 140009111922496 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:16:22.290185 140009111922496 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W0930 09:16:24.155501 140009111922496 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 09:16:24.275356 140009111922496 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 09:16:26.481713 140009111922496 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 09:16:26.602954 140009111922496 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 09:16:28.417883 140009111922496 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 09:16:28.553110 140009111922496 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 09:16:30.809117 140009111922496 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 09:16:31.283779 140009111922496 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 09:16:33.775889 140009111922496 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:16:36.701653 140009111922496 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:16:39.635867 140009111922496 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:16:41.625711 140009111922496 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I0930 09:16:41.629904 140009111922496 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I0930 09:16:41.719630 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I0930 09:16:41.721487 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:16:41.730063 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I0930 09:16:41.730156 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.730222 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.730313 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.730373 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.730427 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.730480 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.730532 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.730585 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.730636 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.730688 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.730740 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.730791 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.730851 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.730904 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.730955 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.731005 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.731055 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.731106 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.731156 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.731206 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.731256 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.731305 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.731355 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.731408 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.731460 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.731511 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.731561 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.731611 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.731662 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.731712 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.731762 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.731812 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.731862 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.731912 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.731962 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.732013 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.732068 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.732119 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.732169 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.732219 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.732269 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.732318 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.732368 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.732418 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.732468 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.732518 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.732568 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.732617 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.732667 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.732717 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.732767 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.732817 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.732866 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.732916 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.732966 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.733015 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.733065 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.733114 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.733164 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.733214 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.733263 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.733317 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.733368 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.733417 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.733467 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.733517 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.733566 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.733616 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.733666 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.733716 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.733765 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.733814 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.733865 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.733914 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.733963 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.734012 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.734062 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.734112 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.734161 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.734211 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.734274 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.734330 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.734379 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.734429 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.734478 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.734533 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.734583 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.734632 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.734682 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.734731 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.734781 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.734831 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.734881 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.734931 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.734980 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.735030 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.735079 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.735129 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.735178 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.735227 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.735276 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.735326 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.735374 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.735423 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.735472 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.735522 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.735570 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.735619 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.735669 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.735718 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.735773 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.735824 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.735873 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.735923 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.735972 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.736021 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.736071 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.736121 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.736171 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.736220 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.736269 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.736319 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.736369 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.736418 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.736468 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.736518 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.736567 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.736617 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.736666 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.736716 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.736765 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.736815 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.736864 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.736913 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.736967 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.737017 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.737066 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.737116 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.737165 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.737214 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.737264 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.737314 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.737363 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.737413 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.737462 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.737511 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.737561 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.737611 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.737660 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.737709 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.737759 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.737809 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.737859 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.737908 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.737957 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.738007 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.738058 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.738107 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.738157 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.738211 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.738279 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.738336 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.738386 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.738435 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.738485 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.738535 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.738585 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.738635 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.738684 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.738733 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.738782 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.738832 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.738882 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.738931 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.738981 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.739030 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.739079 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.739129 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.739179 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.739228 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.739278 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.739327 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.739376 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.739430 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.739480 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.739530 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.739580 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.739629 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.739679 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.739728 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.739777 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.739826 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.739875 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.739924 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.739974 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.740024 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.740073 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.740122 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.740172 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.740221 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.740271 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.740320 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.740370 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.740418 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.740468 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.740518 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.740566 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.740616 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.740669 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.740719 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.740769 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.740819 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.740869 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.740918 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.740967 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.741017 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.741066 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.741115 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.741164 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.741214 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.741263 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.741312 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.741361 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.741410 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.741462 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.741513 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.741563 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.741612 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.741662 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.741711 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.741760 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.741809 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.741863 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.741914 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.741963 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.742012 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.742062 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.742112 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.742161 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.742211 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.742273 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.742329 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.742379 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.742429 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.742479 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.742529 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.742579 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.742630 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.742679 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.742728 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.742777 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.742826 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.742876 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.742925 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.742975 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.743024 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.743073 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.743128 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.743179 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.743229 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.743278 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.743327 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.743377 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.743427 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.743477 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.743526 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.743575 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.743624 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.743673 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.743722 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.743772 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.743821 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.743870 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.743920 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.743969 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.744018 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.744067 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.744116 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.744165 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.744215 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.744265 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.744315 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.744369 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.744420 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.744470 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.744519 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.744569 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.744618 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.744668 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.744718 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.744767 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.744816 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.744867 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.744916 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.744966 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.745016 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.745066 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.745116 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.745166 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.745216 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.745266 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.745316 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.745367 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.745416 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.745466 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.745516 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.745575 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.745627 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.745683 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.745735 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.745785 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.745835 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.745886 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.745936 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.745986 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.746036 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.746086 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.746137 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.746186 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.746236 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.746407 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.746460 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.746511 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.746562 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.746612 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.746661 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.746711 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.746761 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.746811 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.746861 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.746911 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.746966 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.747018 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.747068 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.747119 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.747169 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.747220 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.747270 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.747320 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.747370 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.747420 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.747470 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.747520 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.747570 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.747621 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.747671 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.747721 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.747771 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.747821 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.747870 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.747920 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.747970 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.748019 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.748069 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.748119 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.748174 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.748225 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.748276 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.748326 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.748377 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.748428 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.748478 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.748529 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.748579 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.748629 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.748679 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.748729 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.748779 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.748829 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.748878 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.748928 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.748978 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.749028 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.749078 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.749128 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.749177 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.749227 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.749276 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.749325 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.749379 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.749429 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.749479 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.749529 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.749579 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.749629 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.749679 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.749728 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.749778 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.749827 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.749877 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.749927 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.749976 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.750026 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.750076 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.750127 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.750176 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.750226 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.750297 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.750351 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.750401 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.750451 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.750501 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.750551 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.750601 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.750655 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.750706 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.750756 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.750806 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.750856 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.750906 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.750956 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.751006 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.751055 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.751105 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.751155 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.751205 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.751255 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.751305 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.751355 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.751404 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.751454 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.751504 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.751559 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.751609 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.751659 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.751709 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.751760 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.751810 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.751863 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.751914 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.751963 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.752013 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.752063 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.752113 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.752163 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.752213 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.752262 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.752313 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.752362 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.752412 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.752462 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.752511 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.752560 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.752609 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.752659 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.752709 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.752758 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.752807 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.752857 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.752907 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.752957 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.753006 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.753056 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.753110 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.753160 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.753209 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.753258 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.753308 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.753357 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.753407 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.753456 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.753505 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.753555 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.753604 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.753654 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.753703 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.753753 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.753802 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.753851 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.753901 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.753949 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.753999 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.754049 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.754098 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.754148 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.754197 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.754247 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.754320 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.754372 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.754422 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.754472 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.754522 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.754572 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.754622 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.754672 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.754721 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.754770 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.754819 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.754868 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.754918 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.754967 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.755016 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.755065 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.755115 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.755164 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.755214 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.755264 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.755313 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.755363 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.755413 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.755462 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.755513 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.755567 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.755618 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.755668 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.755718 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.755768 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.755817 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.755866 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.755916 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.755965 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.756015 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.756064 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.756114 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.756163 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.756213 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.756263 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.756314 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.756364 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.756414 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.756464 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.756514 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.756564 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.756614 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.756664 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.756714 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.756770 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.756820 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:16:41.756870 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:16:41.756920 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:16:41.756969 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:16:41.757018 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:16:41.757068 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:16:41.757118 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:16:41.757168 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:16:41.757218 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:16:41.757268 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:16:41.757317 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:16:41.757367 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:16:41.757416 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:16:41.757465 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:16:41.757514 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:16:41.757563 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I0930 09:16:41.757612 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I0930 09:16:41.757661 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I0930 09:16:41.757710 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I0930 09:16:41.757760 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I0930 09:16:41.757810 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I0930 09:16:41.757859 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I0930 09:16:41.757909 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I0930 09:16:41.757959 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I0930 09:16:41.758008 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I0930 09:16:41.758062 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I0930 09:16:41.758112 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I0930 09:16:41.758162 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I0930 09:16:41.758212 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I0930 09:16:41.758274 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I0930 09:16:41.758329 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I0930 09:16:41.758380 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I0930 09:16:41.758430 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I0930 09:16:41.758479 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I0930 09:16:41.758530 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I0930 09:16:41.758580 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I0930 09:16:41.758630 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I0930 09:16:41.758681 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I0930 09:16:41.758730 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I0930 09:16:41.758780 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I0930 09:16:41.758830 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I0930 09:16:41.758879 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I0930 09:16:41.758930 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I0930 09:16:41.758979 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I0930 09:16:41.759030 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I0930 09:16:41.759080 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I0930 09:16:41.759130 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I0930 09:16:41.759180 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I0930 09:16:41.759231 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I0930 09:16:47.147954 140009111922496 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:16:52.882448 140009111922496 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:16:58.366132 140009111922496 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:17:03.922964 140009111922496 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 09:17:14.790132 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I0930 09:17:14.790356 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.790443 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.790520 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.790588 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.790656 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.790719 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.790780 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.790841 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.790906 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.790966 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.791029 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.791090 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.791153 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.791212 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.791286 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.791348 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.791407 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.791465 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.791524 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.791587 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.791646 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.791709 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.791769 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.791828 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.791887 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.791949 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.792007 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.792068 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.792127 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.792194 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.792254 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.792315 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.792374 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.792432 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.792491 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.792550 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.792612 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.792670 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.792732 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.792792 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.792851 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.792911 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.792973 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.793032 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.793098 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.793159 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.793221 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.793281 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.793344 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.793403 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.793462 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.793520 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.793579 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.793640 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.793700 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.793762 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.793820 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.793879 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.793937 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.794004 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.794064 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.794126 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.794185 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.794247 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.794329 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.794393 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.794453 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.794512 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.794572 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.794630 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.794692 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.794752 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.794813 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.794872 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.794936 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.794997 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.795060 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.795125 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.795188 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.795248 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.795310 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.795369 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.795431 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.795490 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.795549 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.795608 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.795667 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.795729 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.795794 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.795857 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.795917 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.795976 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.796035 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.796098 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.796157 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.796219 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.796278 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.796340 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.796398 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.796461 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.796519 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.796577 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.796635 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.796699 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.796761 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.796820 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.796882 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.796941 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.797000 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.797060 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.797122 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.797181 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.797243 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.797302 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.797365 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.797424 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.797486 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.797549 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.797608 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.797666 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.797724 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.797785 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.797844 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.797905 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.797963 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.798022 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.798081 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.798143 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.798202 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.798282 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.798348 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.798410 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.798475 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.798539 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.798598 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.798657 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.798715 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.798774 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.798835 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.798895 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.798956 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.799015 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.799075 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.799134 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.799197 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.799257 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.799319 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.799382 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.799445 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.799504 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.799566 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.799624 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.799684 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.799743 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.799802 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.799863 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.799922 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.799984 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.800043 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.800101 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.800159 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.800221 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.800286 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.800349 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.800409 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.800471 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.800529 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.800592 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.800651 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.800710 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.800769 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.800827 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.800889 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.800948 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.801009 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.801070 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.801128 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.801192 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.801255 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.801314 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.801377 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.801436 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.801498 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.801557 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.801619 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.801677 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.801735 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.801793 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.801852 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.801913 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.801971 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.802037 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.802098 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.802156 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.802216 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.802294 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.802357 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.802419 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.802478 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.802540 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.802600 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.802661 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.802720 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.802778 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.802835 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.802895 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.802961 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.803021 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.803083 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.803142 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.803205 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.803265 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.803327 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.803386 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.803448 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.803506 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.803568 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.803626 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.803687 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.803745 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.803808 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.803867 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.803926 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.803988 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.804047 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.804109 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.804167 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.804225 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.804284 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.804346 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.804404 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.804465 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.804523 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.804584 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.804643 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.804708 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.804767 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.804825 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.804883 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.804942 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.805002 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.805061 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.805122 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.805186 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.805244 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.805303 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.805365 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.805425 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.805488 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.805547 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.805614 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.805674 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.805737 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.805796 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.805854 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.805913 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.805972 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.806033 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.806091 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.806153 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.806212 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.806287 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.806349 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.806412 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.806476 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.806539 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.806598 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.806660 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.806718 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.806780 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.806838 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.806898 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.806957 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.807016 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.807078 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.807137 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.807199 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.807258 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.807317 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.807380 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.807444 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.807503 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.807565 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.807625 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.807688 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.807748 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.807810 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.807869 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.807929 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.807987 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.808046 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.808108 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.808167 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.808228 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.808291 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.808351 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.808409 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.808472 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.808531 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.808592 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.808651 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.808714 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.808773 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.808835 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.808894 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.808953 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.809012 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.809071 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.809136 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.809196 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.809258 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.809317 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.809376 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.809435 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.809496 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.809556 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.809619 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.809678 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.809740 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.809799 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.809860 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.809919 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.809978 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.810041 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.810100 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.810162 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.810220 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.810302 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.810364 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.810422 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.810481 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.810543 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.810601 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.810663 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.810722 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.810785 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.810843 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.810911 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.810971 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.811029 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.811088 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.811147 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.811208 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.811267 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.811329 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.811388 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.811447 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.811506 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.811568 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.811628 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.811690 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.811749 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.811816 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.811876 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.811938 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.811997 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.812056 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.812114 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.812173 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.812235 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.812294 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.812355 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.812414 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.812473 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.812532 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.812595 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.812655 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.812723 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.812783 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.812844 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.812903 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.812965 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.813024 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.813082 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.813141 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.813199 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.813259 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.813318 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.813379 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.813439 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.813497 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.813556 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.813624 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.813684 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.813746 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.813806 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.813868 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.813927 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.813990 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.814049 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.814108 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.814167 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.814227 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.814306 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.814369 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.814431 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.814496 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.814556 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.814615 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.814677 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.814737 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.814799 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.814858 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.814920 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.814979 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.815041 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.815099 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.815158 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.815221 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.815280 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.815341 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.815405 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.815467 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.815527 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.815586 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.815644 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.815707 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.815767 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.815829 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.815889 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.815952 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.816012 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.816074 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.816133 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.816192 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.816255 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.816315 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.816377 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.816437 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.816498 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.816557 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.816615 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.816674 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.816736 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.816794 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.816857 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.816915 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.816977 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.817035 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.817097 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.817161 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.817220 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.817279 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.817337 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.817398 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.817458 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.817519 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.817578 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.817636 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.817695 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.817758 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.817818 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.817880 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.817939 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.818001 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.818065 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.818129 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.818189 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.818248 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.818326 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.818388 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.818451 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.818511 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.818573 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.818632 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.818691 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.818751 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.818814 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.818874 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.818941 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.819001 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.819064 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.819123 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.819186 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.819246 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.819305 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.819364 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.819423 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.819485 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.819545 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.819607 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.819666 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.819725 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.819783 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.819850 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.819911 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.819973 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.820032 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.820094 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.820153 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.820215 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.820273 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.820332 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.820390 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.820450 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.820511 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.820570 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.820631 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.820690 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.820754 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.820813 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.820876 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.820936 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.820998 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.821057 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.821120 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.821179 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.821241 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.821300 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.821359 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.821419 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.821478 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.821540 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.821605 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.821668 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.821728 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.821787 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.821846 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.821909 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.821968 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.822031 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.822090 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.822151 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.822211 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.822291 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.822355 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.822414 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.822473 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:17:14.822537 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:17:14.822601 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.822661 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:17:14.822721 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.822780 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.822839 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:17:14.822897 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.822959 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.823019 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.823081 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.823139 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.823201 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.823260 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:17:14.823321 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.823384 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.823444 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:17:14.823502 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.823562 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.823620 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.823679 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.823738 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.823796 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.823854 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.823913 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.823971 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.824029 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.824087 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.824144 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.824202 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.824260 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.824318 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.824376 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:17:14.824438 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.824500 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.824561 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.824624 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.824686 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.824748 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.824809 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.824870 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.824932 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.824994 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.825056 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.825118 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.825180 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.825245 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.825309 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:14.825371 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:17:24.410885 140009111922496 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f55ec50da90>>
I0930 09:17:29.429562 140009111922496 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I0930 09:17:29.432464 140009111922496 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I0930 09:17:29.437495 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I0930 09:17:29.442514 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I0930 09:17:29.447498 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I0930 09:17:29.452495 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I0930 09:17:29.457396 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I0930 09:17:29.462400 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I0930 09:17:29.467268 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I0930 09:17:29.472272 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I0930 09:17:29.477165 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I0930 09:17:29.482175 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I0930 09:17:29.487101 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I0930 09:17:29.492075 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I0930 09:17:29.496963 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I0930 09:17:29.501937 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I0930 09:17:29.505684 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I0930 09:17:29.509491 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I0930 09:17:29.514340 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I0930 09:17:29.519411 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I0930 09:17:29.524364 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I0930 09:17:29.529367 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I0930 09:17:29.534299 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I0930 09:17:29.539975 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I0930 09:17:29.544961 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I0930 09:17:29.549852 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I0930 09:17:29.554876 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I0930 09:17:29.559787 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I0930 09:17:29.564911 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I0930 09:17:29.569785 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I0930 09:17:29.574824 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I0930 09:17:29.579729 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I0930 09:17:29.584717 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I0930 09:17:29.589666 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I0930 09:17:29.594691 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I0930 09:17:29.599707 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I0930 09:17:29.604598 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I0930 09:17:29.609596 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I0930 09:17:29.614552 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I0930 09:17:29.619578 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I0930 09:17:29.624502 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I0930 09:17:29.629515 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I0930 09:17:29.634432 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I0930 09:17:29.639516 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I0930 09:17:29.644406 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I0930 09:17:29.649944 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I0930 09:17:29.654981 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I0930 09:17:29.659904 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I0930 09:17:29.664916 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I0930 09:17:29.669821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I0930 09:17:29.673557 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I0930 09:17:29.677318 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I0930 09:17:29.682154 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I0930 09:17:29.687188 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I0930 09:17:29.692225 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I0930 09:17:29.697121 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I0930 09:17:29.702122 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I0930 09:17:29.707156 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I0930 09:17:29.712150 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I0930 09:17:29.717168 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I0930 09:17:29.722196 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I0930 09:17:29.727144 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I0930 09:17:29.732155 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I0930 09:17:29.737066 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I0930 09:17:29.742152 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I0930 09:17:29.747250 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I0930 09:17:29.752164 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I0930 09:17:29.757675 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I0930 09:17:29.762595 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I0930 09:17:29.767683 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I0930 09:17:29.772619 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I0930 09:17:29.777639 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I0930 09:17:29.782589 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I0930 09:17:29.787648 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I0930 09:17:29.792612 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I0930 09:17:29.797683 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I0930 09:17:29.802926 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I0930 09:17:29.808054 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I0930 09:17:29.813190 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I0930 09:17:29.818157 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I0930 09:17:29.823217 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I0930 09:17:29.828114 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I0930 09:17:29.833148 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I0930 09:17:29.838079 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I0930 09:17:29.841959 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I0930 09:17:29.845735 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I0930 09:17:29.850632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I0930 09:17:29.855689 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I0930 09:17:29.860731 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I0930 09:17:29.865625 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I0930 09:17:29.871229 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I0930 09:17:29.876172 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I0930 09:17:29.881185 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I0930 09:17:29.886114 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I0930 09:17:29.891178 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I0930 09:17:29.896250 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I0930 09:17:29.901273 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I0930 09:17:29.906396 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I0930 09:17:29.911349 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I0930 09:17:29.916392 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I0930 09:17:29.921301 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I0930 09:17:29.926349 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I0930 09:17:29.931271 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I0930 09:17:29.936308 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I0930 09:17:29.941238 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I0930 09:17:29.946273 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I0930 09:17:29.951224 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I0930 09:17:29.956231 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I0930 09:17:29.961149 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I0930 09:17:29.966168 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I0930 09:17:29.971198 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I0930 09:17:29.976109 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I0930 09:17:29.981625 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I0930 09:17:29.986569 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I0930 09:17:29.991570 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I0930 09:17:29.996628 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I0930 09:17:30.001654 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I0930 09:17:30.006610 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I0930 09:17:30.010461 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I0930 09:17:30.014125 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I0930 09:17:30.018993 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I0930 09:17:30.024006 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I0930 09:17:30.029089 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I0930 09:17:30.033995 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I0930 09:17:30.039035 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I0930 09:17:30.043978 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I0930 09:17:30.049105 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I0930 09:17:30.054066 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I0930 09:17:30.059110 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I0930 09:17:30.064055 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I0930 09:17:30.069100 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I0930 09:17:30.074171 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I0930 09:17:30.079149 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I0930 09:17:30.084189 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I0930 09:17:30.089151 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I0930 09:17:30.094715 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I0930 09:17:30.099662 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I0930 09:17:30.104705 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I0930 09:17:30.109692 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I0930 09:17:30.114761 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I0930 09:17:30.119687 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I0930 09:17:30.124763 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I0930 09:17:30.129690 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I0930 09:17:30.134786 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I0930 09:17:30.139796 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I0930 09:17:30.144731 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I0930 09:17:30.149784 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I0930 09:17:30.154713 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I0930 09:17:30.159792 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I0930 09:17:30.164821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I0930 09:17:30.169889 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I0930 09:17:30.174879 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I0930 09:17:30.178785 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I0930 09:17:30.182477 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I0930 09:17:30.187327 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I0930 09:17:30.192382 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I0930 09:17:30.197401 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I0930 09:17:30.202425 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I0930 09:17:30.207965 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I0930 09:17:30.212929 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I0930 09:17:30.218017 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I0930 09:17:30.222992 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I0930 09:17:30.228039 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I0930 09:17:30.232968 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I0930 09:17:30.238026 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I0930 09:17:30.243087 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I0930 09:17:30.248033 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I0930 09:17:30.253109 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I0930 09:17:30.258052 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I0930 09:17:30.263101 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I0930 09:17:30.268059 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I0930 09:17:30.273151 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I0930 09:17:30.278142 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I0930 09:17:30.283200 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I0930 09:17:30.288144 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I0930 09:17:30.293150 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I0930 09:17:30.298074 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I0930 09:17:30.303143 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I0930 09:17:30.308341 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I0930 09:17:30.313264 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I0930 09:17:30.318866 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I0930 09:17:30.323840 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I0930 09:17:30.328872 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I0930 09:17:30.333881 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I0930 09:17:30.338970 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I0930 09:17:30.343915 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I0930 09:17:30.347812 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I0930 09:17:30.351498 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I0930 09:17:30.356354 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I0930 09:17:30.361415 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I0930 09:17:30.366492 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I0930 09:17:30.371517 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I0930 09:17:30.376557 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I0930 09:17:30.381522 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I0930 09:17:30.386601 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I0930 09:17:30.391541 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I0930 09:17:30.396579 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I0930 09:17:30.401537 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I0930 09:17:30.406596 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I0930 09:17:30.411624 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I0930 09:17:30.416577 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I0930 09:17:30.421637 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I0930 09:17:30.426754 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I0930 09:17:30.432356 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I0930 09:17:30.437300 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I0930 09:17:30.442383 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I0930 09:17:30.447328 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I0930 09:17:30.452358 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I0930 09:17:30.457293 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I0930 09:17:30.462414 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I0930 09:17:30.467378 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I0930 09:17:30.472439 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I0930 09:17:30.477542 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I0930 09:17:30.482528 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I0930 09:17:30.487572 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I0930 09:17:30.492557 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I0930 09:17:30.497617 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I0930 09:17:30.502589 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I0930 09:17:30.507710 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I0930 09:17:30.512715 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I0930 09:17:30.516607 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I0930 09:17:30.520432 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I0930 09:17:30.525320 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I0930 09:17:30.530427 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I0930 09:17:30.535531 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I0930 09:17:30.540500 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I0930 09:17:30.546059 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I0930 09:17:30.551052 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I0930 09:17:30.556104 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I0930 09:17:30.561055 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I0930 09:17:30.566228 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I0930 09:17:30.571212 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I0930 09:17:30.576294 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I0930 09:17:30.581366 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I0930 09:17:30.586361 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I0930 09:17:30.591500 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I0930 09:17:30.596446 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I0930 09:17:30.601478 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I0930 09:17:30.606469 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I0930 09:17:30.611535 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I0930 09:17:30.616660 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I0930 09:17:30.621677 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I0930 09:17:30.626668 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I0930 09:17:30.631742 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I0930 09:17:30.636690 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I0930 09:17:30.641794 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I0930 09:17:30.646925 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I0930 09:17:30.651883 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I0930 09:17:30.657437 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I0930 09:17:30.662419 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I0930 09:17:30.667476 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I0930 09:17:30.672453 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I0930 09:17:30.677523 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I0930 09:17:30.682497 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I0930 09:17:30.686370 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I0930 09:17:30.690052 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I0930 09:17:30.695036 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I0930 09:17:30.700125 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I0930 09:17:30.705186 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I0930 09:17:30.710228 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I0930 09:17:30.715312 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I0930 09:17:30.720292 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I0930 09:17:30.725341 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I0930 09:17:30.730308 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I0930 09:17:30.735379 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I0930 09:17:30.740364 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I0930 09:17:30.745513 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I0930 09:17:30.750620 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I0930 09:17:30.755576 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I0930 09:17:30.760667 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I0930 09:17:30.765611 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I0930 09:17:30.771221 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I0930 09:17:30.776179 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I0930 09:17:30.781280 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I0930 09:17:30.786290 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I0930 09:17:30.791331 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I0930 09:17:30.796369 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I0930 09:17:30.801420 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I0930 09:17:30.806386 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I0930 09:17:30.811473 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I0930 09:17:30.816501 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I0930 09:17:30.821551 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I0930 09:17:30.826637 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I0930 09:17:30.831621 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I0930 09:17:30.836745 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I0930 09:17:30.841736 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I0930 09:17:30.846840 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I0930 09:17:30.851868 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I0930 09:17:30.855772 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I0930 09:17:30.859495 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I0930 09:17:30.864397 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I0930 09:17:30.869513 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I0930 09:17:30.874586 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I0930 09:17:30.879536 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I0930 09:17:30.885097 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I0930 09:17:30.890071 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I0930 09:17:30.895236 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I0930 09:17:30.900178 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I0930 09:17:30.905223 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I0930 09:17:30.910274 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I0930 09:17:30.915403 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I0930 09:17:30.920420 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I0930 09:17:30.925386 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I0930 09:17:30.930471 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I0930 09:17:30.935409 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I0930 09:17:30.940469 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I0930 09:17:30.945583 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I0930 09:17:30.950683 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I0930 09:17:30.955662 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I0930 09:17:30.960709 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I0930 09:17:30.965670 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I0930 09:17:30.970742 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I0930 09:17:30.975718 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I0930 09:17:30.980763 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I0930 09:17:30.985827 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I0930 09:17:30.990798 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I0930 09:17:30.996353 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I0930 09:17:31.001308 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I0930 09:17:31.006385 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I0930 09:17:31.011356 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I0930 09:17:31.016397 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I0930 09:17:31.021365 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I0930 09:17:31.025357 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I0930 09:17:31.029075 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I0930 09:17:31.033940 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I0930 09:17:31.039050 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I0930 09:17:31.044094 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I0930 09:17:31.049068 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I0930 09:17:31.054133 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I0930 09:17:31.059125 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I0930 09:17:31.064176 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I0930 09:17:31.069115 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I0930 09:17:31.074231 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I0930 09:17:31.079206 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I0930 09:17:31.084266 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I0930 09:17:31.089323 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I0930 09:17:31.094297 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I0930 09:17:31.099383 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I0930 09:17:31.104356 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I0930 09:17:31.110007 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I0930 09:17:31.114990 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I0930 09:17:31.120098 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I0930 09:17:31.125165 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I0930 09:17:31.130244 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I0930 09:17:31.135241 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I0930 09:17:31.140292 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I0930 09:17:31.145229 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I0930 09:17:31.150357 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I0930 09:17:31.155415 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I0930 09:17:31.160375 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I0930 09:17:31.165540 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I0930 09:17:31.170533 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I0930 09:17:31.175610 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I0930 09:17:31.180584 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I0930 09:17:31.185636 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I0930 09:17:31.190632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I0930 09:17:31.194519 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I0930 09:17:31.198197 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I0930 09:17:31.203131 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I0930 09:17:31.208215 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I0930 09:17:31.213269 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I0930 09:17:31.218235 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I0930 09:17:31.223836 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I0930 09:17:31.228822 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I0930 09:17:31.233876 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I0930 09:17:31.238886 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I0930 09:17:31.243949 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I0930 09:17:31.248937 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I0930 09:17:31.254052 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I0930 09:17:31.259186 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I0930 09:17:31.264151 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I0930 09:17:31.269231 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I0930 09:17:31.274186 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I0930 09:17:31.279287 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I0930 09:17:31.284237 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I0930 09:17:31.289332 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I0930 09:17:31.294360 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I0930 09:17:31.299417 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I0930 09:17:31.304396 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I0930 09:17:31.309519 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I0930 09:17:31.314503 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I0930 09:17:31.319585 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I0930 09:17:31.324650 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I0930 09:17:31.329704 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I0930 09:17:31.335284 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I0930 09:17:31.340249 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I0930 09:17:31.345323 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I0930 09:17:31.350301 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I0930 09:17:31.355362 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I0930 09:17:31.360346 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I0930 09:17:31.364270 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I0930 09:17:31.367978 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I0930 09:17:31.372876 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I0930 09:17:31.377942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I0930 09:17:31.383108 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I0930 09:17:31.388082 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I0930 09:17:31.393146 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I0930 09:17:31.398124 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I0930 09:17:31.403218 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I0930 09:17:31.408182 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I0930 09:17:31.413244 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I0930 09:17:31.418253 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I0930 09:17:31.423359 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I0930 09:17:31.428410 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I0930 09:17:31.433464 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I0930 09:17:31.438566 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I0930 09:17:31.443554 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I0930 09:17:31.449118 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I0930 09:17:31.454092 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I0930 09:17:31.459223 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I0930 09:17:31.464201 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I0930 09:17:31.469276 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I0930 09:17:31.474237 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I0930 09:17:31.479333 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I0930 09:17:31.484353 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I0930 09:17:31.489434 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I0930 09:17:31.494522 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I0930 09:17:31.499486 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I0930 09:17:31.504577 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I0930 09:17:31.509663 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I0930 09:17:31.514762 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I0930 09:17:31.519804 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I0930 09:17:31.524850 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I0930 09:17:31.529814 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I0930 09:17:31.533722 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I0930 09:17:31.537512 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I0930 09:17:31.542440 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I0930 09:17:31.547528 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I0930 09:17:31.552603 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I0930 09:17:31.557683 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I0930 09:17:31.563287 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I0930 09:17:31.568352 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I0930 09:17:31.573455 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I0930 09:17:31.578453 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I0930 09:17:31.583504 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I0930 09:17:31.588509 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I0930 09:17:31.593582 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I0930 09:17:31.598671 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I0930 09:17:31.603636 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I0930 09:17:31.608747 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I0930 09:17:31.613724 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I0930 09:17:31.618822 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I0930 09:17:31.623793 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I0930 09:17:31.628908 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I0930 09:17:31.633932 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I0930 09:17:31.639043 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I0930 09:17:31.644018 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I0930 09:17:31.649087 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I0930 09:17:31.654053 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I0930 09:17:31.659211 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I0930 09:17:31.664277 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I0930 09:17:31.669261 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I0930 09:17:31.674872 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I0930 09:17:31.679844 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I0930 09:17:31.684960 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I0930 09:17:31.689951 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I0930 09:17:31.695064 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I0930 09:17:31.700048 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I0930 09:17:31.703969 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I0930 09:17:31.707805 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I0930 09:17:31.712692 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I0930 09:17:31.717809 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I0930 09:17:31.722933 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I0930 09:17:31.727905 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I0930 09:17:31.733003 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I0930 09:17:31.738011 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I0930 09:17:31.743148 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I0930 09:17:31.748119 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I0930 09:17:31.753211 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I0930 09:17:31.758224 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I0930 09:17:31.763359 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I0930 09:17:31.768467 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I0930 09:17:31.773459 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I0930 09:17:31.778563 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I0930 09:17:31.783548 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I0930 09:17:31.789123 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I0930 09:17:31.794104 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I0930 09:17:31.799225 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I0930 09:17:31.804304 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I0930 09:17:31.809426 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I0930 09:17:31.814470 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I0930 09:17:31.819544 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I0930 09:17:31.824539 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I0930 09:17:31.829627 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I0930 09:17:31.834729 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I0930 09:17:31.839739 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I0930 09:17:31.844821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I0930 09:17:31.849839 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I0930 09:17:31.854997 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I0930 09:17:31.860022 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I0930 09:17:31.865103 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I0930 09:17:31.870092 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I0930 09:17:31.874033 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I0930 09:17:31.877779 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I0930 09:17:31.882710 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I0930 09:17:31.887812 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I0930 09:17:31.892912 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I0930 09:17:31.897983 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I0930 09:17:31.903633 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I0930 09:17:31.908694 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I0930 09:17:31.913792 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I0930 09:17:31.918866 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I0930 09:17:31.923956 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I0930 09:17:31.928974 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I0930 09:17:31.934134 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I0930 09:17:31.939396 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I0930 09:17:31.944426 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I0930 09:17:31.949553 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I0930 09:17:31.954575 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I0930 09:17:31.959661 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I0930 09:17:31.964659 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I0930 09:17:31.969794 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I0930 09:17:31.974821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I0930 09:17:31.979904 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I0930 09:17:31.984875 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I0930 09:17:31.989946 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I0930 09:17:31.994973 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I0930 09:17:32.000056 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I0930 09:17:32.005128 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I0930 09:17:32.010111 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I0930 09:17:32.015742 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I0930 09:17:32.020747 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I0930 09:17:32.025865 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I0930 09:17:32.030913 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I0930 09:17:32.036060 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I0930 09:17:32.041080 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I0930 09:17:32.045050 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I0930 09:17:32.048816 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I0930 09:17:32.053740 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I0930 09:17:32.058920 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I0930 09:17:32.064027 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I0930 09:17:32.069043 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I0930 09:17:32.074198 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I0930 09:17:32.079247 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I0930 09:17:32.084386 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I0930 09:17:32.089390 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I0930 09:17:32.094542 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I0930 09:17:32.099584 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I0930 09:17:32.104712 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I0930 09:17:32.109858 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I0930 09:17:32.114919 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I0930 09:17:32.120064 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I0930 09:17:32.125120 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I0930 09:17:32.130829 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I0930 09:17:32.135921 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I0930 09:17:32.141108 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I0930 09:17:32.146216 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I0930 09:17:32.151421 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I0930 09:17:32.156486 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I0930 09:17:32.161690 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I0930 09:17:32.166806 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I0930 09:17:32.172004 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I0930 09:17:32.177205 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I0930 09:17:32.182297 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I0930 09:17:32.187491 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I0930 09:17:32.192552 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I0930 09:17:32.197727 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I0930 09:17:32.202816 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I0930 09:17:32.207970 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I0930 09:17:32.213037 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I0930 09:17:32.217021 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I0930 09:17:32.220820 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I0930 09:17:32.225770 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I0930 09:17:32.231005 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I0930 09:17:32.236177 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I0930 09:17:32.241226 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I0930 09:17:32.246928 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I0930 09:17:32.251970 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I0930 09:17:32.257097 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I0930 09:17:32.262124 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I0930 09:17:32.267266 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I0930 09:17:32.272297 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I0930 09:17:32.277396 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I0930 09:17:32.282536 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I0930 09:17:32.287546 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I0930 09:17:32.292673 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I0930 09:17:32.297701 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I0930 09:17:32.302849 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I0930 09:17:32.308059 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I0930 09:17:32.313184 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I0930 09:17:32.318183 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I0930 09:17:32.323330 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I0930 09:17:32.328356 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I0930 09:17:32.333505 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I0930 09:17:32.338545 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I0930 09:17:32.343652 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I0930 09:17:32.348744 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I0930 09:17:32.353758 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I0930 09:17:32.359491 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I0930 09:17:32.364521 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I0930 09:17:32.369625 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I0930 09:17:32.374672 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I0930 09:17:32.379767 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I0930 09:17:32.384790 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I0930 09:17:32.388738 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I0930 09:17:32.392534 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I0930 09:17:32.397479 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I0930 09:17:32.402601 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I0930 09:17:32.407719 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I0930 09:17:32.412829 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I0930 09:17:32.417963 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I0930 09:17:32.423010 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I0930 09:17:32.428116 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I0930 09:17:32.433143 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I0930 09:17:32.438229 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I0930 09:17:32.443299 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I0930 09:17:32.448420 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I0930 09:17:32.453516 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I0930 09:17:32.458639 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I0930 09:17:32.463758 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I0930 09:17:32.468803 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I0930 09:17:32.474401 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I0930 09:17:32.479410 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I0930 09:17:32.484561 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I0930 09:17:32.489607 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I0930 09:17:32.494740 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I0930 09:17:32.499774 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I0930 09:17:32.504900 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I0930 09:17:32.510014 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I0930 09:17:32.515154 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I0930 09:17:32.520334 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I0930 09:17:32.525354 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I0930 09:17:32.530508 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I0930 09:17:32.535620 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I0930 09:17:32.540724 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I0930 09:17:32.545745 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I0930 09:17:32.550867 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I0930 09:17:32.555914 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I0930 09:17:32.559882 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I0930 09:17:32.563761 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I0930 09:17:32.568689 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I0930 09:17:32.573832 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I0930 09:17:32.578979 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I0930 09:17:32.584007 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I0930 09:17:32.589671 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I0930 09:17:32.594748 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I0930 09:17:32.599870 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I0930 09:17:32.604913 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I0930 09:17:32.610052 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I0930 09:17:32.615133 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I0930 09:17:32.620262 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I0930 09:17:32.625406 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I0930 09:17:32.630446 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I0930 09:17:32.635565 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I0930 09:17:32.640633 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I0930 09:17:32.645755 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I0930 09:17:32.650798 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I0930 09:17:32.655973 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I0930 09:17:32.660993 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I0930 09:17:32.666105 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I0930 09:17:32.671147 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I0930 09:17:32.676272 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I0930 09:17:32.681295 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I0930 09:17:32.686482 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I0930 09:17:32.691614 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I0930 09:17:32.696649 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I0930 09:17:32.702339 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I0930 09:17:32.707421 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I0930 09:17:32.712628 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I0930 09:17:32.717698 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I0930 09:17:32.722841 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I0930 09:17:32.727886 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I0930 09:17:32.731865 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I0930 09:17:32.735646 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I0930 09:17:32.740599 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I0930 09:17:32.745796 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I0930 09:17:32.750955 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I0930 09:17:32.755994 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I0930 09:17:32.761123 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I0930 09:17:32.766199 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I0930 09:17:32.771383 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I0930 09:17:32.776445 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I0930 09:17:32.781590 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I0930 09:17:32.786651 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I0930 09:17:32.791798 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I0930 09:17:32.796917 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I0930 09:17:32.802002 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I0930 09:17:32.807172 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I0930 09:17:32.812223 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I0930 09:17:32.817836 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I0930 09:17:32.822895 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I0930 09:17:32.828039 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I0930 09:17:32.833059 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I0930 09:17:32.838195 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I0930 09:17:32.843225 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I0930 09:17:32.848386 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I0930 09:17:32.853458 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I0930 09:17:32.858651 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I0930 09:17:32.863777 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I0930 09:17:32.868804 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I0930 09:17:32.873945 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I0930 09:17:32.879016 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I0930 09:17:32.884134 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I0930 09:17:32.889210 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I0930 09:17:32.894455 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I0930 09:17:32.899496 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I0930 09:17:32.903471 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I0930 09:17:32.907251 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I0930 09:17:32.912253 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I0930 09:17:32.917394 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I0930 09:17:32.922563 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I0930 09:17:32.927644 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I0930 09:17:32.933276 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I0930 09:17:32.938341 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I0930 09:17:32.943492 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I0930 09:17:32.948555 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I0930 09:17:32.953723 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I0930 09:17:32.958821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I0930 09:17:32.963935 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I0930 09:17:32.969055 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I0930 09:17:32.974096 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I0930 09:17:32.979275 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I0930 09:17:32.984312 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I0930 09:17:32.989451 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I0930 09:17:32.994483 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I0930 09:17:32.999661 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I0930 09:17:33.004680 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I0930 09:17:33.009821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I0930 09:17:33.014893 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I0930 09:17:33.020037 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I0930 09:17:33.025048 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I0930 09:17:33.030244 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I0930 09:17:33.035388 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I0930 09:17:33.040410 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I0930 09:17:33.046025 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I0930 09:17:33.051104 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I0930 09:17:33.056313 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I0930 09:17:33.061357 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I0930 09:17:33.066523 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I0930 09:17:33.071555 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I0930 09:17:33.075555 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I0930 09:17:33.079353 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I0930 09:17:33.084319 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I0930 09:17:33.089516 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I0930 09:17:33.094664 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I0930 09:17:33.099686 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I0930 09:17:33.104843 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I0930 09:17:33.109975 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I0930 09:17:33.115142 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I0930 09:17:33.120187 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I0930 09:17:33.125310 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I0930 09:17:33.130401 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I0930 09:17:33.135551 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I0930 09:17:33.140690 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I0930 09:17:33.145712 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I0930 09:17:33.150921 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I0930 09:17:33.155976 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I0930 09:17:33.161634 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I0930 09:17:33.166708 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I0930 09:17:33.171888 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I0930 09:17:33.176929 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I0930 09:17:33.182053 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I0930 09:17:33.187136 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I0930 09:17:33.192291 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I0930 09:17:33.197351 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I0930 09:17:33.202517 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I0930 09:17:33.207664 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I0930 09:17:33.212716 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I0930 09:17:33.217882 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I0930 09:17:33.222960 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I0930 09:17:33.228114 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I0930 09:17:33.233160 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I0930 09:17:33.238304 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I0930 09:17:33.243371 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I0930 09:17:33.247362 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I0930 09:17:33.251184 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I0930 09:17:33.256164 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I0930 09:17:33.261330 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I0930 09:17:33.266498 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I0930 09:17:33.271531 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I0930 09:17:33.277152 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I0930 09:17:33.282241 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I0930 09:17:33.287457 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I0930 09:17:33.292553 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I0930 09:17:33.297700 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I0930 09:17:33.302803 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I0930 09:17:33.307941 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I0930 09:17:33.313179 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I0930 09:17:33.318317 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I0930 09:17:33.323477 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I0930 09:17:33.328544 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I0930 09:17:33.333693 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I0930 09:17:33.338765 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I0930 09:17:33.343953 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I0930 09:17:33.349015 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I0930 09:17:33.354177 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I0930 09:17:33.359223 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I0930 09:17:33.364394 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I0930 09:17:33.369447 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I0930 09:17:33.374690 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I0930 09:17:33.379817 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I0930 09:17:33.384863 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I0930 09:17:33.390538 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I0930 09:17:33.395601 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I0930 09:17:33.400727 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I0930 09:17:33.405799 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I0930 09:17:33.410957 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I0930 09:17:33.416008 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I0930 09:17:33.420010 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I0930 09:17:33.423841 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I0930 09:17:33.428802 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I0930 09:17:33.433970 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I0930 09:17:33.439146 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I0930 09:17:33.444195 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I0930 09:17:33.449366 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I0930 09:17:33.454457 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I0930 09:17:33.459611 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I0930 09:17:33.464667 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I0930 09:17:33.469814 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I0930 09:17:33.474926 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I0930 09:17:33.480054 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I0930 09:17:33.485206 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I0930 09:17:33.490251 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I0930 09:17:33.495456 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I0930 09:17:33.500534 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I0930 09:17:33.506148 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I0930 09:17:33.511313 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I0930 09:17:33.516503 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I0930 09:17:33.521639 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I0930 09:17:33.526808 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I0930 09:17:33.531887 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I0930 09:17:33.537096 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I0930 09:17:33.542177 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I0930 09:17:33.547408 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I0930 09:17:33.552573 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I0930 09:17:33.557642 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I0930 09:17:33.562846 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I0930 09:17:33.568024 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I0930 09:17:33.573184 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I0930 09:17:33.578248 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I0930 09:17:33.583453 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I0930 09:17:33.588508 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I0930 09:17:33.592489 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I0930 09:17:33.596314 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I0930 09:17:33.601290 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I0930 09:17:33.606487 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I0930 09:17:33.611661 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I0930 09:17:33.616725 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I0930 09:17:33.622431 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I0930 09:17:33.627533 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I0930 09:17:33.632714 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I0930 09:17:33.637786 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I0930 09:17:33.642946 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I0930 09:17:33.648032 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I0930 09:17:33.653211 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I0930 09:17:33.658392 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I0930 09:17:33.663471 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I0930 09:17:33.668622 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I0930 09:17:33.673691 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I0930 09:17:33.678968 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I0930 09:17:33.684026 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I0930 09:17:33.689208 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I0930 09:17:33.694309 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I0930 09:17:33.699481 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I0930 09:17:33.704530 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I0930 09:17:33.709756 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I0930 09:17:33.714843 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I0930 09:17:33.720006 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I0930 09:17:33.725155 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I0930 09:17:33.730242 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I0930 09:17:33.735943 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I0930 09:17:33.741003 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I0930 09:17:33.746177 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I0930 09:17:33.751287 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I0930 09:17:33.756446 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I0930 09:17:33.761522 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I0930 09:17:33.765537 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I0930 09:17:33.769388 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I0930 09:17:33.774404 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I0930 09:17:33.779568 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I0930 09:17:33.784756 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I0930 09:17:33.789810 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I0930 09:17:33.795014 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I0930 09:17:33.800112 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I0930 09:17:33.805262 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I0930 09:17:33.810363 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I0930 09:17:33.815531 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I0930 09:17:33.820632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I0930 09:17:33.825823 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I0930 09:17:33.831001 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I0930 09:17:33.836090 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I0930 09:17:33.841245 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I0930 09:17:33.846350 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I0930 09:17:33.852024 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I0930 09:17:33.857087 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I0930 09:17:33.862309 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I0930 09:17:33.867393 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I0930 09:17:33.872580 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I0930 09:17:33.877632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I0930 09:17:33.882827 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I0930 09:17:33.887910 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I0930 09:17:33.893108 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I0930 09:17:33.898371 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I0930 09:17:33.903459 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I0930 09:17:33.908690 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I0930 09:17:33.913817 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I0930 09:17:33.919029 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I0930 09:17:33.924142 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I0930 09:17:33.929307 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I0930 09:17:33.934400 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I0930 09:17:33.938525 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I0930 09:17:33.942361 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I0930 09:17:33.947373 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I0930 09:17:33.952568 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I0930 09:17:33.957768 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I0930 09:17:33.962869 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I0930 09:17:33.968545 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I0930 09:17:33.973642 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I0930 09:17:34.505772 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I0930 09:17:34.511309 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I0930 09:17:34.516496 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I0930 09:17:34.521739 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I0930 09:17:34.526875 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I0930 09:17:34.532064 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I0930 09:17:34.537229 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I0930 09:17:34.542453 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I0930 09:17:34.547560 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I0930 09:17:34.552753 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I0930 09:17:34.557821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I0930 09:17:34.563117 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I0930 09:17:34.568300 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I0930 09:17:34.573507 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I0930 09:17:34.578752 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I0930 09:17:34.583837 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I0930 09:17:34.589060 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I0930 09:17:34.594183 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I0930 09:17:34.599418 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I0930 09:17:34.604497 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I0930 09:17:34.609701 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I0930 09:17:34.614829 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I0930 09:17:34.620625 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I0930 09:17:34.625734 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I0930 09:17:34.630939 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I0930 09:17:34.636101 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I0930 09:17:34.640075 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I0930 09:17:34.643932 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I0930 09:17:34.649061 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I0930 09:17:34.654155 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I0930 09:17:34.659361 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I0930 09:17:34.664432 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I0930 09:17:34.669620 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I0930 09:17:34.674851 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I0930 09:17:34.679942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I0930 09:17:34.685125 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I0930 09:17:34.690200 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I0930 09:17:34.695457 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I0930 09:17:34.700641 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I0930 09:17:34.705814 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I0930 09:17:34.711009 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I0930 09:17:34.716188 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I0930 09:17:34.721300 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I0930 09:17:34.726531 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I0930 09:17:34.732210 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I0930 09:17:34.737344 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I0930 09:17:34.742587 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I0930 09:17:34.747702 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I0930 09:17:34.752889 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I0930 09:17:34.758011 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I0930 09:17:34.763217 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I0930 09:17:34.768319 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I0930 09:17:34.773520 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I0930 09:17:34.778674 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I0930 09:17:34.783847 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I0930 09:17:34.788968 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I0930 09:17:34.794147 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I0930 09:17:34.799365 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I0930 09:17:34.804489 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I0930 09:17:34.809686 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I0930 09:17:34.813626 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I0930 09:17:34.817481 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I0930 09:17:34.822597 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I0930 09:17:34.827722 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I0930 09:17:34.832904 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I0930 09:17:34.838059 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I0930 09:17:34.843180 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I0930 09:17:34.848881 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I0930 09:17:34.853990 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I0930 09:17:34.859246 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I0930 09:17:34.864359 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I0930 09:17:34.869575 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I0930 09:17:34.874701 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I0930 09:17:34.879898 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I0930 09:17:34.884994 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I0930 09:17:34.890205 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I0930 09:17:34.895496 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I0930 09:17:34.900588 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I0930 09:17:34.905774 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I0930 09:17:34.911085 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I0930 09:17:34.916311 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I0930 09:17:34.921417 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I0930 09:17:34.926633 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I0930 09:17:34.931720 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I0930 09:17:34.936930 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I0930 09:17:34.942055 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I0930 09:17:34.947282 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I0930 09:17:34.952472 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I0930 09:17:34.957550 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I0930 09:17:34.963283 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I0930 09:17:34.968388 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I0930 09:17:34.973598 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I0930 09:17:34.978749 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I0930 09:17:34.983927 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I0930 09:17:34.987900 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I0930 09:17:34.991761 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I0930 09:17:34.996919 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I0930 09:17:35.002137 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I0930 09:17:35.007274 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I0930 09:17:35.012444 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I0930 09:17:35.017550 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I0930 09:17:35.022789 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I0930 09:17:35.027885 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I0930 09:17:35.033097 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I0930 09:17:35.038195 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I0930 09:17:35.043498 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I0930 09:17:35.048592 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I0930 09:17:35.053789 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I0930 09:17:35.059048 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I0930 09:17:35.064132 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I0930 09:17:35.069391 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I0930 09:17:35.074507 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I0930 09:17:35.080188 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I0930 09:17:35.085357 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I0930 09:17:35.090588 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I0930 09:17:35.095703 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I0930 09:17:35.100910 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I0930 09:17:35.106045 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I0930 09:17:35.111322 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I0930 09:17:35.116462 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I0930 09:17:35.121687 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I0930 09:17:35.126913 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I0930 09:17:35.132006 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I0930 09:17:35.137231 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I0930 09:17:35.142344 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I0930 09:17:35.147560 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I0930 09:17:35.152662 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I0930 09:17:35.157876 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I0930 09:17:35.161870 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I0930 09:17:35.165738 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I0930 09:17:35.170921 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I0930 09:17:35.176136 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I0930 09:17:35.181260 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I0930 09:17:35.186470 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I0930 09:17:35.191575 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I0930 09:17:35.197288 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I0930 09:17:35.202421 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I0930 09:17:35.207664 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I0930 09:17:35.212773 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I0930 09:17:35.217994 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I0930 09:17:35.223137 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I0930 09:17:35.228326 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I0930 09:17:35.233512 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I0930 09:17:35.238651 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I0930 09:17:35.243839 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I0930 09:17:35.248935 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I0930 09:17:35.254120 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I0930 09:17:35.259284 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I0930 09:17:35.264500 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I0930 09:17:35.269615 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I0930 09:17:35.274857 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I0930 09:17:35.280017 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I0930 09:17:35.285221 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I0930 09:17:35.290357 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I0930 09:17:35.295586 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I0930 09:17:35.300812 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I0930 09:17:35.305936 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I0930 09:17:35.311774 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I0930 09:17:35.316878 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I0930 09:17:35.322077 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I0930 09:17:35.327194 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I0930 09:17:35.332438 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I0930 09:17:35.336407 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I0930 09:17:35.340284 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I0930 09:17:35.345428 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I0930 09:17:35.350684 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I0930 09:17:35.355860 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I0930 09:17:35.361075 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I0930 09:17:35.366206 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I0930 09:17:35.371472 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I0930 09:17:35.376579 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I0930 09:17:35.381816 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I0930 09:17:35.386955 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I0930 09:17:35.392203 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I0930 09:17:35.397331 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I0930 09:17:35.402552 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I0930 09:17:35.407778 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I0930 09:17:35.412917 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I0930 09:17:35.418144 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I0930 09:17:35.423276 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I0930 09:17:35.428955 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I0930 09:17:35.434229 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I0930 09:17:35.439510 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I0930 09:17:35.444615 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I0930 09:17:35.449843 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I0930 09:17:35.455016 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I0930 09:17:35.460212 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I0930 09:17:35.465343 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I0930 09:17:35.470564 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I0930 09:17:35.475795 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I0930 09:17:35.480927 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I0930 09:17:35.486138 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I0930 09:17:35.491290 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I0930 09:17:35.496515 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I0930 09:17:35.501638 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I0930 09:17:35.506855 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I0930 09:17:35.512066 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I0930 09:17:35.517285 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I0930 09:17:35.522478 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I0930 09:17:35.527707 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I0930 09:17:35.532817 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I0930 09:17:35.538107 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I0930 09:17:35.543839 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I0930 09:17:35.548973 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I0930 09:17:35.554196 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I0930 09:17:35.559316 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I0930 09:17:35.564619 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I0930 09:17:35.569767 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I0930 09:17:35.575031 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I0930 09:17:35.580158 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I0930 09:17:35.585386 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I0930 09:17:35.590536 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I0930 09:17:35.595739 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I0930 09:17:35.600879 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I0930 09:17:35.606115 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I0930 09:17:35.611370 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I0930 09:17:35.616507 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I0930 09:17:35.621738 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I0930 09:17:35.626908 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I0930 09:17:35.632167 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I0930 09:17:35.637295 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I0930 09:17:35.642568 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I0930 09:17:35.647728 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I0930 09:17:35.652981 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I0930 09:17:35.658109 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I0930 09:17:35.663864 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I0930 09:17:35.669086 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I0930 09:17:35.674225 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I0930 09:17:35.679511 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I0930 09:17:35.684680 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I0930 09:17:35.689908 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I0930 09:17:35.695095 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I0930 09:17:35.700336 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I0930 09:17:35.705584 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I0930 09:17:35.710904 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I0930 09:17:35.716050 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I0930 09:17:35.721272 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I0930 09:17:35.726552 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I0930 09:17:35.731694 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I0930 09:17:35.736949 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I0930 09:17:35.742114 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I0930 09:17:35.747399 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I0930 09:17:35.752551 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I0930 09:17:35.757811 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I0930 09:17:35.762993 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I0930 09:17:35.768217 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I0930 09:17:35.773348 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I0930 09:17:35.779139 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I0930 09:17:36.506132 140009111922496 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I0930 09:17:36.508233 140009111922496 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:39.100370 140009111922496 py_utils.py:1474] MODEL ANALYSIS: 
I0930 09:17:39.100527 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.emb.src_token_emb.wm                               (32000, 2048)          65536000 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var
I0930 09:17:39.100592 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.100647 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.100696 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.100743 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.100789 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.100835 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.100880 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.100936 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.100984 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.101030 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.101075 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.101120 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.101166 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.101211 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.101256 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.101301 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.101347 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.101393 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.101438 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.101484 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.101529 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.101575 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.101620 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.101671 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.101717 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.101762 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.101808 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.101853 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.101899 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.101944 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.101989 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.102035 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.102080 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.102125 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.102171 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.102216 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.102277 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.102329 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.102380 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.102427 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.102473 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.102518 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.102563 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.102608 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.102653 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.102698 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.102743 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.102788 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.102833 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.102878 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.102923 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.102969 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.103014 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.103063 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.103109 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.103153 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.103198 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.103243 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.103288 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.103333 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.103378 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.103423 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.103468 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.103513 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.103558 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.103602 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.103647 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.103692 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.103737 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.103785 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.103831 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.103876 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.103921 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.103966 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.104011 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.104056 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.104101 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.104146 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.104191 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.104236 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.104281 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.104326 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.104372 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.104417 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.104467 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.104512 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.104557 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.104602 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.104652 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.104698 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.104743 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.104788 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.104833 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.104878 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.104923 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.104968 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.105013 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.105057 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.105103 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.105155 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.105201 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.105246 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.105290 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.105335 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.105389 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.105434 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.105479 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.105524 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.105568 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.105612 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.105657 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.105701 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.105746 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.105791 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.105835 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.105885 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.105930 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.105974 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.106019 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.106064 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.106109 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.106153 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.106198 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.106243 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.106306 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.106352 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.106397 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.106441 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.106487 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.106531 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.106580 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.106626 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.106671 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.106715 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.106760 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.106806 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.106851 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.106895 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.106939 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.106984 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.107028 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.107073 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.107117 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.107162 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.107207 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.107252 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.107301 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.107347 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.107392 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.107436 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.107480 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.107525 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.107569 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.107614 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.107659 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.107703 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.107747 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.107791 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.107836 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.107880 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.107925 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.107974 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.108020 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.108065 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.108109 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.108153 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.108198 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.108243 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.108287 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.108332 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.108376 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.108421 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.108465 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.108510 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.108554 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.108598 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.108642 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.108691 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.108736 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.108781 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.108826 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.108870 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.108916 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.108960 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.109005 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.109050 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.109095 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.109139 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.109184 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.109229 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.109274 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.109319 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.109367 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.109413 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.109458 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.109503 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.109548 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.109593 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.109638 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.109683 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.109727 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.109772 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.109817 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.109862 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.109907 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.109952 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.109997 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.110046 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.110091 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.110136 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.110180 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.110225 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.110291 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.110340 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.110386 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.110431 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.110477 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.110522 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.110567 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.110613 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.110658 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.110703 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.110749 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.110798 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.110844 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.110889 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.110934 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.110978 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.111024 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.111069 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.111114 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.111159 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.111204 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.111249 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.111293 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.111338 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.111382 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.111427 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.111476 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.111522 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.111587 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.111660 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.111732 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.111796 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.111844 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.111890 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.111935 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.111980 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.112025 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.112070 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.112115 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.112160 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.112205 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.112256 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.112302 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.112348 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                  (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.112392 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                  (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.112437 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.112482 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                  (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.112527 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                  (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.112572 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.112620 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale     (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.112665 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj           (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.112711 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b         (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.112756 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.112801 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.112847 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj              (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.112891 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b            (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.112936 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.112986 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.113031 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias               (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.113076 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.113121 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.113166 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.113211 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.113256 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.113301 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.113346 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.113391 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.113437 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.113482 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.113528 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.113573 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.113618 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.113667 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.113712 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.113756 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.113801 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.113846 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.113890 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.113935 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.113979 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.114023 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.114068 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.114112 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.114156 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.114201 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.114246 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.114312 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.114360 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.114409 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.114456 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.114501 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.114546 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.114591 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.114635 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.114684 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.114730 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.114775 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.114819 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.114864 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.114909 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.114954 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.114999 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.115043 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.115092 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.115137 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.115182 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.115226 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.115270 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.115315 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.115360 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.115405 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.115450 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.115495 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.115540 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.115585 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.115630 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.115675 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.115719 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.115768 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.115814 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.115859 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.115905 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.115950 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.115995 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.116040 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.116085 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.116130 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.116175 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.116220 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.116265 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.116309 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.116354 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.116398 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.116443 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.116492 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.116537 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.116582 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.116627 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.116672 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.116716 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.116761 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.116806 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.116850 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.116895 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.116940 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.116986 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.117031 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.117076 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.117121 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.117170 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.117216 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.117261 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.117306 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.117351 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.117396 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.117441 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.117486 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.117532 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.117577 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.117622 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.117667 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.117713 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.117758 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.117804 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.117852 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.117898 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.117942 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.117987 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.118032 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.118077 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.118122 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.118168 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.118213 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.118270 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.118321 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.118367 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.118412 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.118457 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.118503 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.118548 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.118598 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.118644 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.118689 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.118734 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.118779 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.118824 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.118869 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.118914 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.118958 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.119003 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.119048 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.119092 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.119137 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.119182 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.119227 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.119277 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.119322 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.119367 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.119412 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.119457 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.119501 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.119546 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.119590 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.119635 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.119679 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.119724 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.119769 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.119814 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.119858 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.119903 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.119952 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.119997 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.120042 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.120086 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.120131 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.120175 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.120220 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.120264 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.120308 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.120352 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.120397 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.120442 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.120487 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.120532 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.120576 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.120621 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.120669 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.120714 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.120759 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.120804 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.120849 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.120894 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.120938 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.120983 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.121027 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.121072 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.121116 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.121160 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.121205 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.121249 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.121293 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.121345 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.121391 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.121436 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.121481 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.121525 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.121570 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.121614 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.121659 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.121704 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.121748 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.121792 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.121836 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.121880 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.121924 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.121969 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.122014 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.122062 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.122108 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.122152 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.122197 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.122242 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.122308 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.122355 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.122401 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.122445 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.122490 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.122535 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.122579 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.122623 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.122668 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.122713 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.122763 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.122809 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.122854 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.122899 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.122944 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.122988 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.123033 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.123077 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.123122 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.123167 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.123212 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.123257 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.123302 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.123347 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.123392 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.123440 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.123486 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.123531 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.123575 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.123620 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.123664 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.123709 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.123753 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.123798 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.123842 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.123886 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.123931 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.123975 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.124019 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.124063 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.124108 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.124156 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.124201 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.124244 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.124289 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.124333 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.124378 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.124422 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.124466 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.124509 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.124553 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.124598 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.124642 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.124686 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                 (8192,)                    8192 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var
I0930 09:17:39.124734 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                 (2048, 8192)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var
I0930 09:17:39.124779 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var
I0930 09:17:39.124829 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                 (8192, 2048)           16777216 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var
I0930 09:17:39.124874 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                 (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var
I0930 09:17:39.124918 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var
I0930 09:17:39.124962 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale    (128,)                      128 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var
I0930 09:17:39.125006 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj          (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var
I0930 09:17:39.125051 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b        (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var
I0930 09:17:39.125096 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj               (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var
I0930 09:17:39.125141 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var
I0930 09:17:39.125185 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj             (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var
I0930 09:17:39.125230 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b           (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var
I0930 09:17:39.125275 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj            (2048, 2048)            4194304 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var
I0930 09:17:39.125320 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b          (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var
I0930 09:17:39.125364 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias              (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var
I0930 09:17:39.125409 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale             (2048,)                    2048 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var
I0930 09:17:39.125453 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_0                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var
I0930 09:17:39.125504 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_1                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var
I0930 09:17:39.125548 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_10                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var
I0930 09:17:39.125592 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_11                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var
I0930 09:17:39.125636 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_12                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var
I0930 09:17:39.125681 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_13                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var
I0930 09:17:39.125725 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_14                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var
I0930 09:17:39.125769 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_15                                    (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var
I0930 09:17:39.125814 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_2                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var
I0930 09:17:39.125858 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_3                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var
I0930 09:17:39.125903 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_4                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var
I0930 09:17:39.125947 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_5                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var
I0930 09:17:39.125991 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_6                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var
I0930 09:17:39.126035 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_7                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var
I0930 09:17:39.126079 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_8                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var
I0930 09:17:39.126124 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.bias_9                                     (2000,)                    2000 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var
I0930 09:17:39.126168 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_0                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var
I0930 09:17:39.126212 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_1                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var
I0930 09:17:39.126268 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_10                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var
I0930 09:17:39.126323 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_11                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var
I0930 09:17:39.126369 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_12                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var
I0930 09:17:39.126414 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_13                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var
I0930 09:17:39.126459 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_14                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var
I0930 09:17:39.126503 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_15                                  (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var
I0930 09:17:39.126548 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_2                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var
I0930 09:17:39.126593 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_3                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var
I0930 09:17:39.126638 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_4                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var
I0930 09:17:39.126683 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_5                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var
I0930 09:17:39.126728 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_6                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var
I0930 09:17:39.126772 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_7                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var
I0930 09:17:39.126816 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_8                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var
I0930 09:17:39.126862 140009111922496 py_utils.py:1474] MODEL ANALYSIS: _task.lm.stack.cell_3.softmax.weight_9                                   (2048, 2000)            4096000 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var
I0930 09:17:39.126906 140009111922496 py_utils.py:1474] MODEL ANALYSIS: ====================================================================================================
I0930 09:17:39.126950 140009111922496 py_utils.py:1474] MODEL ANALYSIS: total #params: 1742572800
I0930 09:17:39.126995 140009111922496 py_utils.py:1474] MODEL ANALYSIS: 
I0930 09:17:58.905254 140009111922496 trainer.py:1502] Job trainer_client start
I0930 09:17:58.918407 140009111922496 base_runner.py:57] ============================================================
I0930 09:17:58.924173 140009111922496 base_runner.py:59] allow_implicit_capture : NoneType
I0930 09:17:58.924296 140009111922496 base_runner.py:59] cls : type/lingvo.core.base_model/SingleTaskModel
I0930 09:17:58.924360 140009111922496 base_runner.py:59] cluster.add_summary : NoneType
I0930 09:17:58.924417 140009111922496 base_runner.py:59] cluster.cls : type/lingvo.core.cluster/_Cluster
I0930 09:17:58.924470 140009111922496 base_runner.py:59] cluster.controller.cpus_per_replica : 1
I0930 09:17:58.924522 140009111922496 base_runner.py:59] cluster.controller.devices_per_split : 1
I0930 09:17:58.924585 140009111922496 base_runner.py:59] cluster.controller.gpus_per_replica : 1
I0930 09:17:58.924638 140009111922496 base_runner.py:59] cluster.controller.name : '/job:local'
I0930 09:17:58.924689 140009111922496 base_runner.py:59] cluster.controller.num_tpu_hosts : 0
I0930 09:17:58.924746 140009111922496 base_runner.py:59] cluster.controller.replicas : 1
I0930 09:17:58.924796 140009111922496 base_runner.py:59] cluster.controller.targets : ''
I0930 09:17:58.924846 140009111922496 base_runner.py:59] cluster.controller.tpus_per_replica : 0
I0930 09:17:58.924896 140009111922496 base_runner.py:59] cluster.decoder.cpus_per_replica : 1
I0930 09:17:58.924946 140009111922496 base_runner.py:59] cluster.decoder.devices_per_split : 1
I0930 09:17:58.924996 140009111922496 base_runner.py:59] cluster.decoder.gpus_per_replica : 1
I0930 09:17:58.925045 140009111922496 base_runner.py:59] cluster.decoder.name : '/job:local'
I0930 09:17:58.925094 140009111922496 base_runner.py:59] cluster.decoder.num_tpu_hosts : 0
I0930 09:17:58.925144 140009111922496 base_runner.py:59] cluster.decoder.replicas : 1
I0930 09:17:58.925193 140009111922496 base_runner.py:59] cluster.decoder.targets : ''
I0930 09:17:58.925243 140009111922496 base_runner.py:59] cluster.decoder.tpus_per_replica : 0
I0930 09:17:58.925292 140009111922496 base_runner.py:59] cluster.evaler.cpus_per_replica : 1
I0930 09:17:58.925342 140009111922496 base_runner.py:59] cluster.evaler.devices_per_split : 1
I0930 09:17:58.925391 140009111922496 base_runner.py:59] cluster.evaler.gpus_per_replica : 1
I0930 09:17:58.925441 140009111922496 base_runner.py:59] cluster.evaler.name : '/job:local'
I0930 09:17:58.925490 140009111922496 base_runner.py:59] cluster.evaler.num_tpu_hosts : 0
I0930 09:17:58.925538 140009111922496 base_runner.py:59] cluster.evaler.replicas : 1
I0930 09:17:58.925587 140009111922496 base_runner.py:59] cluster.evaler.targets : ''
I0930 09:17:58.925637 140009111922496 base_runner.py:59] cluster.evaler.tpus_per_replica : 0
I0930 09:17:58.925686 140009111922496 base_runner.py:59] cluster.input.cpus_per_replica : 1
I0930 09:17:58.925735 140009111922496 base_runner.py:59] cluster.input.devices_per_split : 1
I0930 09:17:58.925784 140009111922496 base_runner.py:59] cluster.input.gpus_per_replica : 0
I0930 09:17:58.925832 140009111922496 base_runner.py:59] cluster.input.name : '/job:local'
I0930 09:17:58.925880 140009111922496 base_runner.py:59] cluster.input.num_tpu_hosts : 0
I0930 09:17:58.925929 140009111922496 base_runner.py:59] cluster.input.replicas : 0
I0930 09:17:58.925978 140009111922496 base_runner.py:59] cluster.input.targets : ''
I0930 09:17:58.926027 140009111922496 base_runner.py:59] cluster.input.tpus_per_replica : 0
I0930 09:17:58.926075 140009111922496 base_runner.py:59] cluster.job : 'trainer_client'
I0930 09:17:58.926124 140009111922496 base_runner.py:59] cluster.logdir : ''
I0930 09:17:58.926173 140009111922496 base_runner.py:59] cluster.mode : 'sync'
I0930 09:17:58.926222 140009111922496 base_runner.py:59] cluster.ps.cpus_per_replica : 1
I0930 09:17:58.926289 140009111922496 base_runner.py:59] cluster.ps.devices_per_split : 1
I0930 09:17:58.926342 140009111922496 base_runner.py:59] cluster.ps.gpus_per_replica : 0
I0930 09:17:58.926392 140009111922496 base_runner.py:59] cluster.ps.name : '/job:local'
I0930 09:17:58.926441 140009111922496 base_runner.py:59] cluster.ps.num_tpu_hosts : 0
I0930 09:17:58.926490 140009111922496 base_runner.py:59] cluster.ps.replicas : 1
I0930 09:17:58.926538 140009111922496 base_runner.py:59] cluster.ps.targets : ''
I0930 09:17:58.926587 140009111922496 base_runner.py:59] cluster.ps.tpus_per_replica : 0
I0930 09:17:58.926635 140009111922496 base_runner.py:59] cluster.task : 0
I0930 09:17:58.926684 140009111922496 base_runner.py:59] cluster.worker.cpus_per_replica : 1
I0930 09:17:58.926733 140009111922496 base_runner.py:59] cluster.worker.devices_per_split : 1
I0930 09:17:58.926782 140009111922496 base_runner.py:59] cluster.worker.gpus_per_replica : 1
I0930 09:17:58.926837 140009111922496 base_runner.py:59] cluster.worker.name : '/job:local'
I0930 09:17:58.926887 140009111922496 base_runner.py:59] cluster.worker.num_tpu_hosts : 0
I0930 09:17:58.926937 140009111922496 base_runner.py:59] cluster.worker.replicas : 1
I0930 09:17:58.926985 140009111922496 base_runner.py:59] cluster.worker.targets : ''
I0930 09:17:58.927034 140009111922496 base_runner.py:59] cluster.worker.tpus_per_replica : 0
I0930 09:17:58.927082 140009111922496 base_runner.py:59] dtype : float32
I0930 09:17:58.927131 140009111922496 base_runner.py:59] fprop_dtype : NoneType
I0930 09:17:58.927179 140009111922496 base_runner.py:59] inference_driver_name : NoneType
I0930 09:17:58.927228 140009111922496 base_runner.py:59] input.allow_implicit_capture : NoneType
I0930 09:17:58.927277 140009111922496 base_runner.py:59] input.bucket_adjust_every_n : 0
I0930 09:17:58.927326 140009111922496 base_runner.py:59] input.bucket_batch_limit : [32]
I0930 09:17:58.927375 140009111922496 base_runner.py:59] input.bucket_upper_bound : [1024]
I0930 09:17:58.927424 140009111922496 base_runner.py:59] input.cls : type/lingvo.tasks.lm.input_generator/LmInput
I0930 09:17:58.927473 140009111922496 base_runner.py:59] input.dtype : float32
I0930 09:17:58.927523 140009111922496 base_runner.py:59] input.file_buffer_size : 10000000
I0930 09:17:58.927571 140009111922496 base_runner.py:59] input.file_datasource : NoneType
I0930 09:17:58.927620 140009111922496 base_runner.py:59] input.file_parallelism : 10
I0930 09:17:58.927669 140009111922496 base_runner.py:59] input.file_pattern : 'text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*'
I0930 09:17:58.927720 140009111922496 base_runner.py:59] input.file_random_seed : 301
I0930 09:17:58.927769 140009111922496 base_runner.py:59] input.fixed_input_shape : True
I0930 09:17:58.927818 140009111922496 base_runner.py:59] input.flush_every_n : 0
I0930 09:17:58.927866 140009111922496 base_runner.py:59] input.fprop_dtype : NoneType
I0930 09:17:58.927915 140009111922496 base_runner.py:59] input.inference_driver_name : NoneType
I0930 09:17:58.927964 140009111922496 base_runner.py:59] input.is_eval : NoneType
I0930 09:17:58.928013 140009111922496 base_runner.py:59] input.is_inference : NoneType
I0930 09:17:58.928062 140009111922496 base_runner.py:59] input.name : '1bwds_train_set'
I0930 09:17:58.928111 140009111922496 base_runner.py:59] input.num_batcher_threads : 16
I0930 09:17:58.928159 140009111922496 base_runner.py:59] input.num_samples : 0
I0930 09:17:58.928208 140009111922496 base_runner.py:59] input.pad_to_max_seq_length : False
I0930 09:17:58.928257 140009111922496 base_runner.py:59] input.params_init.method : 'xavier'
I0930 09:17:58.928307 140009111922496 base_runner.py:59] input.params_init.scale : 1.000001
I0930 09:17:58.928355 140009111922496 base_runner.py:59] input.params_init.seed : NoneType
I0930 09:17:58.928404 140009111922496 base_runner.py:59] input.random_seed : NoneType
I0930 09:17:58.928453 140009111922496 base_runner.py:59] input.remote.max_inflights_per_target : 32
I0930 09:17:58.928503 140009111922496 base_runner.py:59] input.remote.shardable_batch : False
I0930 09:17:58.928552 140009111922496 base_runner.py:59] input.require_sequential_order : False
I0930 09:17:58.928600 140009111922496 base_runner.py:59] input.skip_lp_regularization : NoneType
I0930 09:17:58.928649 140009111922496 base_runner.py:59] input.source_max_length : NoneType
I0930 09:17:58.928698 140009111922496 base_runner.py:59] input.target_max_length : 1024
I0930 09:17:58.928747 140009111922496 base_runner.py:59] input.tokenizer.allow_implicit_capture : NoneType
I0930 09:17:58.928796 140009111922496 base_runner.py:59] input.tokenizer.append_eos : True
I0930 09:17:58.928844 140009111922496 base_runner.py:59] input.tokenizer.cls : type/lingvo.core.tokenizers/AsciiTokenizer
I0930 09:17:58.928893 140009111922496 base_runner.py:59] input.tokenizer.dtype : float32
I0930 09:17:58.928942 140009111922496 base_runner.py:59] input.tokenizer.fprop_dtype : NoneType
I0930 09:17:58.928995 140009111922496 base_runner.py:59] input.tokenizer.inference_driver_name : NoneType
I0930 09:17:58.929050 140009111922496 base_runner.py:59] input.tokenizer.is_eval : NoneType
I0930 09:17:58.929100 140009111922496 base_runner.py:59] input.tokenizer.is_inference : NoneType
I0930 09:17:58.929150 140009111922496 base_runner.py:59] input.tokenizer.name : 'tokenizer'
I0930 09:17:58.929199 140009111922496 base_runner.py:59] input.tokenizer.pad_to_max_length : True
I0930 09:17:58.929248 140009111922496 base_runner.py:59] input.tokenizer.params_init.method : 'xavier'
I0930 09:17:58.929297 140009111922496 base_runner.py:59] input.tokenizer.params_init.scale : 1.000001
I0930 09:17:58.929346 140009111922496 base_runner.py:59] input.tokenizer.params_init.seed : NoneType
I0930 09:17:58.929395 140009111922496 base_runner.py:59] input.tokenizer.random_seed : NoneType
I0930 09:17:58.929443 140009111922496 base_runner.py:59] input.tokenizer.skip_lp_regularization : NoneType
I0930 09:17:58.929492 140009111922496 base_runner.py:59] input.tokenizer.target_eos_id : 2
I0930 09:17:58.929541 140009111922496 base_runner.py:59] input.tokenizer.target_sos_id : 1
I0930 09:17:58.929589 140009111922496 base_runner.py:59] input.tokenizer.target_unk_id : 0
I0930 09:17:58.929637 140009111922496 base_runner.py:59] input.tokenizer.vn.global_vn : False
I0930 09:17:58.929686 140009111922496 base_runner.py:59] input.tokenizer.vn.per_step_vn : False
I0930 09:17:58.929734 140009111922496 base_runner.py:59] input.tokenizer.vn.scale : NoneType
I0930 09:17:58.929783 140009111922496 base_runner.py:59] input.tokenizer.vn.seed : NoneType
I0930 09:17:58.929832 140009111922496 base_runner.py:59] input.tokenizer.vocab_size : 32000
I0930 09:17:58.929881 140009111922496 base_runner.py:59] input.tokenizer_dict : {}
I0930 09:17:58.929930 140009111922496 base_runner.py:59] input.tpu_infeed_parallelism : 1
I0930 09:17:58.929978 140009111922496 base_runner.py:59] input.use_chaining : False
I0930 09:17:58.930028 140009111922496 base_runner.py:59] input.use_per_host_infeed : False
I0930 09:17:58.930077 140009111922496 base_runner.py:59] input.use_within_batch_mixing : False
I0930 09:17:58.930127 140009111922496 base_runner.py:59] input.vn.global_vn : False
I0930 09:17:58.930175 140009111922496 base_runner.py:59] input.vn.per_step_vn : False
I0930 09:17:58.930224 140009111922496 base_runner.py:59] input.vn.scale : NoneType
I0930 09:17:58.930290 140009111922496 base_runner.py:59] input.vn.seed : NoneType
I0930 09:17:58.930343 140009111922496 base_runner.py:59] is_eval : NoneType
I0930 09:17:58.930392 140009111922496 base_runner.py:59] is_inference : NoneType
I0930 09:17:58.930442 140009111922496 base_runner.py:59] model : 'lm.one_billion_wds.OneBWdsGPipeTransformerWPM@/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/tasks/lm/params/one_billion_wds.py:187'
I0930 09:17:58.930492 140009111922496 base_runner.py:59] name : ''
I0930 09:17:58.930541 140009111922496 base_runner.py:59] params_init.method : 'xavier'
I0930 09:17:58.930590 140009111922496 base_runner.py:59] params_init.scale : 1.000001
I0930 09:17:58.930638 140009111922496 base_runner.py:59] params_init.seed : NoneType
I0930 09:17:58.930687 140009111922496 base_runner.py:59] random_seed : NoneType
I0930 09:17:58.930736 140009111922496 base_runner.py:59] skip_lp_regularization : NoneType
I0930 09:17:58.930784 140009111922496 base_runner.py:59] task.allow_implicit_capture : NoneType
I0930 09:17:58.930832 140009111922496 base_runner.py:59] task.cls : type/lingvo.tasks.lm.model/FixedShapeInputLanguageModel
I0930 09:17:58.930881 140009111922496 base_runner.py:59] task.decoder : NoneType
I0930 09:17:58.930929 140009111922496 base_runner.py:59] task.dtype : float32
I0930 09:17:58.930978 140009111922496 base_runner.py:59] task.encoder : NoneType
I0930 09:17:58.931026 140009111922496 base_runner.py:59] task.eval.decoder_samples_per_summary : 0
I0930 09:17:58.931074 140009111922496 base_runner.py:59] task.eval.load_checkpoint_from : NoneType
I0930 09:17:58.931123 140009111922496 base_runner.py:59] task.eval.samples_per_summary : 0
I0930 09:17:58.931177 140009111922496 base_runner.py:59] task.eval.start_decoder_after : 0
I0930 09:17:58.931226 140009111922496 base_runner.py:59] task.eval.start_eval_after : 0
I0930 09:17:58.931274 140009111922496 base_runner.py:59] task.fprop_dtype : NoneType
I0930 09:17:58.931324 140009111922496 base_runner.py:59] task.inference_driver_name : NoneType
I0930 09:17:58.931372 140009111922496 base_runner.py:59] task.input : NoneType
I0930 09:17:58.931421 140009111922496 base_runner.py:59] task.is_eval : NoneType
I0930 09:17:58.931469 140009111922496 base_runner.py:59] task.is_inference : NoneType
I0930 09:17:58.931518 140009111922496 base_runner.py:59] task.lm.allow_implicit_capture : NoneType
I0930 09:17:58.931566 140009111922496 base_runner.py:59] task.lm.cls : type/lingvo.tasks.lm.layers/GPipeTransformerLm
I0930 09:17:58.931614 140009111922496 base_runner.py:59] task.lm.dtype : float32
I0930 09:17:58.931663 140009111922496 base_runner.py:59] task.lm.fprop_dtype : NoneType
I0930 09:17:58.931713 140009111922496 base_runner.py:59] task.lm.inference_driver_name : NoneType
I0930 09:17:58.931762 140009111922496 base_runner.py:59] task.lm.is_eval : NoneType
I0930 09:17:58.931810 140009111922496 base_runner.py:59] task.lm.is_inference : NoneType
I0930 09:17:58.931859 140009111922496 base_runner.py:59] task.lm.name : 'transformerlm'
I0930 09:17:58.931907 140009111922496 base_runner.py:59] task.lm.params_init.method : 'xavier'
I0930 09:17:58.931956 140009111922496 base_runner.py:59] task.lm.params_init.scale : 1.000001
I0930 09:17:58.932005 140009111922496 base_runner.py:59] task.lm.params_init.seed : NoneType
I0930 09:17:58.932054 140009111922496 base_runner.py:59] task.lm.random_seed : NoneType
I0930 09:17:58.932103 140009111922496 base_runner.py:59] task.lm.skip_lp_regularization : NoneType
I0930 09:17:58.932151 140009111922496 base_runner.py:59] task.lm.stack.allow_implicit_capture : NoneType
I0930 09:17:58.932200 140009111922496 base_runner.py:59] task.lm.stack.batch_dim : 1
I0930 09:17:58.932248 140009111922496 base_runner.py:59] task.lm.stack.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerStack
I0930 09:17:58.932297 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.932347 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 09:17:58.932396 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.dtype : float32
I0930 09:17:58.932444 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.final_enc_layer : False
I0930 09:17:58.932492 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.fprop_dtype : NoneType
I0930 09:17:58.932541 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.has_aux_atten : True
I0930 09:17:58.932589 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.inference_driver_name : NoneType
I0930 09:17:58.932638 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.is_decoder : False
I0930 09:17:58.932687 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.is_eval : NoneType
I0930 09:17:58.932735 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.is_inference : NoneType
I0930 09:17:58.932784 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.is_transparent : False
I0930 09:17:58.932833 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.932882 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:17:58.932930 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.dtype : float32
I0930 09:17:58.932979 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.epsilon : 1e-06
I0930 09:17:58.933027 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:17:58.933076 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:17:58.933129 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.input_dim : 0
I0930 09:17:58.933179 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_eval : NoneType
I0930 09:17:58.933228 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.is_inference : NoneType
I0930 09:17:58.933277 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.name : ''
I0930 09:17:58.933325 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:17:58.933374 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:17:58.933423 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:17:58.933471 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.random_seed : NoneType
I0930 09:17:58.933520 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.933568 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.global_vn : False
I0930 09:17:58.933617 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:17:58.933666 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.scale : NoneType
I0930 09:17:58.933715 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.ln_tpl.vn.seed : NoneType
I0930 09:17:58.933763 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.mask_self_atten : True
I0930 09:17:58.933812 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.name : ''
I0930 09:17:58.933860 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.normalize_output : False
I0930 09:17:58.933909 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.output_dim : 0
I0930 09:17:58.933957 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.packed_input : False
I0930 09:17:58.934005 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.method : 'xavier'
I0930 09:17:58.934054 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.scale : 1.000001
I0930 09:17:58.934102 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.params_init.seed : NoneType
I0930 09:17:58.934150 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.random_seed : NoneType
I0930 09:17:58.934198 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.934247 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.source_dim : 0
I0930 09:17:58.934319 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 09:17:58.934370 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.934419 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 09:17:58.934468 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 09:17:58.934517 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.934566 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 09:17:58.934615 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 09:17:58.934663 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 09:17:58.934712 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 09:17:58.934759 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 09:17:58.934808 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 09:17:58.934860 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 09:17:58.934910 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : False
I0930 09:17:58.934958 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 09:17:58.935007 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 09:17:58.935056 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 09:17:58.935104 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 09:17:58.935153 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 09:17:58.935202 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 09:17:58.935250 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 09:17:58.935299 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 09:17:58.935348 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 09:17:58.935397 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 09:17:58.935446 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 09:17:58.935494 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 09:17:58.935543 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 09:17:58.935591 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 09:17:58.935640 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 09:17:58.935688 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 09:17:58.935736 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 09:17:58.935785 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 09:17:58.935833 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 09:17:58.935882 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 09:17:58.935930 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 09:17:58.935978 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 09:17:58.936027 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 09:17:58.936075 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 09:17:58.936124 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 09:17:58.936172 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 09:17:58.936226 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 09:17:58.936275 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 09:17:58.936324 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 09:17:58.936372 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 09:17:58.936421 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 09:17:58.936469 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 09:17:58.936518 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 09:17:58.936566 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 09:17:58.936615 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 09:17:58.936664 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 09:17:58.936712 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 09:17:58.936761 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 09:17:58.936810 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 09:17:58.936858 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 09:17:58.936907 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 09:17:58.936956 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 09:17:58.937005 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 09:17:58.937054 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 09:17:58.937103 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 09:17:58.937153 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.937202 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 09:17:58.937251 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 09:17:58.937300 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 09:17:58.937349 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 09:17:58.937397 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 09:17:58.937446 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 09:17:58.937494 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 09:17:58.937543 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.context_dim : 0
I0930 09:17:58.937592 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.dtype : float32
I0930 09:17:58.937640 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 09:17:58.937689 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 09:17:58.937742 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 09:17:58.937792 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 09:17:58.937841 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.is_masked : False
I0930 09:17:58.937890 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.937939 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:17:58.937988 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 09:17:58.938036 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 09:17:58.938085 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:17:58.938134 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:17:58.938183 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 09:17:58.938231 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 09:17:58.938297 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 09:17:58.938348 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 09:17:58.938397 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:17:58.938446 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:17:58.938495 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:17:58.938544 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 09:17:58.938593 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.938642 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 09:17:58.938691 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:17:58.938740 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 09:17:58.938788 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 09:17:58.938836 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 09:17:58.938885 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.name : ''
I0930 09:17:58.938933 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.num_attention_heads : 8
I0930 09:17:58.938982 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.packed_input : False
I0930 09:17:58.939031 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 09:17:58.939082 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 09:17:58.939133 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 09:17:58.939182 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 09:17:58.939232 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 09:17:58.939281 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.939329 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:17:58.939383 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 09:17:58.939432 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 09:17:58.939482 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 09:17:58.939531 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 09:17:58.939580 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 09:17:58.939629 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 09:17:58.939677 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 09:17:58.939726 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 09:17:58.939774 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 09:17:58.939823 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:17:58.939871 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 09:17:58.939921 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 09:17:58.939970 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 09:17:58.940018 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 09:17:58.940067 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.940116 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 09:17:58.940165 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 09:17:58.940214 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 09:17:58.940263 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 09:17:58.940311 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.940361 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.source_dim : 0
I0930 09:17:58.940409 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 09:17:58.940458 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 09:17:58.940506 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 09:17:58.940556 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 09:17:58.940604 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_aux_atten_tpl : NoneType
I0930 09:17:58.940652 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 09:17:58.940701 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.940750 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 09:17:58.940799 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 09:17:58.940849 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 09:17:58.940902 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.940952 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 09:17:58.941001 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 09:17:58.941051 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 09:17:58.941100 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 09:17:58.941149 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:17:58.941198 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 09:17:58.941247 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 09:17:58.941296 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 09:17:58.941345 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 09:17:58.941395 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 09:17:58.941443 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 09:17:58.941492 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 09:17:58.941540 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 09:17:58.941589 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 09:17:58.941638 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 09:17:58.941687 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 09:17:58.941735 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 09:17:58.941784 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 09:17:58.941832 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 09:17:58.941880 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 09:17:58.941928 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 09:17:58.941977 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 09:17:58.942026 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 09:17:58.942074 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 09:17:58.942123 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 09:17:58.942172 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 09:17:58.942221 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 09:17:58.942299 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 09:17:58.942355 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 09:17:58.942405 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 09:17:58.942454 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 09:17:58.942502 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 09:17:58.942551 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 09:17:58.942600 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 09:17:58.942649 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 09:17:58.942697 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 09:17:58.942746 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 09:17:58.942795 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 09:17:58.942844 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 09:17:58.942893 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 09:17:58.942941 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 09:17:58.942990 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 09:17:58.943038 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 09:17:58.943087 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 09:17:58.943135 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 09:17:58.943184 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 09:17:58.943233 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 09:17:58.943281 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 09:17:58.943330 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 09:17:58.943378 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 09:17:58.943427 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 09:17:58.943475 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 09:17:58.943524 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 09:17:58.943573 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 09:17:58.943621 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 09:17:58.943669 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 09:17:58.943722 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 09:17:58.943773 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 09:17:58.943822 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 09:17:58.943870 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 09:17:58.943918 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 09:17:58.943966 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 09:17:58.944015 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 09:17:58.944064 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 09:17:58.944112 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.944161 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 09:17:58.944210 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 09:17:58.944259 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 09:17:58.944308 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 09:17:58.944356 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 09:17:58.944405 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 09:17:58.944453 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.hidden_dim : 2048
I0930 09:17:58.944503 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 09:17:58.944551 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 09:17:58.944600 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 09:17:58.944649 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 09:17:58.944698 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.944746 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:17:58.944795 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 09:17:58.944844 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 09:17:58.944892 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:17:58.944941 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:17:58.944989 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 09:17:58.945037 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 09:17:58.945086 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 09:17:58.945134 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 09:17:58.945183 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:17:58.945231 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:17:58.945284 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:17:58.945333 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 09:17:58.945381 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.945430 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 09:17:58.945478 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:17:58.945527 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 09:17:58.945575 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 09:17:58.945624 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.name : ''
I0930 09:17:58.945672 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 09:17:58.945720 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 09:17:58.945769 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 09:17:58.945816 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 09:17:58.945865 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 09:17:58.945913 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 09:17:58.945961 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 09:17:58.946010 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 09:17:58.946058 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.946106 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 09:17:58.946155 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 09:17:58.946203 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 09:17:58.946252 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 09:17:58.946324 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 09:17:58.946374 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 09:17:58.946423 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 09:17:58.946472 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 09:17:58.946521 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 09:17:58.946570 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 09:17:58.946619 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 09:17:58.946668 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 09:17:58.946717 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 09:17:58.946765 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 09:17:58.946819 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 09:17:58.946870 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 09:17:58.946919 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 09:17:58.946968 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 09:17:58.947018 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.947067 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 09:17:58.947117 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 09:17:58.947166 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 09:17:58.947215 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 09:17:58.947264 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 09:17:58.947314 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 09:17:58.947363 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.947412 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:17:58.947461 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 09:17:58.947510 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 09:17:58.947559 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 09:17:58.947607 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 09:17:58.947656 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 09:17:58.947704 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 09:17:58.947753 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 09:17:58.947802 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 09:17:58.947851 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 09:17:58.947900 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:17:58.947949 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 09:17:58.947998 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 09:17:58.948047 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 09:17:58.948096 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 09:17:58.948145 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.948193 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 09:17:58.948249 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 09:17:58.948299 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 09:17:58.948348 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 09:17:58.948397 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.948447 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 09:17:58.948496 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 09:17:58.948545 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 09:17:58.948595 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 09:17:58.948643 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.transparent_merger_tpl : NoneType
I0930 09:17:58.948693 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.vn.global_vn : False
I0930 09:17:58.948741 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.vn.per_step_vn : False
I0930 09:17:58.948791 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.vn.scale : NoneType
I0930 09:17:58.948840 140009111922496 base_runner.py:59] task.lm.stack.decoder_tpl.vn.seed : NoneType
I0930 09:17:58.948889 140009111922496 base_runner.py:59] task.lm.stack.dtype : float32
I0930 09:17:58.948938 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.add_tgt_embedding_layer : False
I0930 09:17:58.948986 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.949035 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.batch_dim : 1
I0930 09:17:58.949084 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerEmbeddingLayer
I0930 09:17:58.949137 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dec_task_emb : NoneType
I0930 09:17:58.949187 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.949236 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:17:58.949285 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dropout_at_eval : False
I0930 09:17:58.949334 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.dtype : float32
I0930 09:17:58.949383 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 09:17:58.949432 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 09:17:58.949481 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_eval : NoneType
I0930 09:17:58.949529 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.is_inference : NoneType
I0930 09:17:58.949578 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.keep_prob : 1.0
I0930 09:17:58.949627 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.name : ''
I0930 09:17:58.949676 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape : NoneType
I0930 09:17:58.949725 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:17:58.949774 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 09:17:58.949823 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 09:17:58.949872 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.params_init.seed : NoneType
I0930 09:17:58.949922 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.random_seed : NoneType
I0930 09:17:58.949981 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.950031 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.global_vn : False
I0930 09:17:58.950080 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.per_step_vn : False
I0930 09:17:58.950128 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.scale : NoneType
I0930 09:17:58.950177 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dropout_tpl.vn.seed : NoneType
I0930 09:17:58.950225 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.dtype : float32
I0930 09:17:58.950291 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.enc_task_emb : NoneType
I0930 09:17:58.950343 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.fprop_dtype : NoneType
I0930 09:17:58.950392 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.inference_driver_name : NoneType
I0930 09:17:58.950441 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.input_dropout_prob : 0.0
I0930 09:17:58.950490 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.is_eval : NoneType
I0930 09:17:58.950539 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.is_inference : NoneType
I0930 09:17:58.950588 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.is_transparent : False
I0930 09:17:58.950638 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.max_seq_len : 300
I0930 09:17:58.950686 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.name : ''
I0930 09:17:58.950736 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.packed_input : False
I0930 09:17:58.950784 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.params_init.method : 'xavier'
I0930 09:17:58.950833 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.params_init.scale : 1.000001
I0930 09:17:58.950882 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.params_init.seed : NoneType
I0930 09:17:58.950931 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.allow_implicit_capture : NoneType
I0930 09:17:58.950980 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.cls : type/lingvo.core.layers/PositionalEmbeddingLayer
I0930 09:17:58.951029 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.dtype : float32
I0930 09:17:58.951077 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.embedding_dim : 2048
I0930 09:17:58.951125 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.fprop_dtype : NoneType
I0930 09:17:58.951175 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.inference_driver_name : NoneType
I0930 09:17:58.951224 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_eval : NoneType
I0930 09:17:58.951272 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.is_inference : NoneType
I0930 09:17:58.951321 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.max_timescale : 10000
I0930 09:17:58.951370 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.min_timescale : 1
I0930 09:17:58.951419 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.name : ''
I0930 09:17:58.951468 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.method : 'xavier'
I0930 09:17:58.951516 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.scale : 1.000001
I0930 09:17:58.951565 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.params_init.seed : NoneType
I0930 09:17:58.951614 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.random_seed : NoneType
I0930 09:17:58.951662 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.skip_lp_regularization : NoneType
I0930 09:17:58.951711 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling : False
I0930 09:17:58.951760 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.trainable_scaling_init : 1.0
I0930 09:17:58.951813 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.global_vn : False
I0930 09:17:58.951863 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.per_step_vn : False
I0930 09:17:58.951913 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.scale : NoneType
I0930 09:17:58.951961 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.position_emb.vn.seed : NoneType
I0930 09:17:58.952011 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.random_seed : NoneType
I0930 09:17:58.952060 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.952108 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.allow_implicit_capture : NoneType
I0930 09:17:58.952157 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.apply_pruning : False
I0930 09:17:58.952205 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.cls : type/lingvo.core.layers/SimpleEmbeddingLayer
I0930 09:17:58.952254 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.dtype : float32
I0930 09:17:58.952302 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.embedding_dim : 2048
I0930 09:17:58.952351 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_dtype : NoneType
I0930 09:17:58.952400 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.fprop_mode : NoneType
I0930 09:17:58.952449 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.inference_driver_name : NoneType
I0930 09:17:58.952497 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_eval : NoneType
I0930 09:17:58.952545 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.is_inference : NoneType
I0930 09:17:58.952594 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.name : ''
I0930 09:17:58.952642 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.method : 'gaussian'
I0930 09:17:58.952690 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.scale : 0.022097086912079608
I0930 09:17:58.952738 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.params_init.seed : NoneType
I0930 09:17:58.952787 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.qdomain.default : NoneType
I0930 09:17:58.952835 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.random_seed : NoneType
I0930 09:17:58.952883 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.skip_lp_regularization : NoneType
I0930 09:17:58.952932 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_3d_weight_tensor : False
I0930 09:17:58.952980 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.use_matmul : False
I0930 09:17:58.953029 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.global_vn : False
I0930 09:17:58.953077 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.per_step_vn : False
I0930 09:17:58.953126 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.scale : NoneType
I0930 09:17:58.953175 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vn.seed : NoneType
I0930 09:17:58.953223 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.token_emb.vocab_size : 32000
I0930 09:17:58.953272 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.vn.global_vn : False
I0930 09:17:58.953320 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.vn.per_step_vn : False
I0930 09:17:58.953368 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.vn.scale : NoneType
I0930 09:17:58.953416 140009111922496 base_runner.py:59] task.lm.stack.emb_tpl.vn.seed : NoneType
I0930 09:17:58.953464 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.953512 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerLayer
I0930 09:17:58.953564 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.dtype : float32
I0930 09:17:58.953613 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.final_enc_layer : False
I0930 09:17:58.953661 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.fprop_dtype : NoneType
I0930 09:17:58.953710 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.has_aux_atten : False
I0930 09:17:58.953758 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.inference_driver_name : NoneType
I0930 09:17:58.953806 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.is_decoder : False
I0930 09:17:58.953854 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.is_eval : NoneType
I0930 09:17:58.953902 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.is_inference : NoneType
I0930 09:17:58.953950 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.is_transparent : False
I0930 09:17:58.953999 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.954047 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:17:58.954096 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.dtype : float32
I0930 09:17:58.954146 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.epsilon : 1e-06
I0930 09:17:58.954195 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:17:58.954244 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:17:58.954310 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.input_dim : 0
I0930 09:17:58.954361 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_eval : NoneType
I0930 09:17:58.954410 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.is_inference : NoneType
I0930 09:17:58.954458 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.name : ''
I0930 09:17:58.954506 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:17:58.954554 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:17:58.954603 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:17:58.954651 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.random_seed : NoneType
I0930 09:17:58.954699 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.954751 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.global_vn : False
I0930 09:17:58.954801 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:17:58.954851 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.scale : NoneType
I0930 09:17:58.954899 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.ln_tpl.vn.seed : NoneType
I0930 09:17:58.954949 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.mask_self_atten : True
I0930 09:17:58.954998 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.name : ''
I0930 09:17:58.955047 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.normalize_output : False
I0930 09:17:58.955095 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.output_dim : 0
I0930 09:17:58.955144 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.packed_input : False
I0930 09:17:58.955192 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.method : 'xavier'
I0930 09:17:58.955241 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.scale : 1.000001
I0930 09:17:58.955290 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.params_init.seed : NoneType
I0930 09:17:58.955338 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.random_seed : NoneType
I0930 09:17:58.955391 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.955441 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.source_dim : 2048
I0930 09:17:58.955491 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.add_unnormalized_input : False
I0930 09:17:58.955540 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.955589 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_dropout_prob : 0.0
I0930 09:17:58.955638 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_hidden_dim : 0
I0930 09:17:58.955687 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.955736 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_deterministic : False
I0930 09:17:58.955785 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.atten_dropout_prob : 0.0
I0930 09:17:58.955833 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.cls : type/lingvo.core.attention/MultiHeadedAttention
I0930 09:17:58.955882 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.context_dim : 0
I0930 09:17:58.955931 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.ctx_post_proj_dim : 0
I0930 09:17:58.955979 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.dtype : float32
I0930 09:17:58.956028 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_post_proj : True
I0930 09:17:58.956078 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_ctx_pre_proj : True
I0930 09:17:58.956127 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_query_proj : True
I0930 09:17:58.956176 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.enable_source_proj : True
I0930 09:17:58.956223 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.fprop_dtype : NoneType
I0930 09:17:58.956272 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.hidden_dim : 0
I0930 09:17:58.956321 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inference_driver_name : NoneType
I0930 09:17:58.956369 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.allow_implicit_capture : NoneType
I0930 09:17:58.956418 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_deterministic : False
I0930 09:17:58.956466 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.atten_dropout_prob : 0.0
I0930 09:17:58.956515 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.cls : type/lingvo.core.attention/DotProductAttention
I0930 09:17:58.956564 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.dtype : float32
I0930 09:17:58.956612 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.fprop_dtype : NoneType
I0930 09:17:58.956661 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.hidden_dim : 0
I0930 09:17:58.956709 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.inference_driver_name : NoneType
I0930 09:17:58.956758 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_eval : NoneType
I0930 09:17:58.956806 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.is_inference : NoneType
I0930 09:17:58.956859 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.name : ''
I0930 09:17:58.956909 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.packed_input : False
I0930 09:17:58.956958 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.method : 'xavier'
I0930 09:17:58.957007 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.scale : 1.000001
I0930 09:17:58.957055 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.params_init.seed : NoneType
I0930 09:17:58.957104 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.default : NoneType
I0930 09:17:58.957153 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.fullyconnected : NoneType
I0930 09:17:58.957202 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.qdomain.softmax : NoneType
I0930 09:17:58.957251 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.query_dim : 0
I0930 09:17:58.957300 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.random_seed : NoneType
I0930 09:17:58.957349 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.skip_lp_regularization : NoneType
I0930 09:17:58.957398 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.source_dim : 0
I0930 09:17:58.957447 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.global_vn : False
I0930 09:17:58.957496 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.per_step_vn : False
I0930 09:17:58.957543 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.scale : NoneType
I0930 09:17:58.957592 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.inner_atten_params.vn.seed : NoneType
I0930 09:17:58.957641 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_eval : NoneType
I0930 09:17:58.957689 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.is_inference : NoneType
I0930 09:17:58.957737 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.name : ''
I0930 09:17:58.957785 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.num_attention_heads : 2
I0930 09:17:58.957834 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.packed_input : False
I0930 09:17:58.957882 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.method : 'xavier'
I0930 09:17:58.957931 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.scale : 1.0
I0930 09:17:58.957980 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.params_init.seed : NoneType
I0930 09:17:58.958028 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.atten_context : NoneType
I0930 09:17:58.958077 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.default : NoneType
I0930 09:17:58.958126 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.fullyconnected : NoneType
I0930 09:17:58.958174 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.qdomain.softmax : NoneType
I0930 09:17:58.958223 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.query_dim : 0
I0930 09:17:58.958292 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.random_seed : NoneType
I0930 09:17:58.958348 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.958399 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.source_dim : 0
I0930 09:17:58.958448 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.use_source_vec_as_attention_value : False
I0930 09:17:58.958496 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.global_vn : False
I0930 09:17:58.958545 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.per_step_vn : False
I0930 09:17:58.958594 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.scale : NoneType
I0930 09:17:58.958643 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.atten_tpl.vn.seed : NoneType
I0930 09:17:58.958691 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.cls : type/lingvo.core.layers_with_attention/TransformerAttentionLayer
I0930 09:17:58.958740 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.context_dim : 0
I0930 09:17:58.958789 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.dtype : float32
I0930 09:17:58.958838 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.fprop_dtype : NoneType
I0930 09:17:58.958887 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.inference_driver_name : NoneType
I0930 09:17:58.958936 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_eval : NoneType
I0930 09:17:58.958984 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_inference : NoneType
I0930 09:17:58.959033 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.is_masked : True
I0930 09:17:58.959082 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.959130 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:17:58.959182 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.dtype : float32
I0930 09:17:58.959233 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.epsilon : 1e-06
I0930 09:17:58.959282 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:17:58.959331 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:17:58.959379 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.input_dim : 0
I0930 09:17:58.959428 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_eval : NoneType
I0930 09:17:58.959476 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.is_inference : NoneType
I0930 09:17:58.959524 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.name : ''
I0930 09:17:58.959573 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:17:58.959621 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:17:58.959668 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:17:58.959716 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.random_seed : NoneType
I0930 09:17:58.959764 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.959812 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.global_vn : False
I0930 09:17:58.959861 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:17:58.959913 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.scale : NoneType
I0930 09:17:58.959963 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.ln_tpl.vn.seed : NoneType
I0930 09:17:58.960012 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.mask_type : 'future'
I0930 09:17:58.960060 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.name : ''
I0930 09:17:58.960109 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.num_attention_heads : 16
I0930 09:17:58.960158 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.packed_input : False
I0930 09:17:58.960207 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.method : 'xavier'
I0930 09:17:58.960256 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.scale : 1.000001
I0930 09:17:58.960304 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.params_init.seed : NoneType
I0930 09:17:58.960352 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.random_seed : NoneType
I0930 09:17:58.960401 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_prob : 0.0
I0930 09:17:58.960449 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.960498 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:17:58.960546 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 09:17:58.960595 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.dtype : float32
I0930 09:17:58.960643 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 09:17:58.960691 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 09:17:58.960739 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 09:17:58.960788 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 09:17:58.960836 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 09:17:58.960885 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.name : ''
I0930 09:17:58.960933 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 09:17:58.960980 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:17:58.961030 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 09:17:58.961078 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 09:17:58.961127 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 09:17:58.961176 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 09:17:58.961224 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.961273 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 09:17:58.961322 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 09:17:58.961375 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 09:17:58.961424 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 09:17:58.961472 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.961520 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.source_dim : 0
I0930 09:17:58.961569 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.global_vn : False
I0930 09:17:58.961617 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.per_step_vn : False
I0930 09:17:58.961666 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.scale : NoneType
I0930 09:17:58.961714 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_atten_tpl.vn.seed : NoneType
I0930 09:17:58.961763 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_aux_atten_tpl : NoneType
I0930 09:17:58.961811 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.activation : 'RELU'
I0930 09:17:58.961860 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.961909 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.cls : type/lingvo.core.layers_with_attention/TransformerFeedForwardLayer
I0930 09:17:58.961957 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.dtype : float32
I0930 09:17:58.962006 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.activation : ['RELU', 'NONE']
I0930 09:17:58.962055 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.962103 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.batch_norm : False
I0930 09:17:58.962152 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.bn_fold_weights : NoneType
I0930 09:17:58.962200 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.cls : type/lingvo.core.layers/FeedForwardNet
I0930 09:17:58.962249 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.allow_implicit_capture : NoneType
I0930 09:17:58.962317 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:17:58.962368 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dropout_at_eval : False
I0930 09:17:58.962417 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.dtype : float32
I0930 09:17:58.962466 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.fprop_dtype : NoneType
I0930 09:17:58.962514 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.inference_driver_name : NoneType
I0930 09:17:58.962563 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_eval : NoneType
I0930 09:17:58.962611 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.is_inference : NoneType
I0930 09:17:58.962660 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.keep_prob : 1.0
I0930 09:17:58.962708 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.name : ''
I0930 09:17:58.962757 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape : NoneType
I0930 09:17:58.962805 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.noise_shape_broadcast_dims : NoneType
I0930 09:17:58.962854 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.method : 'xavier'
I0930 09:17:58.962907 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.scale : 1.000001
I0930 09:17:58.962956 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.params_init.seed : NoneType
I0930 09:17:58.963004 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.random_seed : NoneType
I0930 09:17:58.963052 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.skip_lp_regularization : NoneType
I0930 09:17:58.963100 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.global_vn : False
I0930 09:17:58.963148 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.per_step_vn : False
I0930 09:17:58.963196 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.scale : NoneType
I0930 09:17:58.963245 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dropout.vn.seed : NoneType
I0930 09:17:58.963293 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.dtype : float32
I0930 09:17:58.963341 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.fprop_dtype : NoneType
I0930 09:17:58.963390 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.inference_driver_name : NoneType
I0930 09:17:58.963439 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.input_dim : 0
I0930 09:17:58.963488 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_eval : NoneType
I0930 09:17:58.963536 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.is_inference : NoneType
I0930 09:17:58.963585 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.name : ''
I0930 09:17:58.963633 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.method : 'xavier'
I0930 09:17:58.963682 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.scale : 1.000001
I0930 09:17:58.963730 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.params_init.seed : NoneType
I0930 09:17:58.963779 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.activation : 'RELU'
I0930 09:17:58.963827 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.affine_last : False
I0930 09:17:58.963876 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.allow_implicit_capture : NoneType
I0930 09:17:58.963925 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.batch_norm : True
I0930 09:17:58.963973 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bias_init : 0.0
I0930 09:17:58.964021 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.bn_fold_weights : NoneType
I0930 09:17:58.964069 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.cls : type/lingvo.core.layers/ProjectionLayer
I0930 09:17:58.964118 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.dtype : float32
I0930 09:17:58.964166 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.fprop_dtype : NoneType
I0930 09:17:58.964214 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.has_bias : False
I0930 09:17:58.964263 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.inference_driver_name : NoneType
I0930 09:17:58.964315 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.input_dim : 0
I0930 09:17:58.964365 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_eval : NoneType
I0930 09:17:58.964413 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.is_inference : NoneType
I0930 09:17:58.964462 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.name : ''
I0930 09:17:58.964510 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.output_dim : 0
I0930 09:17:58.964559 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.method : 'xavier'
I0930 09:17:58.964607 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.scale : 1.000001
I0930 09:17:58.964655 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.params_init.seed : NoneType
I0930 09:17:58.964703 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.qdomain.default : NoneType
I0930 09:17:58.964752 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.random_seed : NoneType
I0930 09:17:58.964800 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.skip_lp_regularization : NoneType
I0930 09:17:58.964848 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.global_vn : False
I0930 09:17:58.964897 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.per_step_vn : False
I0930 09:17:58.964946 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.scale : NoneType
I0930 09:17:58.964994 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.vn.seed : NoneType
I0930 09:17:58.965043 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.projection.weight_norm : False
I0930 09:17:58.965091 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.qdomain.default : NoneType
I0930 09:17:58.965139 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.random_seed : NoneType
I0930 09:17:58.965187 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_connections : NoneType
I0930 09:17:58.965235 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.965283 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.global_vn : False
I0930 09:17:58.965332 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.per_step_vn : False
I0930 09:17:58.965381 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.scale : NoneType
I0930 09:17:58.965430 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.vn.seed : NoneType
I0930 09:17:58.965478 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fflayer_tpl.weight_norm : False
I0930 09:17:58.965527 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.fprop_dtype : NoneType
I0930 09:17:58.965575 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.hidden_dim : 8192
I0930 09:17:58.965624 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.inference_driver_name : NoneType
I0930 09:17:58.965673 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.input_dim : 0
I0930 09:17:58.965721 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_eval : NoneType
I0930 09:17:58.965775 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.is_inference : NoneType
I0930 09:17:58.965825 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.965874 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.cls : type/lingvo.core.layers/LayerNorm
I0930 09:17:58.965923 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.dtype : float32
I0930 09:17:58.965972 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.epsilon : 1e-06
I0930 09:17:58.966021 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.fprop_dtype : NoneType
I0930 09:17:58.966069 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.inference_driver_name : NoneType
I0930 09:17:58.966118 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.input_dim : 0
I0930 09:17:58.966167 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_eval : NoneType
I0930 09:17:58.966215 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.is_inference : NoneType
I0930 09:17:58.966278 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.name : ''
I0930 09:17:58.966331 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.method : 'xavier'
I0930 09:17:58.966381 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.scale : 1.000001
I0930 09:17:58.966429 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.params_init.seed : NoneType
I0930 09:17:58.966477 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.random_seed : NoneType
I0930 09:17:58.966526 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.966574 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.global_vn : False
I0930 09:17:58.966622 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.per_step_vn : False
I0930 09:17:58.966670 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.scale : NoneType
I0930 09:17:58.966719 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.ln_tpl.vn.seed : NoneType
I0930 09:17:58.966767 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.name : ''
I0930 09:17:58.966815 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.output_dim : 0
I0930 09:17:58.966863 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.method : 'xavier'
I0930 09:17:58.966911 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.scale : 1.000001
I0930 09:17:58.966959 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.params_init.seed : NoneType
I0930 09:17:58.967008 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.random_seed : NoneType
I0930 09:17:58.967056 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.relu_dropout_prob : 0.0
I0930 09:17:58.967105 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.activation : 'RELU'
I0930 09:17:58.967154 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.affine_last : False
I0930 09:17:58.967202 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.967251 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.batch_norm : True
I0930 09:17:58.967299 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bias_init : 0.0
I0930 09:17:58.967352 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.bn_fold_weights : NoneType
I0930 09:17:58.967401 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.cls : type/lingvo.core.layers/ProjectionLayer
I0930 09:17:58.967450 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.dtype : float32
I0930 09:17:58.967498 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.fprop_dtype : NoneType
I0930 09:17:58.967546 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.has_bias : False
I0930 09:17:58.967595 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.inference_driver_name : NoneType
I0930 09:17:58.967643 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.input_dim : 0
I0930 09:17:58.967691 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_eval : NoneType
I0930 09:17:58.967739 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.is_inference : NoneType
I0930 09:17:58.967787 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.name : ''
I0930 09:17:58.967835 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.output_dim : 0
I0930 09:17:58.967884 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.method : 'xavier'
I0930 09:17:58.967933 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.scale : 1.000001
I0930 09:17:58.967981 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.params_init.seed : NoneType
I0930 09:17:58.968030 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.qdomain.default : NoneType
I0930 09:17:58.968078 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.random_seed : NoneType
I0930 09:17:58.968127 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.968176 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.global_vn : False
I0930 09:17:58.968225 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.per_step_vn : False
I0930 09:17:58.968274 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.scale : NoneType
I0930 09:17:58.968322 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.vn.seed : NoneType
I0930 09:17:58.968371 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.res_proj_tpl.weight_norm : False
I0930 09:17:58.968421 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_prob : 0.0
I0930 09:17:58.968470 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.968519 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.cls : type/lingvo.core.layers/DropoutLayer
I0930 09:17:58.968567 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dropout_at_eval : False
I0930 09:17:58.968615 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.dtype : float32
I0930 09:17:58.968664 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.fprop_dtype : NoneType
I0930 09:17:58.968712 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.inference_driver_name : NoneType
I0930 09:17:58.968761 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_eval : NoneType
I0930 09:17:58.968813 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.is_inference : NoneType
I0930 09:17:58.968862 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.keep_prob : 1.0
I0930 09:17:58.968911 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.name : ''
I0930 09:17:58.968960 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape : NoneType
I0930 09:17:58.969008 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:17:58.969057 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.method : 'xavier'
I0930 09:17:58.969105 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.scale : 1.000001
I0930 09:17:58.969154 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.params_init.seed : NoneType
I0930 09:17:58.969202 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.random_seed : NoneType
I0930 09:17:58.969255 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.969305 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.global_vn : False
I0930 09:17:58.969353 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.per_step_vn : False
I0930 09:17:58.969402 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.scale : NoneType
I0930 09:17:58.969450 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.residual_dropout_tpl.vn.seed : NoneType
I0930 09:17:58.969499 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.969548 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.global_vn : False
I0930 09:17:58.969597 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.per_step_vn : False
I0930 09:17:58.969646 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.scale : NoneType
I0930 09:17:58.969695 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.tr_fflayer_tpl.vn.seed : NoneType
I0930 09:17:58.969744 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.transparent_merger_tpl : NoneType
I0930 09:17:58.969793 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.vn.global_vn : False
I0930 09:17:58.969841 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.vn.per_step_vn : False
I0930 09:17:58.969889 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.vn.scale : NoneType
I0930 09:17:58.969937 140009111922496 base_runner.py:59] task.lm.stack.encoder_tpl.vn.seed : NoneType
I0930 09:17:58.969985 140009111922496 base_runner.py:59] task.lm.stack.fprop_dtype : NoneType
I0930 09:17:58.970033 140009111922496 base_runner.py:59] task.lm.stack.inference_driver_name : NoneType
I0930 09:17:58.970081 140009111922496 base_runner.py:59] task.lm.stack.is_eval : NoneType
I0930 09:17:58.970130 140009111922496 base_runner.py:59] task.lm.stack.is_inference : NoneType
I0930 09:17:58.970178 140009111922496 base_runner.py:59] task.lm.stack.is_transparent : False
I0930 09:17:58.970226 140009111922496 base_runner.py:59] task.lm.stack.label_smoothing : NoneType
I0930 09:17:58.970294 140009111922496 base_runner.py:59] task.lm.stack.model_dim : 2048
I0930 09:17:58.970347 140009111922496 base_runner.py:59] task.lm.stack.name : ''
I0930 09:17:58.970396 140009111922496 base_runner.py:59] task.lm.stack.normalize_encoder : False
I0930 09:17:58.970445 140009111922496 base_runner.py:59] task.lm.stack.num_decoder_layers : 0
I0930 09:17:58.970499 140009111922496 base_runner.py:59] task.lm.stack.num_encoder_layers : 32
I0930 09:17:58.970549 140009111922496 base_runner.py:59] task.lm.stack.num_micro_batches : 32
I0930 09:17:58.970599 140009111922496 base_runner.py:59] task.lm.stack.packed_input : False
I0930 09:17:58.970647 140009111922496 base_runner.py:59] task.lm.stack.params_init.method : 'xavier'
I0930 09:17:58.970696 140009111922496 base_runner.py:59] task.lm.stack.params_init.scale : 1.000001
I0930 09:17:58.970744 140009111922496 base_runner.py:59] task.lm.stack.params_init.seed : NoneType
I0930 09:17:58.970793 140009111922496 base_runner.py:59] task.lm.stack.random_seed : NoneType
I0930 09:17:58.970842 140009111922496 base_runner.py:59] task.lm.stack.skip_lp_regularization : NoneType
I0930 09:17:58.970891 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.970940 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.apply_pruning : False
I0930 09:17:58.970988 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.chunk_size : 4194
I0930 09:17:58.971037 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.cls : type/lingvo.core.layers_with_gpipe/GPipeTransformerSoftmaxLayer
I0930 09:17:58.971086 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.dtype : float32
I0930 09:17:58.971134 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.fprop_dtype : NoneType
I0930 09:17:58.971183 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.inference_driver_name : NoneType
I0930 09:17:58.971231 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.input_dim : 2048
I0930 09:17:58.971280 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.inputs_from_decoder : False
I0930 09:17:58.971329 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.is_eval : NoneType
I0930 09:17:58.971377 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.is_inference : NoneType
I0930 09:17:58.971426 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.logits_abs_max : NoneType
I0930 09:17:58.971474 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.name : ''
I0930 09:17:58.971522 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.num_classes : 32000
I0930 09:17:58.971570 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.num_sampled : 0
I0930 09:17:58.971618 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.num_shards : 16
I0930 09:17:58.971666 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.method : 'xavier'
I0930 09:17:58.971715 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.scale : 1.000001
I0930 09:17:58.971763 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.params_init.seed : NoneType
I0930 09:17:58.971811 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.qdomain.default : NoneType
I0930 09:17:58.971859 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.random_seed : NoneType
I0930 09:17:58.971908 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.971956 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.vn.global_vn : False
I0930 09:17:58.972004 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.vn.per_step_vn : False
I0930 09:17:58.972051 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.vn.scale : NoneType
I0930 09:17:58.972099 140009111922496 base_runner.py:59] task.lm.stack.softmax_tpl.vn.seed : NoneType
I0930 09:17:58.972147 140009111922496 base_runner.py:59] task.lm.stack.splits : [8, 16, 24, 32]
I0930 09:17:58.972195 140009111922496 base_runner.py:59] task.lm.stack.state_dtype : float32
I0930 09:17:58.972242 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_dropout_prob : 0.1
I0930 09:17:58.972290 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.972338 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.cls : type/lingvo.core.layers_with_gpipe/DeterministicWeightsLayer
I0930 09:17:58.972391 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.allow_implicit_capture : NoneType
I0930 09:17:58.972439 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.cls : type/lingvo.core.layers/DeterministicDropoutLayer
I0930 09:17:58.972487 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dropout_at_eval : False
I0930 09:17:58.972535 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.dtype : float32
I0930 09:17:58.972584 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.fprop_dtype : NoneType
I0930 09:17:58.972632 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.inference_driver_name : NoneType
I0930 09:17:58.972680 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_eval : NoneType
I0930 09:17:58.972729 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.is_inference : NoneType
I0930 09:17:58.972777 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.keep_prob : 1.0
I0930 09:17:58.972825 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.name : ''
I0930 09:17:58.972873 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape : NoneType
I0930 09:17:58.972922 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.noise_shape_broadcast_dims : NoneType
I0930 09:17:58.972970 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.method : 'xavier'
I0930 09:17:58.973019 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.scale : 1.000001
I0930 09:17:58.973067 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.params_init.seed : NoneType
I0930 09:17:58.973116 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.random_seed : NoneType
I0930 09:17:58.973164 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.973213 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.global_vn : False
I0930 09:17:58.973261 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.per_step_vn : False
I0930 09:17:58.973311 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.scale : NoneType
I0930 09:17:58.973359 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dropout_tpl.vn.seed : NoneType
I0930 09:17:58.973408 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.dtype : float32
I0930 09:17:58.973457 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.fprop_dtype : NoneType
I0930 09:17:58.973505 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.global_weight_scale : 1.0
I0930 09:17:58.973554 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.inference_driver_name : NoneType
I0930 09:17:58.973603 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_eval : NoneType
I0930 09:17:58.973651 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.is_inference : NoneType
I0930 09:17:58.973700 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.minimal_prob : 0.0
I0930 09:17:58.973748 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.name : ''
I0930 09:17:58.973796 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.num_sources : 0
I0930 09:17:58.973845 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.method : 'xavier'
I0930 09:17:58.973894 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.scale : 1.000001
I0930 09:17:58.973946 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.params_init.seed : NoneType
I0930 09:17:58.973995 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.random_seed : NoneType
I0930 09:17:58.974043 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.skip_lp_regularization : NoneType
I0930 09:17:58.974091 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.global_vn : False
I0930 09:17:58.974140 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.per_step_vn : False
I0930 09:17:58.974189 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.scale : NoneType
I0930 09:17:58.974237 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.vn.seed : NoneType
I0930 09:17:58.974303 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_dropout_prob : 0.0
I0930 09:17:58.974354 140009111922496 base_runner.py:59] task.lm.stack.transparent_merger_tpl.weighted_merger_softmax : True
I0930 09:17:58.974402 140009111922496 base_runner.py:59] task.lm.stack.use_pipelined_embeddings : True
I0930 09:17:58.974451 140009111922496 base_runner.py:59] task.lm.stack.vn.global_vn : False
I0930 09:17:58.974500 140009111922496 base_runner.py:59] task.lm.stack.vn.per_step_vn : False
I0930 09:17:58.974547 140009111922496 base_runner.py:59] task.lm.stack.vn.scale : NoneType
I0930 09:17:58.974596 140009111922496 base_runner.py:59] task.lm.stack.vn.seed : NoneType
I0930 09:17:58.974644 140009111922496 base_runner.py:59] task.lm.vn.global_vn : False
I0930 09:17:58.974693 140009111922496 base_runner.py:59] task.lm.vn.per_step_vn : False
I0930 09:17:58.974741 140009111922496 base_runner.py:59] task.lm.vn.scale : NoneType
I0930 09:17:58.974789 140009111922496 base_runner.py:59] task.lm.vn.seed : NoneType
I0930 09:17:58.974838 140009111922496 base_runner.py:59] task.lm.vocab_size : 32000
I0930 09:17:58.974886 140009111922496 base_runner.py:59] task.name : '1bwds_wpm_level_lm'
I0930 09:17:58.974934 140009111922496 base_runner.py:59] task.online_encoder : NoneType
I0930 09:17:58.974982 140009111922496 base_runner.py:59] task.params_init.method : 'xavier'
I0930 09:17:58.975030 140009111922496 base_runner.py:59] task.params_init.scale : 1.000001
I0930 09:17:58.975078 140009111922496 base_runner.py:59] task.params_init.seed : NoneType
I0930 09:17:58.975127 140009111922496 base_runner.py:59] task.random_seed : NoneType
I0930 09:17:58.975176 140009111922496 base_runner.py:59] task.skip_lp_regularization : NoneType
I0930 09:17:58.975224 140009111922496 base_runner.py:59] task.train.bprop_variable_exclusion : NoneType
I0930 09:17:58.975272 140009111922496 base_runner.py:59] task.train.bprop_variable_filter : NoneType
I0930 09:17:58.975321 140009111922496 base_runner.py:59] task.train.clip_gradient_norm_to_value : 0.0
I0930 09:17:58.975369 140009111922496 base_runner.py:59] task.train.clip_gradient_single_norm_to_value : 0.0
I0930 09:17:58.975417 140009111922496 base_runner.py:59] task.train.colocate_gradients_with_ops : True
I0930 09:17:58.975465 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.jobname : 'eval_dev'
I0930 09:17:58.975513 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.local_filesystem : False
I0930 09:17:58.975562 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.logdir : ''
I0930 09:17:58.975610 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.metric : 'log_pplx'
I0930 09:17:58.975659 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.minimize : True
I0930 09:17:58.975708 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.name : 'MetricHistory'
I0930 09:17:58.975757 140009111922496 base_runner.py:59] task.train.early_stop.metric_history.tfevent_file : False
I0930 09:17:58.975805 140009111922496 base_runner.py:59] task.train.early_stop.min_steps : 0
I0930 09:17:58.975858 140009111922496 base_runner.py:59] task.train.early_stop.name : 'EarlyStop'
I0930 09:17:58.975908 140009111922496 base_runner.py:59] task.train.early_stop.tolerance : 0.0
I0930 09:17:58.975956 140009111922496 base_runner.py:59] task.train.early_stop.verbose : True
I0930 09:17:58.976005 140009111922496 base_runner.py:59] task.train.early_stop.window : 0
I0930 09:17:58.976053 140009111922496 base_runner.py:59] task.train.ema_decay : 0.0
I0930 09:17:58.976102 140009111922496 base_runner.py:59] task.train.enqueue_max_steps : -1
I0930 09:17:58.976150 140009111922496 base_runner.py:59] task.train.gate_gradients : False
I0930 09:17:58.976199 140009111922496 base_runner.py:59] task.train.grad_aggregation_method : 1
I0930 09:17:58.976247 140009111922496 base_runner.py:59] task.train.grad_norm_to_clip_to_zero : 0.0
I0930 09:17:58.976296 140009111922496 base_runner.py:59] task.train.grad_norm_tracker : NoneType
I0930 09:17:58.976344 140009111922496 base_runner.py:59] task.train.init_from_checkpoint_rules : {}
I0930 09:17:58.976392 140009111922496 base_runner.py:59] task.train.l1_regularizer_weight : NoneType
I0930 09:17:58.976439 140009111922496 base_runner.py:59] task.train.l2_regularizer_weight : 1e-06
I0930 09:17:58.976488 140009111922496 base_runner.py:59] task.train.learner : NoneType
I0930 09:17:58.976536 140009111922496 base_runner.py:59] task.train.learning_rate : 0.5
I0930 09:17:58.976585 140009111922496 base_runner.py:59] task.train.lr_schedule.allow_implicit_capture : NoneType
I0930 09:17:58.976633 140009111922496 base_runner.py:59] task.train.lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 09:17:58.976682 140009111922496 base_runner.py:59] task.train.lr_schedule.decay_end : NoneType
I0930 09:17:58.976730 140009111922496 base_runner.py:59] task.train.lr_schedule.dtype : float32
I0930 09:17:58.976778 140009111922496 base_runner.py:59] task.train.lr_schedule.fprop_dtype : NoneType
I0930 09:17:58.976827 140009111922496 base_runner.py:59] task.train.lr_schedule.inference_driver_name : NoneType
I0930 09:17:58.976876 140009111922496 base_runner.py:59] task.train.lr_schedule.is_eval : NoneType
I0930 09:17:58.976925 140009111922496 base_runner.py:59] task.train.lr_schedule.is_inference : NoneType
I0930 09:17:58.976973 140009111922496 base_runner.py:59] task.train.lr_schedule.model_dim : 2048
I0930 09:17:58.977022 140009111922496 base_runner.py:59] task.train.lr_schedule.name : 'LRSched'
I0930 09:17:58.977071 140009111922496 base_runner.py:59] task.train.lr_schedule.params_init.method : 'xavier'
I0930 09:17:58.977119 140009111922496 base_runner.py:59] task.train.lr_schedule.params_init.scale : 1.000001
I0930 09:17:58.977167 140009111922496 base_runner.py:59] task.train.lr_schedule.params_init.seed : NoneType
I0930 09:17:58.977216 140009111922496 base_runner.py:59] task.train.lr_schedule.random_seed : NoneType
I0930 09:17:58.977264 140009111922496 base_runner.py:59] task.train.lr_schedule.skip_lp_regularization : NoneType
I0930 09:17:58.977312 140009111922496 base_runner.py:59] task.train.lr_schedule.vn.global_vn : False
I0930 09:17:58.977360 140009111922496 base_runner.py:59] task.train.lr_schedule.vn.per_step_vn : False
I0930 09:17:58.977407 140009111922496 base_runner.py:59] task.train.lr_schedule.vn.scale : NoneType
I0930 09:17:58.977456 140009111922496 base_runner.py:59] task.train.lr_schedule.vn.seed : NoneType
I0930 09:17:58.977504 140009111922496 base_runner.py:59] task.train.lr_schedule.warmup_steps : 40000
I0930 09:17:58.977551 140009111922496 base_runner.py:59] task.train.lr_schedule.worker_replicas : 1
I0930 09:17:58.977600 140009111922496 base_runner.py:59] task.train.max_lstm_gradient_norm : 0.0
I0930 09:17:58.977647 140009111922496 base_runner.py:59] task.train.max_steps : 4000000
I0930 09:17:58.977695 140009111922496 base_runner.py:59] task.train.optimizer.allow_implicit_capture : NoneType
I0930 09:17:58.977743 140009111922496 base_runner.py:59] task.train.optimizer.beta1 : 0.9
I0930 09:17:58.977790 140009111922496 base_runner.py:59] task.train.optimizer.beta2 : 0.997
I0930 09:17:58.977843 140009111922496 base_runner.py:59] task.train.optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 09:17:58.977893 140009111922496 base_runner.py:59] task.train.optimizer.dtype : float32
I0930 09:17:58.977941 140009111922496 base_runner.py:59] task.train.optimizer.epsilon : 1e-09
I0930 09:17:58.977989 140009111922496 base_runner.py:59] task.train.optimizer.fprop_dtype : NoneType
I0930 09:17:58.978037 140009111922496 base_runner.py:59] task.train.optimizer.inference_driver_name : NoneType
I0930 09:17:58.978085 140009111922496 base_runner.py:59] task.train.optimizer.is_eval : NoneType
I0930 09:17:58.978133 140009111922496 base_runner.py:59] task.train.optimizer.is_inference : NoneType
I0930 09:17:58.978181 140009111922496 base_runner.py:59] task.train.optimizer.name : 'Adam'
I0930 09:17:58.978229 140009111922496 base_runner.py:59] task.train.optimizer.params_init.method : 'xavier'
I0930 09:17:58.978291 140009111922496 base_runner.py:59] task.train.optimizer.params_init.scale : 1.000001
I0930 09:17:58.978342 140009111922496 base_runner.py:59] task.train.optimizer.params_init.seed : NoneType
I0930 09:17:58.978390 140009111922496 base_runner.py:59] task.train.optimizer.random_seed : NoneType
I0930 09:17:58.978438 140009111922496 base_runner.py:59] task.train.optimizer.skip_lp_regularization : NoneType
I0930 09:17:58.978487 140009111922496 base_runner.py:59] task.train.optimizer.vn.global_vn : False
I0930 09:17:58.978535 140009111922496 base_runner.py:59] task.train.optimizer.vn.per_step_vn : False
I0930 09:17:58.978583 140009111922496 base_runner.py:59] task.train.optimizer.vn.scale : NoneType
I0930 09:17:58.978631 140009111922496 base_runner.py:59] task.train.optimizer.vn.seed : NoneType
I0930 09:17:58.978679 140009111922496 base_runner.py:59] task.train.pruning_hparams_dict : NoneType
I0930 09:17:58.978727 140009111922496 base_runner.py:59] task.train.save_interval_seconds : 600
I0930 09:17:58.978775 140009111922496 base_runner.py:59] task.train.save_keep_checkpoint_every_n_hours : 0.5
I0930 09:17:58.978823 140009111922496 base_runner.py:59] task.train.save_max_to_keep : 100
I0930 09:17:58.978871 140009111922496 base_runner.py:59] task.train.start_up_delay_steps : 200
I0930 09:17:58.978919 140009111922496 base_runner.py:59] task.train.sum_loss_across_tokens_in_batch : False
I0930 09:17:58.978967 140009111922496 base_runner.py:59] task.train.summary_interval_steps : 100
I0930 09:17:58.979015 140009111922496 base_runner.py:59] task.train.tpu_steps_per_loop : 100
I0930 09:17:58.979063 140009111922496 base_runner.py:59] task.train.vn_start_step : 20000
I0930 09:17:58.979111 140009111922496 base_runner.py:59] task.train.vn_std : 0.0
I0930 09:17:58.979159 140009111922496 base_runner.py:59] task.vn.global_vn : False
I0930 09:17:58.979207 140009111922496 base_runner.py:59] task.vn.per_step_vn : False
I0930 09:17:58.979254 140009111922496 base_runner.py:59] task.vn.scale : NoneType
I0930 09:17:58.979306 140009111922496 base_runner.py:59] task.vn.seed : NoneType
I0930 09:17:58.979355 140009111922496 base_runner.py:59] train.early_stop.metric_history.jobname : 'eval_dev'
I0930 09:17:58.979403 140009111922496 base_runner.py:59] train.early_stop.metric_history.local_filesystem : False
I0930 09:17:58.979452 140009111922496 base_runner.py:59] train.early_stop.metric_history.logdir : ''
I0930 09:17:58.979501 140009111922496 base_runner.py:59] train.early_stop.metric_history.metric : 'log_pplx'
I0930 09:17:58.979548 140009111922496 base_runner.py:59] train.early_stop.metric_history.minimize : True
I0930 09:17:58.979597 140009111922496 base_runner.py:59] train.early_stop.metric_history.name : 'MetricHistory'
I0930 09:17:58.979645 140009111922496 base_runner.py:59] train.early_stop.metric_history.tfevent_file : False
I0930 09:17:58.979693 140009111922496 base_runner.py:59] train.early_stop.min_steps : 0
I0930 09:17:58.979741 140009111922496 base_runner.py:59] train.early_stop.name : 'EarlyStop'
I0930 09:17:58.979789 140009111922496 base_runner.py:59] train.early_stop.tolerance : 0.0
I0930 09:17:58.979843 140009111922496 base_runner.py:59] train.early_stop.verbose : True
I0930 09:17:58.979892 140009111922496 base_runner.py:59] train.early_stop.window : 0
I0930 09:17:58.979940 140009111922496 base_runner.py:59] train.ema_decay : 0.0
I0930 09:17:58.979989 140009111922496 base_runner.py:59] train.enqueue_max_steps : -1
I0930 09:17:58.980037 140009111922496 base_runner.py:59] train.init_from_checkpoint_rules : {}
I0930 09:17:58.980085 140009111922496 base_runner.py:59] train.max_steps : 4000000
I0930 09:17:58.980134 140009111922496 base_runner.py:59] train.save_interval_seconds : 600
I0930 09:17:58.980182 140009111922496 base_runner.py:59] train.save_keep_checkpoint_every_n_hours : 0.5
I0930 09:17:58.980230 140009111922496 base_runner.py:59] train.save_max_to_keep : 100
I0930 09:17:58.980278 140009111922496 base_runner.py:59] train.start_up_delay_steps : 200
I0930 09:17:58.980326 140009111922496 base_runner.py:59] train.summary_interval_steps : 100
I0930 09:17:58.980374 140009111922496 base_runner.py:59] train.tpu_steps_per_loop : 100
I0930 09:17:58.980422 140009111922496 base_runner.py:59] vn.global_vn : False
I0930 09:17:58.980470 140009111922496 base_runner.py:59] vn.per_step_vn : False
I0930 09:17:58.980518 140009111922496 base_runner.py:59] vn.scale : NoneType
I0930 09:17:58.980567 140009111922496 base_runner.py:59] vn.seed : NoneType
I0930 09:17:58.980615 140009111922496 base_runner.py:59] 
I0930 09:17:58.980704 140009111922496 base_runner.py:60] ============================================================
I0930 09:17:58.982933 140009111922496 base_runner.py:106] Starting ...
I0930 09:17:58.983140 140009111922496 cluster.py:497] _LeastLoadedPlacer : ['/job:local/replica:0/task:0/device:CPU:0']
I0930 09:17:58.992019 140009111922496 cluster.py:515] Place variable global_step on /job:local/replica:0/task:0/device:CPU:0 8
I0930 09:17:59.005343 140009111922496 base_model.py:1093] Training parameters for <class 'lingvo.core.base_model.SingleTaskModel'>: {
  early_stop: {
    metric_history: {
"eval_dev"
      local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
      minimize: True
"MetricHistory"
      tfevent_file: False
    }
    min_steps: 0
"EarlyStop"
    tolerance: 0.0
    verbose: True
    window: 0
  }
  ema_decay: 0.0
  enqueue_max_steps: -1
  init_from_checkpoint_rules: {}
  max_steps: 4000000
  save_interval_seconds: 600
  save_keep_checkpoint_every_n_hours: 0.5
  save_max_to_keep: 100
  start_up_delay_steps: 200
  summary_interval_steps: 100
  tpu_steps_per_loop: 100
}
I0930 09:17:59.021500 140009111922496 base_model.py:301] input_params: {
  allow_implicit_capture: None
  bucket_adjust_every_n: 0
  bucket_batch_limit: [32]
  bucket_upper_bound: [1024]
  cls: <class 'lingvo.tasks.lm.input_generator.LmInput'>
  dtype: <dtype: 'float32'>
  file_buffer_size: 10000000
  file_datasource: None
  file_parallelism: 10
"text:/tmp/lm1b/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en*"
  file_random_seed: 301
  fixed_input_shape: True
  flush_every_n: 0
  fprop_dtype: None
  inference_driver_name: None
  is_eval: None
  is_inference: None
"1bwds_train_set"
  num_batcher_threads: 16
  num_samples: 0
  pad_to_max_seq_length: False
  params_init: {
"xavier"
    scale: 1.000001
    seed: None
  }
  random_seed: None
  remote: {
    max_inflights_per_target: 32
    shardable_batch: False
  }
  require_sequential_order: False
  skip_lp_regularization: None
  source_max_length: None
  target_max_length: 1024
  tokenizer: {
    allow_implicit_capture: None
    append_eos: True
    cls: <class 'lingvo.core.tokenizers.AsciiTokenizer'>
    dtype: <dtype: 'float32'>
    fprop_dtype: None
    inference_driver_name: None
    is_eval: None
    is_inference: None
"tokenizer"
    pad_to_max_length: True
    params_init: {
"xavier"
      scale: 1.000001
      seed: None
    }
    random_seed: None
    skip_lp_regularization: None
    target_eos_id: 2
    target_sos_id: 1
    target_unk_id: 0
    vn: {
      global_vn: False
      per_step_vn: False
      scale: None
      seed: None
    }
    vocab_size: 32000
  }
  tokenizer_dict: {}
  tpu_infeed_parallelism: 1
  use_chaining: False
  use_per_host_infeed: False
  use_within_batch_mixing: False
  vn: {
    global_vn: False
    per_step_vn: False
    scale: None
    seed: None
  }
}
I0930 09:17:59.024937 140009111922496 base_input_generator.py:624] bucket_batch_limit [32]
I0930 09:17:59.073415 140009111922496 learner.py:351] Ignoring legacy param start_up_delay_steps=200 for optimization program
I0930 09:17:59.073551 140009111922496 learner.py:351] Ignoring legacy param max_steps=4000000 for optimization program
I0930 09:17:59.073617 140009111922496 learner.py:351] Ignoring legacy param tpu_steps_per_loop=100 for optimization program
I0930 09:17:59.073673 140009111922496 learner.py:351] Ignoring legacy param vn_start_step=20000 for optimization program
I0930 09:17:59.073725 140009111922496 learner.py:351] Ignoring legacy param vn_std=0.0 for optimization program
I0930 09:17:59.073780 140009111922496 learner.py:351] Ignoring legacy param early_stop={
  metric_history: {
"eval_dev"
    local_filesystem: False
"/tmp/mnist/log"
"log_pplx"
    minimize: True
"MetricHistory"
    tfevent_file: False
  }
  min_steps: 0
"EarlyStop"
  tolerance: 0.0
  verbose: True
  window: 0
} for optimization program
I0930 09:17:59.073888 140009111922496 learner.py:351] Ignoring legacy param ema_decay=0.0 for optimization program
I0930 09:17:59.073943 140009111922496 learner.py:351] Ignoring legacy param init_from_checkpoint_rules={} for optimization program
I0930 09:17:59.073995 140009111922496 learner.py:351] Ignoring legacy param pruning_hparams_dict=None for optimization program
I0930 09:17:59.074045 140009111922496 learner.py:351] Ignoring legacy param enqueue_max_steps=-1 for optimization program
I0930 09:17:59.074095 140009111922496 learner.py:351] Ignoring legacy param save_interval_seconds=600 for optimization program
I0930 09:17:59.074143 140009111922496 learner.py:351] Ignoring legacy param save_max_to_keep=100 for optimization program
I0930 09:17:59.074191 140009111922496 learner.py:351] Ignoring legacy param save_keep_checkpoint_every_n_hours=0.5 for optimization program
I0930 09:17:59.074243 140009111922496 learner.py:351] Ignoring legacy param summary_interval_steps=100 for optimization program
I0930 09:17:59.074316 140009111922496 learner.py:351] Ignoring legacy param learner=None for optimization program
I0930 09:17:59.074402 140009111922496 learner.py:351] Ignoring legacy param max_lstm_gradient_norm=0.0 for optimization program
I0930 09:17:59.074456 140009111922496 learner.py:351] Ignoring legacy param sum_loss_across_tokens_in_batch=False for optimization program
I0930 09:17:59.074903 140009111922496 learner.py:356] Learner params: allow_implicit_capture : NoneType
I0930 09:17:59.074990 140009111922496 learner.py:356] Learner params: bprop_variable_exclusion : NoneType
I0930 09:17:59.075053 140009111922496 learner.py:356] Learner params: bprop_variable_filter : NoneType
I0930 09:17:59.075108 140009111922496 learner.py:356] Learner params: clip_gradient_norm_to_value : 0.0
I0930 09:17:59.075160 140009111922496 learner.py:356] Learner params: clip_gradient_single_norm_to_value : 0.0
I0930 09:17:59.075212 140009111922496 learner.py:356] Learner params: cls : type/lingvo.core.learner/Learner
I0930 09:17:59.075262 140009111922496 learner.py:356] Learner params: colocate_gradients_with_ops : True
I0930 09:17:59.075312 140009111922496 learner.py:356] Learner params: dtype : float32
I0930 09:17:59.075362 140009111922496 learner.py:356] Learner params: fprop_dtype : NoneType
I0930 09:17:59.075412 140009111922496 learner.py:356] Learner params: gate_gradients : False
I0930 09:17:59.075462 140009111922496 learner.py:356] Learner params: grad_aggregation_method : 1
I0930 09:17:59.075513 140009111922496 learner.py:356] Learner params: grad_norm_to_clip_to_zero : 0.0
I0930 09:17:59.075563 140009111922496 learner.py:356] Learner params: grad_norm_tracker : NoneType
I0930 09:17:59.075614 140009111922496 learner.py:356] Learner params: inference_driver_name : NoneType
I0930 09:17:59.075673 140009111922496 learner.py:356] Learner params: is_eval : NoneType
I0930 09:17:59.075725 140009111922496 learner.py:356] Learner params: is_inference : NoneType
I0930 09:17:59.075775 140009111922496 learner.py:356] Learner params: l1_regularizer_weight : NoneType
I0930 09:17:59.075825 140009111922496 learner.py:356] Learner params: l2_regularizer_weight : 1e-06
I0930 09:17:59.075875 140009111922496 learner.py:356] Learner params: learning_rate : 0.5
I0930 09:17:59.075925 140009111922496 learner.py:356] Learner params: lr_schedule.allow_implicit_capture : NoneType
I0930 09:17:59.075975 140009111922496 learner.py:356] Learner params: lr_schedule.cls : type/lingvo.core.schedule/TransformerLearningRateSchedule
I0930 09:17:59.076025 140009111922496 learner.py:356] Learner params: lr_schedule.decay_end : NoneType
I0930 09:17:59.076075 140009111922496 learner.py:356] Learner params: lr_schedule.dtype : float32
I0930 09:17:59.076124 140009111922496 learner.py:356] Learner params: lr_schedule.fprop_dtype : NoneType
I0930 09:17:59.076174 140009111922496 learner.py:356] Learner params: lr_schedule.inference_driver_name : NoneType
I0930 09:17:59.076223 140009111922496 learner.py:356] Learner params: lr_schedule.is_eval : NoneType
I0930 09:17:59.076272 140009111922496 learner.py:356] Learner params: lr_schedule.is_inference : NoneType
I0930 09:17:59.076321 140009111922496 learner.py:356] Learner params: lr_schedule.model_dim : 2048
I0930 09:17:59.076370 140009111922496 learner.py:356] Learner params: lr_schedule.name : 'LRSched'
I0930 09:17:59.076419 140009111922496 learner.py:356] Learner params: lr_schedule.params_init.method : 'xavier'
I0930 09:17:59.076468 140009111922496 learner.py:356] Learner params: lr_schedule.params_init.scale : 1.000001
I0930 09:17:59.076517 140009111922496 learner.py:356] Learner params: lr_schedule.params_init.seed : NoneType
I0930 09:17:59.076566 140009111922496 learner.py:356] Learner params: lr_schedule.random_seed : NoneType
I0930 09:17:59.076616 140009111922496 learner.py:356] Learner params: lr_schedule.skip_lp_regularization : NoneType
I0930 09:17:59.076665 140009111922496 learner.py:356] Learner params: lr_schedule.vn.global_vn : False
I0930 09:17:59.076714 140009111922496 learner.py:356] Learner params: lr_schedule.vn.per_step_vn : False
I0930 09:17:59.076763 140009111922496 learner.py:356] Learner params: lr_schedule.vn.scale : NoneType
I0930 09:17:59.076812 140009111922496 learner.py:356] Learner params: lr_schedule.vn.seed : NoneType
I0930 09:17:59.076861 140009111922496 learner.py:356] Learner params: lr_schedule.warmup_steps : 40000
I0930 09:17:59.076910 140009111922496 learner.py:356] Learner params: lr_schedule.worker_replicas : 1
I0930 09:17:59.076959 140009111922496 learner.py:356] Learner params: name : 'loss'
I0930 09:17:59.077008 140009111922496 learner.py:356] Learner params: optimizer.allow_implicit_capture : NoneType
I0930 09:17:59.077057 140009111922496 learner.py:356] Learner params: optimizer.beta1 : 0.9
I0930 09:17:59.077106 140009111922496 learner.py:356] Learner params: optimizer.beta2 : 0.997
I0930 09:17:59.077155 140009111922496 learner.py:356] Learner params: optimizer.cls : type/lingvo.core.optimizer/Adam
I0930 09:17:59.077203 140009111922496 learner.py:356] Learner params: optimizer.dtype : float32
I0930 09:17:59.077253 140009111922496 learner.py:356] Learner params: optimizer.epsilon : 1e-09
I0930 09:17:59.077302 140009111922496 learner.py:356] Learner params: optimizer.fprop_dtype : NoneType
I0930 09:17:59.077352 140009111922496 learner.py:356] Learner params: optimizer.inference_driver_name : NoneType
I0930 09:17:59.077401 140009111922496 learner.py:356] Learner params: optimizer.is_eval : NoneType
I0930 09:17:59.077449 140009111922496 learner.py:356] Learner params: optimizer.is_inference : NoneType
I0930 09:17:59.077498 140009111922496 learner.py:356] Learner params: optimizer.name : 'Adam'
I0930 09:17:59.077548 140009111922496 learner.py:356] Learner params: optimizer.params_init.method : 'xavier'
I0930 09:17:59.077598 140009111922496 learner.py:356] Learner params: optimizer.params_init.scale : 1.000001
I0930 09:17:59.077652 140009111922496 learner.py:356] Learner params: optimizer.params_init.seed : NoneType
I0930 09:17:59.077702 140009111922496 learner.py:356] Learner params: optimizer.random_seed : NoneType
I0930 09:17:59.077752 140009111922496 learner.py:356] Learner params: optimizer.skip_lp_regularization : NoneType
I0930 09:17:59.077802 140009111922496 learner.py:356] Learner params: optimizer.vn.global_vn : False
I0930 09:17:59.077851 140009111922496 learner.py:356] Learner params: optimizer.vn.per_step_vn : False
I0930 09:17:59.077901 140009111922496 learner.py:356] Learner params: optimizer.vn.scale : NoneType
I0930 09:17:59.077950 140009111922496 learner.py:356] Learner params: optimizer.vn.seed : NoneType
I0930 09:17:59.077999 140009111922496 learner.py:356] Learner params: params_init.method : 'xavier'
I0930 09:17:59.078048 140009111922496 learner.py:356] Learner params: params_init.scale : 1.000001
I0930 09:17:59.078097 140009111922496 learner.py:356] Learner params: params_init.seed : NoneType
I0930 09:17:59.078147 140009111922496 learner.py:356] Learner params: random_seed : NoneType
I0930 09:17:59.078196 140009111922496 learner.py:356] Learner params: skip_lp_regularization : NoneType
I0930 09:17:59.078245 140009111922496 learner.py:356] Learner params: vn.global_vn : False
I0930 09:17:59.078322 140009111922496 learner.py:356] Learner params: vn.per_step_vn : False
I0930 09:17:59.078373 140009111922496 learner.py:356] Learner params: vn.scale : NoneType
I0930 09:17:59.078423 140009111922496 learner.py:356] Learner params: vn.seed : NoneType
I0930 09:17:59.078472 140009111922496 learner.py:356] Learner params: 
I0930 09:17:59.322515 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var on /job:local/replica:0/task:0/device:CPU:0 262144008
I0930 09:17:59.324458 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0 shape=(32000, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.343292 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 278921224
I0930 09:17:59.345194 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.347743 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 278929416
I0930 09:17:59.349444 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.356293 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 295706632
I0930 09:17:59.358177 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.360810 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 295714824
I0930 09:17:59.362440 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.369237 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 312492040
I0930 09:17:59.371229 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.373766 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 312500232
I0930 09:17:59.375379 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.382272 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 329277448
I0930 09:17:59.384160 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.386718 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 329285640
I0930 09:17:59.388422 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.392115 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 329286152
I0930 09:17:59.393721 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.397552 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 329294344
I0930 09:17:59.399191 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.401840 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 329302536
I0930 09:17:59.403465 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.413675 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.420001 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 396411400
I0930 09:17:59.422008 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.424586 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 396444168
I0930 09:17:59.426188 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.428138 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.434320 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 463553032
I0930 09:17:59.436228 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.438868 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 463561224
I0930 09:17:59.440479 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.445124 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 463569416
I0930 09:17:59.446740 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.449392 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 463577608
I0930 09:17:59.451009 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.471255 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 480354824
I0930 09:17:59.473143 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.475678 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 480363016
I0930 09:17:59.477394 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.484248 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 497140232
I0930 09:17:59.486138 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.488784 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 497148424
I0930 09:17:59.490407 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.497342 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 513925640
I0930 09:17:59.499326 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.501842 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 513933832
I0930 09:17:59.503475 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.510864 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 530711048
I0930 09:17:59.512755 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.515365 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 530719240
I0930 09:17:59.517070 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.520798 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 530719752
I0930 09:17:59.522442 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.526393 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 530727944
I0930 09:17:59.527996 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.530649 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 530736136
I0930 09:17:59.532253 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.541905 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.548097 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 597845000
I0930 09:17:59.550042 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.552601 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 597877768
I0930 09:17:59.554202 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.556170 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.562349 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 664986632
I0930 09:17:59.564306 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.567608 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 664994824
I0930 09:17:59.569227 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.573872 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 665003016
I0930 09:17:59.575506 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.578176 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 665011208
I0930 09:17:59.579795 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.599593 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 681788424
I0930 09:17:59.601491 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.604054 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 681796616
I0930 09:17:59.605773 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.612643 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 698573832
I0930 09:17:59.614547 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.617720 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 698582024
I0930 09:17:59.619364 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.626320 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 715359240
I0930 09:17:59.628268 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.630843 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 715367432
I0930 09:17:59.632446 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.639379 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 732144648
I0930 09:17:59.641276 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.643879 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 732152840
I0930 09:17:59.645593 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.649309 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 732153352
I0930 09:17:59.650956 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.654869 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 732161544
I0930 09:17:59.656488 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.659167 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 732169736
I0930 09:17:59.660791 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.670472 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.677248 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 799278600
I0930 09:17:59.679245 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.681800 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 799311368
I0930 09:17:59.683438 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.685382 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.691544 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 866420232
I0930 09:17:59.693437 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.696093 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 866428424
I0930 09:17:59.697705 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.702363 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 866436616
I0930 09:17:59.703983 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.706682 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 866444808
I0930 09:17:59.708310 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.728945 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 883222024
I0930 09:17:59.730911 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.733481 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 883230216
I0930 09:17:59.735234 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.742124 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 900007432
I0930 09:17:59.744061 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.746770 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 900015624
I0930 09:17:59.748384 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.755233 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 916792840
I0930 09:17:59.757187 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.759752 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 916801032
I0930 09:17:59.761365 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.768345 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 933578248
I0930 09:17:59.770248 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.772843 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 933586440
I0930 09:17:59.774600 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.778317 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 933586952
I0930 09:17:59.779952 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.783857 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 933595144
I0930 09:17:59.785474 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.788144 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 933603336
I0930 09:17:59.789761 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.800063 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.806488 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1000712200
I0930 09:17:59.808489 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.811074 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1000744968
I0930 09:17:59.812731 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.814716 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.820862 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1067853832
I0930 09:17:59.822792 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.825430 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1067862024
I0930 09:17:59.827078 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.831772 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1067870216
I0930 09:17:59.833382 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.836087 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1067878408
I0930 09:17:59.837718 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.858133 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1084655624
I0930 09:17:59.860065 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.862634 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1084663816
I0930 09:17:59.864367 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.871250 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1101441032
I0930 09:17:59.873153 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.875843 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1101449224
I0930 09:17:59.877467 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.884359 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1118226440
I0930 09:17:59.886359 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.888931 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1118234632
I0930 09:17:59.890574 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.898386 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1135011848
I0930 09:17:59.900420 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.903157 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1135020040
I0930 09:17:59.904928 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.908714 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1135020552
I0930 09:17:59.910386 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.914341 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1135028744
I0930 09:17:59.915971 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.918667 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1135036936
I0930 09:17:59.920299 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.930177 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.936511 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1202145800
I0930 09:17:59.938562 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.941154 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1202178568
I0930 09:17:59.942825 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:17:59.944857 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:17:59.951089 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1269287432
I0930 09:17:59.953008 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.956367 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1269295624
I0930 09:17:59.957997 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.962687 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1269303816
I0930 09:17:59.964318 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.967036 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1269312008
I0930 09:17:59.968667 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.988590 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1286089224
I0930 09:17:59.990690 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:17:59.993234 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1286097416
I0930 09:17:59.995006 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.001846 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1302874632
I0930 09:18:00.003794 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.007002 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1302882824
I0930 09:18:00.008641 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.015549 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1319660040
I0930 09:18:00.017519 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.020101 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1319668232
I0930 09:18:00.021733 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.028672 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1336445448
I0930 09:18:00.030613 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.033190 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1336453640
I0930 09:18:00.034954 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.038649 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1336454152
I0930 09:18:00.040298 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.044213 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1336462344
I0930 09:18:00.045859 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.048550 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1336470536
I0930 09:18:00.050184 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.059883 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.066695 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1403579400
I0930 09:18:00.068677 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.071280 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1403612168
I0930 09:18:00.072928 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.074963 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.081199 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1470721032
I0930 09:18:00.083164 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.085825 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1470729224
I0930 09:18:00.087513 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.092286 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1470737416
I0930 09:18:00.093922 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.096628 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1470745608
I0930 09:18:00.098312 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.118964 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1487522824
I0930 09:18:00.120895 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.123485 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1487531016
I0930 09:18:00.125308 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.132179 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1504308232
I0930 09:18:00.134089 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.136793 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1504316424
I0930 09:18:00.138450 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.145313 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1521093640
I0930 09:18:00.147334 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.149894 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1521101832
I0930 09:18:00.151562 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.158505 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1537879048
I0930 09:18:00.160427 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.163036 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1537887240
I0930 09:18:00.164826 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.168567 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1537887752
I0930 09:18:00.170218 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.174148 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1537895944
I0930 09:18:00.175826 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.178512 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1537904136
I0930 09:18:00.180162 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.190436 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.196644 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1605013000
I0930 09:18:00.198662 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.201222 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1605045768
I0930 09:18:00.202919 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.204928 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.211182 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1672154632
I0930 09:18:00.213168 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.215947 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1672162824
I0930 09:18:00.217644 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.222684 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1672171016
I0930 09:18:00.224348 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.227108 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1672179208
I0930 09:18:00.228750 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.249464 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1688956424
I0930 09:18:00.251442 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.253995 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1688964616
I0930 09:18:00.255771 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.262715 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1705741832
I0930 09:18:00.264653 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.267352 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1705750024
I0930 09:18:00.269004 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.275910 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1722527240
I0930 09:18:00.277916 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.280516 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1722535432
I0930 09:18:00.282156 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.289623 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1739312648
I0930 09:18:00.291605 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.294209 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1739320840
I0930 09:18:00.295981 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.299708 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1739321352
I0930 09:18:00.301367 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.305335 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1739329544
I0930 09:18:00.307029 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.309709 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1739337736
I0930 09:18:00.311377 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.321079 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.327389 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 1806446600
I0930 09:18:00.329399 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.332011 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 1806479368
I0930 09:18:00.333668 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.335695 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.341839 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 1873588232
I0930 09:18:00.343797 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.347070 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 1873596424
I0930 09:18:00.348717 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.353475 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1873604616
I0930 09:18:00.355141 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.357860 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1873612808
I0930 09:18:00.359527 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.432883 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 1890390024
I0930 09:18:00.434938 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.437539 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1890398216
I0930 09:18:00.439306 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.446171 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 1907175432
I0930 09:18:00.448141 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.450860 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1907183624
I0930 09:18:00.452525 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.459865 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 1923960840
I0930 09:18:00.461861 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.464489 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1923969032
I0930 09:18:00.466141 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.473134 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 1940746248
I0930 09:18:00.475095 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.477722 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 1940754440
I0930 09:18:00.479504 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.483434 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 1940754952
I0930 09:18:00.485090 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.489127 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 1940763144
I0930 09:18:00.490807 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.493505 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 1940771336
I0930 09:18:00.495189 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.504971 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.511292 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2007880200
I0930 09:18:00.514026 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.516777 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2007912968
I0930 09:18:00.518509 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.520734 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.527220 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2075021832
I0930 09:18:00.529244 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.532018 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2075030024
I0930 09:18:00.533682 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.538631 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2075038216
I0930 09:18:00.540288 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.543030 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2075046408
I0930 09:18:00.544687 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.565451 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2091823624
I0930 09:18:00.567497 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.570077 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2091831816
I0930 09:18:00.571871 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.578789 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2108609032
I0930 09:18:00.580731 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.583427 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2108617224
I0930 09:18:00.585078 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.592000 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2125394440
I0930 09:18:00.594010 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.596630 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2125402632
I0930 09:18:00.598315 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.605244 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2142179848
I0930 09:18:00.607255 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.609869 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2142188040
I0930 09:18:00.611651 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.615409 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2142188552
I0930 09:18:00.617075 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.621089 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2142196744
I0930 09:18:00.622769 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.625454 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2142204936
I0930 09:18:00.627134 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.637485 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.643705 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2209313800
I0930 09:18:00.645703 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.648323 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2209346568
I0930 09:18:00.649982 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.652025 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.658212 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2276455432
I0930 09:18:00.660191 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.662886 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2276463624
I0930 09:18:00.664558 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.669328 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2276471816
I0930 09:18:00.671008 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.673726 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2276480008
I0930 09:18:00.675412 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.696125 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2293257224
I0930 09:18:00.698083 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.700712 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2293265416
I0930 09:18:00.702500 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.709385 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2310042632
I0930 09:18:00.711359 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.714054 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2310050824
I0930 09:18:00.715746 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.722687 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2326828040
I0930 09:18:00.724748 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.727379 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2326836232
I0930 09:18:00.729067 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.736046 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2343613448
I0930 09:18:00.737987 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.740649 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2343621640
I0930 09:18:00.742887 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.746640 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2343622152
I0930 09:18:00.748308 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.752352 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2343630344
I0930 09:18:00.754008 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.756731 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2343638536
I0930 09:18:00.758434 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.768153 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.774414 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2410747400
I0930 09:18:00.776426 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.779043 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2410780168
I0930 09:18:00.780716 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.782786 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.789004 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2477889032
I0930 09:18:00.790981 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.793678 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2477897224
I0930 09:18:00.795371 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.800141 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2477905416
I0930 09:18:00.801826 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.805032 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2477913608
I0930 09:18:00.807036 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.827260 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2494690824
I0930 09:18:00.829276 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.831959 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2494699016
I0930 09:18:00.833784 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.840823 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2511476232
I0930 09:18:00.842827 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.845560 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2511484424
I0930 09:18:00.847265 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.854918 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2528261640
I0930 09:18:00.856963 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.859620 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2528269832
I0930 09:18:00.861292 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.868316 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2545047048
I0930 09:18:00.870312 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.872951 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2545055240
I0930 09:18:00.874744 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.878716 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2545055752
I0930 09:18:00.880405 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.884472 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2545063944
I0930 09:18:00.886144 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.888882 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2545072136
I0930 09:18:00.890596 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.900573 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.906881 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2612181000
I0930 09:18:00.909527 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.912195 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2612213768
I0930 09:18:00.913874 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:00.915956 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:00.922167 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2679322632
I0930 09:18:00.924155 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.926974 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2679330824
I0930 09:18:00.928653 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.933494 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2679339016
I0930 09:18:00.935180 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.937927 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2679347208
I0930 09:18:00.939638 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.960428 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2696124424
I0930 09:18:00.962434 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.965026 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2696132616
I0930 09:18:00.966832 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.973785 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2712909832
I0930 09:18:00.975782 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.978524 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2712918024
I0930 09:18:00.980216 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.987141 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2729695240
I0930 09:18:00.989156 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:00.991805 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2729703432
I0930 09:18:00.993483 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.000471 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2746480648
I0930 09:18:01.002513 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.005152 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2746488840
I0930 09:18:01.006966 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.010765 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2746489352
I0930 09:18:01.012442 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.016507 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2746497544
I0930 09:18:01.018183 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.020907 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2746505736
I0930 09:18:01.022647 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.032997 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.039279 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 2813614600
I0930 09:18:01.041309 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.043945 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 2813647368
I0930 09:18:01.045621 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.047718 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.053916 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 2880756232
I0930 09:18:01.055902 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.058629 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 2880764424
I0930 09:18:01.060316 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.065156 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2880772616
I0930 09:18:01.066851 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.069608 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2880780808
I0930 09:18:01.071325 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.091919 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 2897558024
I0930 09:18:01.093902 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.096584 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2897566216
I0930 09:18:01.098397 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.105322 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 2914343432
I0930 09:18:01.107322 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.110029 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2914351624
I0930 09:18:01.111732 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.118719 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 2931128840
I0930 09:18:01.120753 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.123402 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2931137032
I0930 09:18:01.125148 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.132184 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 2947914248
I0930 09:18:01.134156 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.136836 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 2947922440
I0930 09:18:01.139185 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.143125 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 2947922952
I0930 09:18:01.144863 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.149010 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 2947931144
I0930 09:18:01.150718 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.153423 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 2947939336
I0930 09:18:01.155133 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.165148 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.171486 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3015048200
I0930 09:18:01.173522 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.176186 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3015080968
I0930 09:18:01.177882 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.179989 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.186239 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3082189832
I0930 09:18:01.188225 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.190954 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3082198024
I0930 09:18:01.192662 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.197504 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3082206216
I0930 09:18:01.199223 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.202505 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3082214408
I0930 09:18:01.204198 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.224434 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3098991624
I0930 09:18:01.226449 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.229076 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3098999816
I0930 09:18:01.230892 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.237811 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3115777032
I0930 09:18:01.239811 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.242563 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3115785224
I0930 09:18:01.244246 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.251713 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3132562440
I0930 09:18:01.253746 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.256417 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3132570632
I0930 09:18:01.258115 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.265137 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3149347848
I0930 09:18:01.267129 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.269792 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3149356040
I0930 09:18:01.271651 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.275467 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3149356552
I0930 09:18:01.277161 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.281282 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3149364744
I0930 09:18:01.282997 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.285725 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3149372936
I0930 09:18:01.287438 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.297317 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.303594 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3216481800
I0930 09:18:01.306185 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.308833 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3216514568
I0930 09:18:01.310556 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.312666 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.318933 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3283623432
I0930 09:18:01.320904 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.323696 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3283631624
I0930 09:18:01.325470 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.330362 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3283639816
I0930 09:18:01.332055 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.334840 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3283648008
I0930 09:18:01.336532 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.357250 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3300425224
I0930 09:18:01.359257 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.361903 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3300433416
I0930 09:18:01.363723 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.370718 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3317210632
I0930 09:18:01.372697 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.375454 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3317218824
I0930 09:18:01.377144 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.384105 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3333996040
I0930 09:18:01.386128 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.388795 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3334004232
I0930 09:18:01.390517 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.397534 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3350781448
I0930 09:18:01.399533 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.402200 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3350789640
I0930 09:18:01.404022 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.407842 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3350790152
I0930 09:18:01.409550 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.413657 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3350798344
I0930 09:18:01.415372 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.418127 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3350806536
I0930 09:18:01.419870 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.430309 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.436635 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3417915400
I0930 09:18:01.438724 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.441370 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3417948168
I0930 09:18:01.443098 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.445225 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.451562 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3485057032
I0930 09:18:01.453569 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.456350 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3485065224
I0930 09:18:01.458077 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.463056 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3485073416
I0930 09:18:01.464749 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.467555 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3485081608
I0930 09:18:01.469265 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.541849 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3501858824
I0930 09:18:01.543952 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.546649 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3501867016
I0930 09:18:01.548453 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.555454 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3518644232
I0930 09:18:01.557430 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.560178 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3518652424
I0930 09:18:01.561872 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.568933 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3535429640
I0930 09:18:01.571007 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.573665 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3535437832
I0930 09:18:01.575397 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.582389 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3552215048
I0930 09:18:01.584392 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.587077 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3552223240
I0930 09:18:01.588876 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.593209 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3552223752
I0930 09:18:01.594941 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.599117 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3552231944
I0930 09:18:01.600826 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.603590 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3552240136
I0930 09:18:01.605295 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.615264 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.621549 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3619349000
I0930 09:18:01.623633 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.626297 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3619381768
I0930 09:18:01.628015 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.630132 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.636419 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3686490632
I0930 09:18:01.638449 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.641187 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3686498824
I0930 09:18:01.642922 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.647846 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3686507016
I0930 09:18:01.649545 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.652333 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3686515208
I0930 09:18:01.654047 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.674785 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3703292424
I0930 09:18:01.676779 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.679428 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3703300616
I0930 09:18:01.681238 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.688215 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3720077832
I0930 09:18:01.690205 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.692978 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3720086024
I0930 09:18:01.694730 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.701655 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3736863240
I0930 09:18:01.704217 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.706890 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3736871432
I0930 09:18:01.708598 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.715640 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3753648648
I0930 09:18:01.717632 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.720347 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3753656840
I0930 09:18:01.722160 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.726095 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3753657352
I0930 09:18:01.727831 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.732079 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3753665544
I0930 09:18:01.733826 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.736632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3753673736
I0930 09:18:01.738374 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.748573 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.755505 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 3820782600
I0930 09:18:01.757769 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.760571 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 3820815368
I0930 09:18:01.762316 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.765188 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.771620 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 3887924232
I0930 09:18:01.773624 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.776391 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 3887932424
I0930 09:18:01.778116 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.783100 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3887940616
I0930 09:18:01.784810 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.787653 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3887948808
I0930 09:18:01.789360 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.809908 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 3904726024
I0930 09:18:01.811972 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.814664 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3904734216
I0930 09:18:01.817045 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.824060 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 3921511432
I0930 09:18:01.826117 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.828918 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3921519624
I0930 09:18:01.830657 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.837641 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 3938296840
I0930 09:18:01.839745 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.842438 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3938305032
I0930 09:18:01.844181 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.851235 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 3955082248
I0930 09:18:01.853237 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.855984 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 3955090440
I0930 09:18:01.857811 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.861736 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 3955090952
I0930 09:18:01.863487 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.867704 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 3955099144
I0930 09:18:01.869418 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.872208 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 3955107336
I0930 09:18:01.873930 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.884593 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.890963 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4022216200
I0930 09:18:01.893041 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.895821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4022248968
I0930 09:18:01.897551 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:01.899719 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:01.905977 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4089357832
I0930 09:18:01.908000 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.910770 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4089366024
I0930 09:18:01.912512 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.917495 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4089374216
I0930 09:18:01.919244 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.922032 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4089382408
I0930 09:18:01.923777 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.944814 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4106159624
I0930 09:18:01.946852 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.949506 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4106167816
I0930 09:18:01.951348 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.958343 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4122945032
I0930 09:18:01.960348 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.963118 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4122953224
I0930 09:18:01.964856 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.971841 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4139730440
I0930 09:18:01.973914 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.976592 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4139738632
I0930 09:18:01.978327 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.985397 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4156515848
I0930 09:18:01.987496 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.990198 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4156524040
I0930 09:18:01.992045 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:01.996474 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4156524552
I0930 09:18:01.998193 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.002443 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4156532744
I0930 09:18:02.004168 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.006963 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4156540936
I0930 09:18:02.008690 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.018781 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.025123 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4223649800
I0930 09:18:02.027220 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.029891 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4223682568
I0930 09:18:02.031650 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.033829 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.040206 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4290791432
I0930 09:18:02.042248 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.045063 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4290799624
I0930 09:18:02.046825 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.051916 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4290807816
I0930 09:18:02.053641 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.056475 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4290816008
I0930 09:18:02.058206 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.079376 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4307593224
I0930 09:18:02.081452 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.084208 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4307601416
I0930 09:18:02.086050 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.093132 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4324378632
I0930 09:18:02.095213 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.098060 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4324386824
I0930 09:18:02.099825 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.107065 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4341164040
I0930 09:18:02.109858 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.112672 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4341172232
I0930 09:18:02.114436 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.121601 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4357949448
I0930 09:18:02.123651 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.126531 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4357957640
I0930 09:18:02.128368 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.132363 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4357958152
I0930 09:18:02.134104 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.138471 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4357966344
I0930 09:18:02.140199 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.143018 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4357974536
I0930 09:18:02.144755 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.154898 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.161277 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4425083400
I0930 09:18:02.163395 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.166134 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4425116168
I0930 09:18:02.167879 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.170624 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.176916 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4492225032
I0930 09:18:02.178966 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.181742 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4492233224
I0930 09:18:02.183514 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.188536 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4492241416
I0930 09:18:02.190290 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.193128 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4492249608
I0930 09:18:02.194886 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.215407 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4509026824
I0930 09:18:02.217431 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.220113 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4509035016
I0930 09:18:02.222503 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.229607 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4525812232
I0930 09:18:02.232018 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.234819 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4525820424
I0930 09:18:02.236563 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.243557 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4542597640
I0930 09:18:02.245627 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.248331 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4542605832
I0930 09:18:02.250063 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.257125 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4559383048
I0930 09:18:02.259173 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.261887 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4559391240
I0930 09:18:02.263739 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.267660 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4559391752
I0930 09:18:02.269401 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.273675 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4559399944
I0930 09:18:02.275431 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.278212 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4559408136
I0930 09:18:02.279980 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.290684 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.297044 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4626517000
I0930 09:18:02.299166 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.301892 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4626549768
I0930 09:18:02.303647 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.305862 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.312171 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4693658632
I0930 09:18:02.314200 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.317011 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4693666824
I0930 09:18:02.318781 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.323866 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4693675016
I0930 09:18:02.325657 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.328515 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4693683208
I0930 09:18:02.330254 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.352167 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4710460424
I0930 09:18:02.354307 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.357022 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4710468616
I0930 09:18:02.358888 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.366041 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4727245832
I0930 09:18:02.368217 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.371101 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4727254024
I0930 09:18:02.372843 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.379961 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4744031240
I0930 09:18:02.382080 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.384812 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4744039432
I0930 09:18:02.386570 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.393693 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4760816648
I0930 09:18:02.395839 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.398751 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4760824840
I0930 09:18:02.400687 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.405565 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4760825352
I0930 09:18:02.407540 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.412317 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4760833544
I0930 09:18:02.414177 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.417119 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4760841736
I0930 09:18:02.418915 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.429128 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.435601 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 4827950600
I0930 09:18:02.437721 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.440464 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 4827983368
I0930 09:18:02.442225 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.444473 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.450880 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 4895092232
I0930 09:18:02.452912 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.455726 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 4895100424
I0930 09:18:02.457494 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.462650 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4895108616
I0930 09:18:02.464395 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.467237 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4895116808
I0930 09:18:02.468987 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.490467 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 4911894024
I0930 09:18:02.492581 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.495384 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4911902216
I0930 09:18:02.497238 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.504348 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 4928679432
I0930 09:18:02.506417 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.509264 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4928687624
I0930 09:18:02.511043 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.518108 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 4945464840
I0930 09:18:02.520918 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.523655 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4945473032
I0930 09:18:02.525439 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.532652 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 4962250248
I0930 09:18:02.534713 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.537439 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 4962258440
I0930 09:18:02.539369 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.543333 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 4962258952
I0930 09:18:02.545167 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.549524 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 4962267144
I0930 09:18:02.551297 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.554093 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 4962275336
I0930 09:18:02.555866 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.566139 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.572491 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5029384200
I0930 09:18:02.574613 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.577316 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5029416968
I0930 09:18:02.579096 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.581821 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.588151 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5096525832
I0930 09:18:02.590203 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.593775 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5096534024
I0930 09:18:02.595553 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.600642 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5096542216
I0930 09:18:02.602423 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.605251 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5096550408
I0930 09:18:02.607032 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.679770 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5113327624
I0930 09:18:02.681902 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.684783 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5113335816
I0930 09:18:02.686553 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.693963 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5130113032
I0930 09:18:02.696073 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.698794 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5130121224
I0930 09:18:02.700542 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.707645 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5146898440
I0930 09:18:02.709685 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.712409 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5146906632
I0930 09:18:02.714278 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.721261 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5163683848
I0930 09:18:02.723321 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.726201 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5163692040
I0930 09:18:02.727989 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.731925 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5163692552
I0930 09:18:02.733672 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.738039 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5163700744
I0930 09:18:02.739824 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.742673 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5163708936
I0930 09:18:02.744426 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.755293 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.761837 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5230817800
I0930 09:18:02.764026 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.766790 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5230850568
I0930 09:18:02.768549 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.770848 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.777208 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5297959432
I0930 09:18:02.779302 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.782117 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5297967624
I0930 09:18:02.783902 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.789101 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5297975816
I0930 09:18:02.790885 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.793735 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5297984008
I0930 09:18:02.795526 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.817097 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5314761224
I0930 09:18:02.819262 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.822015 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5314769416
I0930 09:18:02.823901 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.830982 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5331546632
I0930 09:18:02.833052 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.835903 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5331554824
I0930 09:18:02.837668 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.844711 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5348332040
I0930 09:18:02.846841 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.849546 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5348340232
I0930 09:18:02.851341 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.858453 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5365117448
I0930 09:18:02.860505 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.863277 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5365125640
I0930 09:18:02.865142 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.869242 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5365126152
I0930 09:18:02.871043 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.875391 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5365134344
I0930 09:18:02.877147 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.880473 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5365142536
I0930 09:18:02.882278 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.892489 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.898893 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5432251400
I0930 09:18:02.901028 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.903903 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5432284168
I0930 09:18:02.905662 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:02.907915 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:02.914200 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5499393032
I0930 09:18:02.916266 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.919072 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5499401224
I0930 09:18:02.920848 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.926002 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5499409416
I0930 09:18:02.927787 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.930660 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5499417608
I0930 09:18:02.932415 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.953463 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5516194824
I0930 09:18:02.955557 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.958253 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5516203016
I0930 09:18:02.960149 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.967178 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5532980232
I0930 09:18:02.969227 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.972065 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5532988424
I0930 09:18:02.973821 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.980926 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5549765640
I0930 09:18:02.983068 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.985787 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5549773832
I0930 09:18:02.987572 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.995054 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5566551048
I0930 09:18:02.997104 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:02.999875 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5566559240
I0930 09:18:03.001751 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.005707 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5566559752
I0930 09:18:03.007501 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.011845 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5566567944
I0930 09:18:03.013614 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.016449 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5566576136
I0930 09:18:03.018207 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:03.028360 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:03.034700 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5633685000
I0930 09:18:03.036815 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.039561 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5633717768
I0930 09:18:03.041331 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:03.043593 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:03.050447 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5700826632
I0930 09:18:03.052508 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.055330 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5700834824
I0930 09:18:03.057100 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.062214 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5700843016
I0930 09:18:03.064015 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.066887 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5700851208
I0930 09:18:03.068647 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.089209 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5717628424
I0930 09:18:03.091302 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.094010 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5717636616
I0930 09:18:03.095916 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.103497 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5734413832
I0930 09:18:03.105559 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.108504 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5734422024
I0930 09:18:03.110293 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.117317 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5751199240
I0930 09:18:03.119440 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.122172 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5751207432
I0930 09:18:03.123963 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.131118 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5767984648
I0930 09:18:03.133168 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.135958 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5767992840
I0930 09:18:03.137821 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.141809 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5767993352
I0930 09:18:03.143609 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.147953 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5768001544
I0930 09:18:03.149719 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.152574 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5768009736
I0930 09:18:03.154371 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:03.165106 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:03.171456 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 5835118600
I0930 09:18:03.173569 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.176340 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 5835151368
I0930 09:18:03.178107 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:03.180388 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:03.186717 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 5902260232
I0930 09:18:03.188766 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.191614 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 5902268424
I0930 09:18:03.193394 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.198536 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5902276616
I0930 09:18:03.200306 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.203194 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5902284808
I0930 09:18:03.204971 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.226071 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 5919062024
I0930 09:18:03.228162 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.230916 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5919070216
I0930 09:18:03.232801 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.239822 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 5935847432
I0930 09:18:03.241892 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.244743 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5935855624
I0930 09:18:03.246526 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.253538 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 5952632840
I0930 09:18:03.255684 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.258468 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5952641032
I0930 09:18:03.260247 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.267353 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 5969418248
I0930 09:18:03.269418 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.272214 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 5969426440
I0930 09:18:03.274086 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.278164 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 5969426952
I0930 09:18:03.279973 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.284346 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 5969435144
I0930 09:18:03.286110 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.289399 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 5969443336
I0930 09:18:03.291205 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:03.301429 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:03.307811 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6036552200
I0930 09:18:03.309935 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.312721 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6036584968
I0930 09:18:03.314534 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:03.316799 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:03.323129 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6103693832
I0930 09:18:03.325200 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.328121 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6103702024
I0930 09:18:03.329915 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.335110 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6103710216
I0930 09:18:03.336894 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.339794 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6103718408
I0930 09:18:03.341568 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.362932 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6120495624
I0930 09:18:03.365024 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.367788 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6120503816
I0930 09:18:03.369662 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.376759 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6137281032
I0930 09:18:03.378860 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.381710 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6137289224
I0930 09:18:03.383518 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.390644 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6154066440
I0930 09:18:03.392766 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.395542 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6154074632
I0930 09:18:03.397319 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.404930 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6170851848
I0930 09:18:03.407027 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.409783 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6170860040
I0930 09:18:03.411687 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.415733 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6170860552
I0930 09:18:03.417522 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.421926 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6170868744
I0930 09:18:03.423742 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.426599 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6170876936
I0930 09:18:03.428429 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:03.438685 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:03.445109 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6237985800
I0930 09:18:03.447279 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.450015 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6238018568
I0930 09:18:03.451848 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:03.454120 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:03.461011 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6305127432
I0930 09:18:03.463129 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.465952 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6305135624
I0930 09:18:03.467773 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.472982 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6305143816
I0930 09:18:03.474788 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.477659 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6305152008
I0930 09:18:03.479466 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.500183 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6321929224
I0930 09:18:03.502296 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.505042 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6321937416
I0930 09:18:03.506956 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.514568 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6338714632
I0930 09:18:03.516638 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.519508 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6338722824
I0930 09:18:03.521360 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.528512 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6355500040
I0930 09:18:03.530675 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.533421 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6355508232
I0930 09:18:03.535235 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.542407 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6372285448
I0930 09:18:03.544481 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.547283 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6372293640
I0930 09:18:03.549157 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.553222 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6372294152
I0930 09:18:03.555041 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.559496 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6372302344
I0930 09:18:03.561281 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:03.564144 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6372310536
I0930 09:18:03.566048 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:03.582867 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.225816 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6439419400
I0930 09:18:04.228213 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.231091 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6439452168
I0930 09:18:04.232888 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.235280 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.241669 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6506561032
I0930 09:18:04.243804 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.246701 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6506569224
I0930 09:18:04.248522 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.253789 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6506577416
I0930 09:18:04.255617 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.258553 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6506585608
I0930 09:18:04.260350 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.281999 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var on /job:local/replica:0/task:0/device:CPU:0 6523362824
I0930 09:18:04.284130 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.287016 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6523371016
I0930 09:18:04.288809 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.295913 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var on /job:local/replica:0/task:0/device:CPU:0 6540148232
I0930 09:18:04.298077 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.300847 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6540156424
I0930 09:18:04.302682 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.309930 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var on /job:local/replica:0/task:0/device:CPU:0 6556933640
I0930 09:18:04.312066 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.314846 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6556941832
I0930 09:18:04.316773 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.323904 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var on /job:local/replica:0/task:0/device:CPU:0 6573719048
I0930 09:18:04.326018 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0 shape=(2048, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.329030 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var on /job:local/replica:0/task:0/device:CPU:0 6573727240
I0930 09:18:04.330873 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.334957 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var on /job:local/replica:0/task:0/device:CPU:0 6573727752
I0930 09:18:04.336785 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0 shape=(128,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.341313 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6573735944
I0930 09:18:04.343160 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.346045 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6573744136
I0930 09:18:04.347904 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.359122 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.365573 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var on /job:local/replica:0/task:0/device:CPU:0 6640853000
I0930 09:18:04.367784 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0 shape=(2048, 8192) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.370600 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var on /job:local/replica:0/task:0/device:CPU:0 6640885768
I0930 09:18:04.372429 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0 shape=(8192,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.374779 140009111922496 py_utils.py:1229] WARNING!!! var w is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.381167 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var on /job:local/replica:0/task:0/device:CPU:0 6707994632
I0930 09:18:04.383307 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0 shape=(8192, 2048) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.386146 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var on /job:local/replica:0/task:0/device:CPU:0 6708002824
I0930 09:18:04.388008 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.393276 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var on /job:local/replica:0/task:0/device:CPU:0 6708011016
I0930 09:18:04.395137 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.398034 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var on /job:local/replica:0/task:0/device:CPU:0 6708019208
I0930 09:18:04.399873 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0 shape=(2048,) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.405464 140009111922496 py_utils.py:1229] WARNING!!! var weight_0 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.411915 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var on /job:local/replica:0/task:0/device:CPU:0 6724403208
I0930 09:18:04.414091 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.414964 140009111922496 py_utils.py:1229] WARNING!!! var weight_1 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.421883 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var on /job:local/replica:0/task:0/device:CPU:0 6740787208
I0930 09:18:04.424012 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.424852 140009111922496 py_utils.py:1229] WARNING!!! var weight_2 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.431158 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var on /job:local/replica:0/task:0/device:CPU:0 6757171208
I0930 09:18:04.433245 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.434093 140009111922496 py_utils.py:1229] WARNING!!! var weight_3 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.440472 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var on /job:local/replica:0/task:0/device:CPU:0 6773555208
I0930 09:18:04.442585 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.443424 140009111922496 py_utils.py:1229] WARNING!!! var weight_4 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.449715 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var on /job:local/replica:0/task:0/device:CPU:0 6789939208
I0930 09:18:04.451823 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.452666 140009111922496 py_utils.py:1229] WARNING!!! var weight_5 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.459000 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var on /job:local/replica:0/task:0/device:CPU:0 6806323208
I0930 09:18:04.461091 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.461930 140009111922496 py_utils.py:1229] WARNING!!! var weight_6 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.468220 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var on /job:local/replica:0/task:0/device:CPU:0 6822707208
I0930 09:18:04.470334 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.471179 140009111922496 py_utils.py:1229] WARNING!!! var weight_7 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.477480 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var on /job:local/replica:0/task:0/device:CPU:0 6839091208
I0930 09:18:04.479588 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.480444 140009111922496 py_utils.py:1229] WARNING!!! var weight_8 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.486703 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var on /job:local/replica:0/task:0/device:CPU:0 6855475208
I0930 09:18:04.488776 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.489611 140009111922496 py_utils.py:1229] WARNING!!! var weight_9 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.496607 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var on /job:local/replica:0/task:0/device:CPU:0 6871859208
I0930 09:18:04.498716 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.499558 140009111922496 py_utils.py:1229] WARNING!!! var weight_10 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.505786 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var on /job:local/replica:0/task:0/device:CPU:0 6888243208
I0930 09:18:04.507904 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.508738 140009111922496 py_utils.py:1229] WARNING!!! var weight_11 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.515064 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var on /job:local/replica:0/task:0/device:CPU:0 6904627208
I0930 09:18:04.517161 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.518028 140009111922496 py_utils.py:1229] WARNING!!! var weight_12 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.524425 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var on /job:local/replica:0/task:0/device:CPU:0 6921011208
I0930 09:18:04.526570 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.527431 140009111922496 py_utils.py:1229] WARNING!!! var weight_13 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.533737 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var on /job:local/replica:0/task:0/device:CPU:0 6937395208
I0930 09:18:04.535835 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.536666 140009111922496 py_utils.py:1229] WARNING!!! var weight_14 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.542968 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var on /job:local/replica:0/task:0/device:CPU:0 6953779208
I0930 09:18:04.545035 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
W0930 09:18:04.545873 140009111922496 py_utils.py:1229] WARNING!!! var weight_15 is using the default xavier initializer. Make sure this is intended.
I0930 09:18:04.552233 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var on /job:local/replica:0/task:0/device:CPU:0 6970163208
I0930 09:18:04.554331 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0 shape=(2048, 2000) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.557147 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var on /job:local/replica:0/task:0/device:CPU:0 6970171208
I0930 09:18:04.559083 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.561824 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var on /job:local/replica:0/task:0/device:CPU:0 6970179208
I0930 09:18:04.563646 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.566615 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var on /job:local/replica:0/task:0/device:CPU:0 6970187208
I0930 09:18:04.568406 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.571172 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var on /job:local/replica:0/task:0/device:CPU:0 6970195208
I0930 09:18:04.572956 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.576351 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var on /job:local/replica:0/task:0/device:CPU:0 6970203208
I0930 09:18:04.578153 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.580919 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var on /job:local/replica:0/task:0/device:CPU:0 6970211208
I0930 09:18:04.582725 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.585551 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var on /job:local/replica:0/task:0/device:CPU:0 6970219208
I0930 09:18:04.587369 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.590116 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var on /job:local/replica:0/task:0/device:CPU:0 6970227208
I0930 09:18:04.592089 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.594942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var on /job:local/replica:0/task:0/device:CPU:0 6970235208
I0930 09:18:04.596744 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.599629 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var on /job:local/replica:0/task:0/device:CPU:0 6970243208
I0930 09:18:04.601418 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.604182 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var on /job:local/replica:0/task:0/device:CPU:0 6970251208
I0930 09:18:04.605983 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.608865 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var on /job:local/replica:0/task:0/device:CPU:0 6970259208
I0930 09:18:04.610695 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.613420 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var on /job:local/replica:0/task:0/device:CPU:0 6970267208
I0930 09:18:04.615249 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.618095 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var on /job:local/replica:0/task:0/device:CPU:0 6970275208
I0930 09:18:04.619905 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.622681 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var on /job:local/replica:0/task:0/device:CPU:0 6970283208
I0930 09:18:04.624567 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:04.627344 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var on /job:local/replica:0/task:0/device:CPU:0 6970291208
I0930 09:18:04.629132 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0 shape=(2000,) on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:05.290483 140009111922496 py_utils.py:1484] === worker 0 ===
I0930 09:18:05.305491 140009111922496 py_utils.py:1474] worker 0: global_step                                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.305588 140009111922496 py_utils.py:1474] worker 0: input._tokenizer_default.global_step                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.305647 140009111922496 py_utils.py:1474] worker 0: input.global_step                                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.305700 140009111922496 py_utils.py:1474] worker 0: learners[0].global_step                                               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.305748 140009111922496 py_utils.py:1474] worker 0: learners[0].lr_schedule.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.305796 140009111922496 py_utils.py:1474] worker 0: learners[0].optimizer.global_step                                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.305852 140009111922496 py_utils.py:1474] worker 0: lm.global_step                                                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.305899 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.global_step                                       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.305945 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_dropout.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.305990 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_pos_emb.global_step                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306036 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306082 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.emb.src_token_emb.wm                                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306127 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306173 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306218 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306284 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306337 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306384 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306429 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306475 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306520 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306565 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306610 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306655 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306701 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306751 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306797 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306843 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306888 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306932 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.306977 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307022 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307068 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307112 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307158 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307203 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307248 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307292 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307337 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307382 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307427 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307471 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307516 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_0.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307561 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307611 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307657 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307702 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307748 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307793 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307839 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307884 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307929 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.307974 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308019 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308065 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308111 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308156 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308201 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308246 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308292 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308337 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308382 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308431 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308477 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308522 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308567 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308612 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308657 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308702 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308746 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308792 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308836 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308881 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308926 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_1.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.308971 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309016 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309061 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309106 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309150 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309195 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309240 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309288 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309334 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309379 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309424 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309469 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309514 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309559 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309604 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309649 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309694 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309739 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309784 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309828 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309873 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309918 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.309963 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310008 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310052 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310101 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310147 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310192 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310238 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310301 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310348 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_2.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310394 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310439 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310484 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310529 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310574 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310620 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310666 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310711 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310756 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310801 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310847 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310891 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310936 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.310985 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311031 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311076 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311121 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311166 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311212 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311257 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311302 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311347 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311391 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311436 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311481 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311526 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311572 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311617 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311662 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311707 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311752 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_3.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311800 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311851 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311897 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311942 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.311986 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312031 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312076 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312121 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312166 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312211 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312257 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312303 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312348 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312393 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312438 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312484 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312528 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312573 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312618 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312666 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312712 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312757 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312802 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312847 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312892 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312936 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.312981 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313026 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313071 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313116 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313161 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_4.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313206 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313251 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313296 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313340 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313385 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313431 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313479 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313524 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313569 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313614 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313659 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313704 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313749 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313794 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313838 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313884 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313928 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.313973 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314018 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314063 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314108 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314153 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314198 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314243 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314305 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314355 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314401 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314447 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314492 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314538 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314583 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_5.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314628 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314673 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314718 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314763 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314808 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314854 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314899 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314944 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.314989 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315034 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315079 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315124 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315172 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315218 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315263 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315308 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315353 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315398 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315443 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315488 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315533 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315578 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315622 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315667 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315712 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315757 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315801 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315846 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315891 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315935 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.315980 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_6.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316028 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316074 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316119 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316164 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316209 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316253 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316298 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316343 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316387 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316432 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316477 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316523 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316568 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316613 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316658 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316703 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316748 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316793 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316842 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316888 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316933 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.316979 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317024 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317069 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317114 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317159 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317203 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317249 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317293 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317338 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317382 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.encoder_7.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317427 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_0.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317472 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317517 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317563 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317608 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317652 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317701 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317747 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317792 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317837 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317882 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317927 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.317971 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318016 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318061 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318106 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318151 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318196 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318241 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318308 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318356 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318401 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318446 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318492 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318541 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318587 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318632 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318677 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318723 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318769 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318814 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318859 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_10.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318905 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318950 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.318995 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319041 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319086 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319131 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319176 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319221 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319267 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319311 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319356 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319406 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319453 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319498 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319543 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319588 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319634 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319679 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319725 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319771 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319817 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319863 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319908 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.319954 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320000 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320045 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320091 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320136 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320182 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320226 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320275 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_11.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320321 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320366 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320410 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320455 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320500 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320545 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320590 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320635 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320680 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320725 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320770 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320815 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320860 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320905 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320951 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.320996 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321041 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321089 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321135 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321181 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321226 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321270 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321316 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321361 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321406 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321451 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321496 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321542 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321587 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321632 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321677 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_12.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321723 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321768 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321813 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321859 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321908 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.321957 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322002 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322047 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322093 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322138 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322183 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322228 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322289 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322338 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322383 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322429 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322474 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322519 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322564 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322609 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322654 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322700 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322745 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322794 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322840 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322886 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322932 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.322977 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323022 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323067 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323113 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_13.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323158 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323204 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323248 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323294 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323339 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323385 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323430 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323476 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323521 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323566 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323612 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323660 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323707 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323752 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323797 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323842 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323887 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323932 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.323977 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324022 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324067 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324112 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324158 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324203 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324247 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324292 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324337 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324382 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324427 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324477 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324523 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_14.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324568 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324613 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324658 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324703 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324748 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324793 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324837 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324882 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324926 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.324971 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325015 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325060 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325105 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325150 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325195 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325239 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325284 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325332 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325378 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325423 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325469 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325514 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325559 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325603 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325648 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325699 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325745 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325790 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325834 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325879 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325924 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_15.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.325969 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326014 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326058 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326103 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326152 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326198 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326243 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326306 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326353 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326398 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326443 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326488 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326534 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326579 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326625 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326670 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326714 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326760 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326804 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326849 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326894 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.326945 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327019 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327098 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327166 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327214 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327260 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327306 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327352 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327398 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327444 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_8.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327489 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[0].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327534 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.dropout[1].global_step      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327580 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327625 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327671 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[0].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327716 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].b                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327761 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327806 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc[1].w                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327851 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.fflayer.global_step                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327896 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.global_step                         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327945 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.327993 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.global_step              /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328040 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328085 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.fflayer.residual_dropout.global_step        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328130 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.global_step                                 /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328176 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328222 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328267 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328313 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b            /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328358 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328403 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328449 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328495 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328540 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328586 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328631 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328677 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.global_step                      /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328722 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328768 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.global_step           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328817 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328864 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.encoder_9.self_atten.residual_dropout.global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328910 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_1.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.328955 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329000 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329046 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329091 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329136 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329182 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329227 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329272 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329318 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329363 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329408 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329454 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329499 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329544 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329589 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329639 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329684 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329730 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329775 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329821 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329866 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329910 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.329955 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330000 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330045 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330090 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330136 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330181 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330226 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330293 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330343 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_16.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330389 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330434 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330479 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330529 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330574 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330620 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330665 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330711 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330756 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330801 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330846 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330891 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330937 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.330982 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331027 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331072 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331117 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331161 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331206 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331251 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331296 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331346 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331392 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331437 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331483 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331528 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331573 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331618 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331664 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331709 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331754 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_17.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331800 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331846 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331892 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331940 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.331987 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332032 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332077 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332122 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332167 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332216 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332262 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332307 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332352 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332397 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332442 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332486 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332531 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332576 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332621 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332665 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332710 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332754 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332799 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332844 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332888 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332933 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.332978 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333023 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333072 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333118 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333163 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_18.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333208 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333253 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333298 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333343 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333388 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333433 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333477 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333521 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333566 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333610 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333655 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333699 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333744 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333789 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333833 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333882 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333928 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.333973 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334018 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334062 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334107 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334152 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334197 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334241 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334302 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334350 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334395 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334439 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334484 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334529 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334574 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_19.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334619 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334664 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334709 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334758 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334804 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334849 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334894 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334939 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.334983 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335029 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335074 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335119 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335164 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335209 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335255 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335300 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335345 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335390 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335435 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335480 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335526 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335575 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335621 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335667 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335712 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335757 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335803 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335847 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335892 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335938 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.335983 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_20.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336028 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336073 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336118 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336163 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336208 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336253 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336298 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336343 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336388 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336437 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336483 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336528 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336573 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336617 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336662 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336707 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336752 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336797 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336842 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336887 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336932 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.336977 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337021 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337065 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337110 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337155 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337199 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337248 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337293 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337338 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337383 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_21.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337427 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337472 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337516 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337561 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337606 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337650 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337694 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337739 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337784 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337828 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337873 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337918 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.337962 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338006 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338051 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338099 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338145 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338190 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338234 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338295 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338342 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338387 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338432 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338477 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338521 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338566 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338611 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338656 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338701 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338745 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338790 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_22.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338835 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338880 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338928 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.338974 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339019 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339064 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339109 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339154 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339198 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339242 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339287 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339332 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339376 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339421 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339465 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339510 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339554 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339598 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339643 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339687 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339732 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339780 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339825 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339870 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339915 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.339960 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340005 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340050 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340095 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340140 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340184 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.encoder_23.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340229 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_2.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340274 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340318 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340363 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340408 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340452 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340496 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340540 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340589 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340634 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340679 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340723 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340768 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340813 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340857 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340902 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340946 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.340991 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341035 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341079 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341123 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341167 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341212 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341256 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341300 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341345 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341390 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341438 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341484 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341529 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341573 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341619 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_24.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341663 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341707 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341752 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341797 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341842 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341887 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341931 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.341977 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342025 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342071 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342115 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342160 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342205 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342254 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342321 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342368 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342413 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342458 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342504 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342549 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342595 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342640 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342685 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342730 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342776 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342822 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342867 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342913 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.342958 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343003 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343049 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_25.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343094 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343145 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343191 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343237 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343282 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343328 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343373 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343419 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343465 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343511 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343557 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343602 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343648 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343693 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343739 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343785 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343830 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343876 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343921 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.343970 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344016 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344062 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344107 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344153 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344198 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344243 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344289 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344334 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344380 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344425 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344471 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_26.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344516 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344562 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344607 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344652 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344697 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344742 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344786 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344835 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344881 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344926 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.344972 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345017 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345062 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345107 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345152 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345196 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345241 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345286 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345331 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345376 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345420 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345466 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345510 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345555 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345600 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345644 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345696 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345741 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345786 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345831 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345876 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_27.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345921 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.345966 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346011 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346056 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346101 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346146 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346191 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346236 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346298 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346345 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346390 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346437 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346482 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346531 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346577 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346622 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346668 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346712 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346757 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346802 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346847 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346892 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346937 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.346982 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347026 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347071 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347116 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347161 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347206 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347251 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347296 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_28.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347341 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347390 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347437 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347482 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347527 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347573 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347617 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347663 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347708 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347753 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347798 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347843 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347889 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347934 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.347979 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348023 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348068 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348113 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348158 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348207 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348253 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348298 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348344 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348389 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348434 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348479 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348524 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348569 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348614 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348659 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348704 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_29.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348748 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348793 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348838 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348883 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348928 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.348973 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349018 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349066 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349112 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349157 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349202 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349247 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349293 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349338 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349383 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349428 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349472 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349517 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349561 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349605 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349650 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349694 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349739 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349783 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349827 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349876 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349921 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.349967 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350011 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350056 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350100 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_30.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350145 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[0].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350190 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.dropout[1].global_step     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350234 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350295 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350343 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[0].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350388 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].b                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350433 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350478 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc[1].w                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350522 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.fflayer.global_step                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350566 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.global_step                        /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350610 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias                    /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350655 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.global_step             /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350699 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale                   /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350748 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.fflayer.residual_dropout.global_step       /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350794 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.global_step                                /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350838 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.global_step         /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350882 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350927 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.350972 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b           /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351017 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj                  /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351061 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351106 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.global_step               /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351150 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351195 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b              /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351239 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj               /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351283 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b             /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351327 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.global_step                     /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351372 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias                 /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351417 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.global_step          /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351461 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale                /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351506 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.encoder_31.self_atten.residual_dropout.global_step    /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351555 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.global_step                                           /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351601 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_0                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351645 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_1                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351691 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_10                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351735 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_11                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351779 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_12                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351824 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_13                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351869 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_14                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351913 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_15                                       /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.351958 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_2                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352002 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_3                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352049 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_4                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352095 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_5                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352140 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_6                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352185 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_7                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352231 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_8                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352275 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.bias_9                                        /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352320 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.global_step                                   /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352365 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_0                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352415 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_1                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352461 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_10                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352505 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_11                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352550 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_12                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352595 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_13                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352640 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_14                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352684 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_15                                     /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352729 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_2                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352774 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_3                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352819 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_4                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352864 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_5                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352909 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_6                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352954 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_7                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.352998 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_8                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.353044 140009111922496 py_utils.py:1474] worker 0: lm.stack.cell_3.softmax.weight_9                                      /job:local/replica:0/task:0/device:CPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.353088 140009111922496 py_utils.py:1474] worker 0: lm.stack.global_step                                                  /job:local/replica:0/task:0/device:GPU:0 -> /job:local/replica:0/task:0/device:GPU:0
I0930 09:18:05.353153 140009111922496 py_utils.py:1490] ==========
I0930 09:18:07.556066 140009111922496 gpipe.py:457] cell 0 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_1:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 09:18:09.726119 140009111922496 gpipe.py:457] cell 1 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_7/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:18:11.843200 140009111922496 gpipe.py:457] cell 2 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_15/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:18:13.956880 140009111922496 gpipe.py:457] cell 3 input [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/encoder_23/add:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/GatherV2_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:18:16.212352 140009111922496 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
W0930 09:18:18.831545 140009111922496 recurrent.py:886] cell_fn contains stateful ops: [('emb/Assert/Assert', 'Assert'), ('emb/Assert_1/Assert', 'Assert'), ('encoder_0/fflayer_0/encoder_0/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_0/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_0/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_0/encoder_1/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_1/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_1/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_0/encoder_2/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_2/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_2/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_0/encoder_3/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_3/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_3/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_0/encoder_4/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_4/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_4/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_0/encoder_5/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_5/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_5/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_0/encoder_6/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_6/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_6/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_0/encoder_7/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_7/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_7/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 09:18:19.251137 140009111922496 gpipe.py:457] cell 1 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 09:18:21.071886 140009111922496 recurrent.py:886] cell_fn contains stateful ops: [('encoder_8/fflayer_0/encoder_8/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_8/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_8/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_0/encoder_9/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_9/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_9/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_0/encoder_10/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_10/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_10/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_0/encoder_11/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_11/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_11/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_0/encoder_12/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_12/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_12/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_0/encoder_13/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_13/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_13/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_0/encoder_14/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_14/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_14/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_0/encoder_15/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_15/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_15/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 09:18:21.503652 140009111922496 gpipe.py:457] cell 2 input [<tf.Tensor 'arg254:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg255:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 09:18:23.326761 140009111922496 recurrent.py:886] cell_fn contains stateful ops: [('encoder_16/fflayer_0/encoder_16/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_16/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_16/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_0/encoder_17/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_17/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_17/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_0/encoder_18/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_18/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_18/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_0/encoder_19/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_19/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_19/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_0/encoder_20/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_20/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_20/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_0/encoder_21/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_21/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_21/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_0/encoder_22/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_22/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_22/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_0/encoder_23/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_23/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_23/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 09:18:23.809225 140009111922496 gpipe.py:457] cell 3 input [<tf.Tensor 'arg286:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg287:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
W0930 09:18:25.644469 140009111922496 recurrent.py:886] cell_fn contains stateful ops: [('encoder_24/fflayer_0/encoder_24/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_24/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_24/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_0/encoder_25/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_25/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_25/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_0/encoder_26/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_26/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_26/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_0/encoder_27/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_27/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_27/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_0/encoder_28/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_28/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_28/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_0/encoder_29/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_29/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_29/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_0/encoder_30/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_30/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_30/fflayer_1/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_0/encoder_31/fflayer_0/add_CheckNumerics', 'CheckNumerics'), ('encoder_31/fflayer_0/Assert/AssertGuard/Assert', 'Assert'), ('encoder_31/fflayer_1/Assert/AssertGuard/Assert', 'Assert')]
I0930 09:18:26.501171 140009111922496 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 09:18:28.983489 140009111922496 gpipe.py:457] cell 1 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:18:31.426887 140009111922496 gpipe.py:457] cell 2 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:18:35.193103 140009111922496 gpipe.py:457] cell 3 input [<tf.Tensor 'Recv_1:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'Recv_2:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:18:37.191496 140009111922496 gpipe.py:548] pipeline output = [<tf.Tensor 'fprop/1bwds_wpm_level_lm/tower_0_0/Reshape_2:0' shape=(1024, 32, 32000) dtype=float32>]
I0930 09:18:37.195788 140009111922496 layers.py:2786] Using sparse_softmax_cross_entropy_with_logits() in SimpleFullSoftmax::_FProp2D logits_shape=[32768, 32000]
I0930 09:18:37.285321 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/total_samples/var on /job:local/replica:0/task:0/device:CPU:0 6970291216
I0930 09:18:37.287332 140009111922496 py_utils.py:1389] Creating var 1bwds_wpm_level_lm/total_samples/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:18:37.295334 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0
I0930 09:18:37.295466 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.295534 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.295593 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.295649 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.295702 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.295755 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.295807 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.295860 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.295913 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.295965 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.296022 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.296074 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.296136 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.296188 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.296239 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.296290 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.296341 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.296391 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.296442 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.296492 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.296542 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.296592 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.296642 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.296693 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.296743 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.296793 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.296843 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.296893 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.296944 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.296994 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.297044 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.297094 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.297144 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.297194 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.297243 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.297293 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.297344 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.297399 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.297450 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.297500 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.297550 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.297601 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.297652 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.297702 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.297752 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.297803 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.297853 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.297903 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.297953 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.298002 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.298052 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.298102 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.298152 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.298202 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.298252 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.298333 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.298384 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.298435 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.298485 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.298535 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.298586 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.298648 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.298700 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.298751 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.298801 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.298853 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.298903 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.298953 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.299004 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.299054 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.299104 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.299155 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.299205 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.299255 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.299305 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.299355 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.299404 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.299454 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.299504 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.299553 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.299603 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.299652 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.299701 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.299751 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.299800 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.299850 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.299904 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.299954 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.300004 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.300053 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.300102 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.300151 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.300201 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.300251 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.300301 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.300351 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.300401 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.300450 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.300500 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.300558 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.300609 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.300658 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.300708 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.300759 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.300808 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.300858 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.300909 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.300958 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.301009 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.301059 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.301109 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.301164 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.301215 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.301265 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.301316 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.301366 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.301415 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.301465 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.301514 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.301564 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.301613 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.301663 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.301713 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.301762 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.301811 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.301861 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.301911 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.301961 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.302011 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.302061 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.302111 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.302160 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.302210 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.302274 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.302330 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.302386 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.302437 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.302488 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.302537 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.302587 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.302636 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.302685 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.302735 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.302785 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.302834 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.302884 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.302934 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.302983 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.303033 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.303082 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.303132 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.303182 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.303231 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.303282 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.303331 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.303380 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.303430 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.303480 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.303530 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.303579 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.303632 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.303683 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.303733 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.303782 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.303831 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.303880 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.303928 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.303977 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.304026 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.304075 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.304125 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.304174 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.304223 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.304272 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.304321 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.304370 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.304418 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.304468 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.304517 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.304566 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.304615 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.304665 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.304714 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.304763 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.304819 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.304869 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.304918 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.304968 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.305018 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.305067 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.305116 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.305165 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.305214 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.305264 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.305313 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.305362 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.305412 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.305461 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.305511 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.305561 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.305611 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.305661 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.305710 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.305760 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.305810 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.305860 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.305909 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.305958 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.306008 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.306067 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.306118 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.306169 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.306218 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.306283 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.306338 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.306390 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.306440 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.306489 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.306539 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.306588 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.306638 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.306688 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.306737 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.306787 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.306835 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.306884 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.306933 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.306982 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.307032 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.307082 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.307132 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.307182 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.307231 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.307286 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.307336 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.307385 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.307436 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.307485 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.307535 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.307585 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.307635 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.307684 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.307734 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.307783 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.307833 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.307883 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.307933 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.307983 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.308033 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.308083 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.308133 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.308183 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.308233 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.308284 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.308334 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.308384 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.308434 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.308484 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.308539 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.308590 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.308641 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.308691 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.308740 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.308790 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.308840 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.308888 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.308938 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.308987 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.309036 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.309086 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.309135 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.309184 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.309234 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.309284 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.309334 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.309384 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.309434 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.309484 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.309533 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.309583 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.309633 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.309683 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.309736 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.309787 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.309836 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.309886 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.309935 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.309985 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.310034 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.310085 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.310135 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.310185 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.310235 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.310306 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.310358 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.310408 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.310458 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.310507 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.310556 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.310606 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.310656 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.310705 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.310754 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.310803 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.310852 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.310902 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.310957 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.311008 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.311058 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.311108 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.311157 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.311206 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.311256 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.311306 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.311356 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.311405 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.311454 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.311504 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.311553 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.311603 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.311652 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.311702 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.311751 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.311801 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.311851 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.311900 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.311950 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.312000 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.312049 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.312099 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.312149 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.312203 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.312254 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.312304 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.312355 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.312405 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.312455 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.312505 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.312555 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.312605 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.312655 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.312705 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.312755 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.312804 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.312854 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.312903 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.312953 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.313003 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.313053 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.313103 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.313152 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.313202 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.313251 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.313300 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.313349 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.313403 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.313454 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.313504 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.313554 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.313603 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.313653 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.313702 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.313752 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.313801 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.313851 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.313901 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.313951 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.314002 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.314052 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.314102 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.314151 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.314201 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.314251 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.314320 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.314372 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.314422 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.314471 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.314521 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.314570 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.314620 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.314674 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.314726 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.314776 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.314826 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.314876 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.314926 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.314975 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.315024 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.315075 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.315125 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.315175 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.315226 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.315275 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.315325 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.315374 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.315423 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.315474 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.315524 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.315575 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.315625 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.315676 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.315726 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.315775 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.315825 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.315879 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.315929 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.315979 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.316029 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.316080 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.316134 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.316185 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.316235 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.316285 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.316334 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.316384 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.316434 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.316485 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.316535 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.316584 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.316634 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.316684 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.316734 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.316784 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.316833 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.316883 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.316933 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.316982 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.317033 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.317082 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.317137 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.317188 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.317238 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.317288 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.317338 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.317389 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.317439 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.317489 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.317539 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.317589 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.317638 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.317688 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.317738 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.317788 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.317838 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.317887 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.317937 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.317987 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.318037 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.318087 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.318137 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.318188 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.318239 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.318305 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.318362 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.318414 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.318464 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.318515 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.318564 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.318613 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.318664 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.318714 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.318764 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.318814 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.318864 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.318915 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.318965 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.319015 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.319065 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.319116 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.319166 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.319215 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.319265 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.319315 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.319365 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.319414 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.319464 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.319515 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.319570 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.319620 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.319671 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.319721 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.319771 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.319821 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.319871 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.319922 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.319972 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.320022 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.320072 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.320123 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.320173 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.320223 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.320273 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.320322 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.320373 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.320422 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.320472 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.320522 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.320572 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.320622 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.320672 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.320723 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.320773 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.320828 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.320879 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.320929 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.320980 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.321030 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.321080 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.321130 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.321179 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.321231 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.321281 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.321331 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.321382 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.321433 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.321482 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.321533 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.321583 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.321634 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.321684 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.321734 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.321784 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.321834 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.321884 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.321935 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.321985 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.322039 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.322090 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0
I0930 09:18:37.322139 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0
I0930 09:18:37.322190 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0
I0930 09:18:37.322240 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0
I0930 09:18:37.322311 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0
I0930 09:18:37.322363 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0
I0930 09:18:37.322414 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0
I0930 09:18:37.322464 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0
I0930 09:18:37.322515 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0
I0930 09:18:37.322566 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0
I0930 09:18:37.322616 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0
I0930 09:18:37.322666 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0
I0930 09:18:37.322715 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0
I0930 09:18:37.322765 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0
I0930 09:18:37.322815 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0
I0930 09:18:37.322865 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0
I0930 09:18:37.322914 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0
I0930 09:18:37.322964 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0
I0930 09:18:37.323015 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0
I0930 09:18:37.323065 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0
I0930 09:18:37.323116 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0
I0930 09:18:37.323166 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0
I0930 09:18:37.323216 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0
I0930 09:18:37.323266 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0
I0930 09:18:37.323317 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0
I0930 09:18:37.323367 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0
I0930 09:18:37.323422 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0
I0930 09:18:37.323473 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0
I0930 09:18:37.323523 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0
I0930 09:18:37.323572 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0
I0930 09:18:37.323623 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0
I0930 09:18:37.323673 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0
I0930 09:18:37.323722 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0
I0930 09:18:37.323773 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0
I0930 09:18:37.323823 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0
I0930 09:18:37.323873 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0
I0930 09:18:37.323924 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0
I0930 09:18:37.323974 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0
I0930 09:18:37.324024 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0
I0930 09:18:37.324074 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0
I0930 09:18:37.324123 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0
I0930 09:18:37.324173 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0
I0930 09:18:37.324223 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0
I0930 09:18:37.324274 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0
I0930 09:18:37.324324 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0
I0930 09:18:37.324375 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0
I0930 09:18:37.324425 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0
I0930 09:18:37.324476 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0
I0930 09:18:37.324526 140009111922496 learner.py:161] loss: bprop variable: 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0
I0930 09:18:41.814231 140009111922496 gpipe.py:457] cell 3 input [<tf.Tensor 'arg287:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg288:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:18:48.276895 140009111922496 gpipe.py:457] cell 2 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:18:53.163005 140009111922496 gpipe.py:457] cell 1 input [<tf.Tensor 'arg255:0' shape=(1024, 1, 2048) dtype=float32>, <tf.Tensor 'arg256:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None]
I0930 09:18:59.515167 140009111922496 gpipe.py:457] cell 0 input [<tf.Tensor 'arg259:0' shape=(1024, 1) dtype=int32>, <tf.Tensor 'arg260:0' shape=(1024, 1) dtype=float32>, None, None, None, None, None, None, None, None]
I0930 09:19:09.353191 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.emb.src_token_emb.wm: <tf.Variable '1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var:0' shape=(32000, 2048) dtype=float32_ref>
I0930 09:19:09.353408 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.353492 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.353568 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.353635 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.353700 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.353762 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.353823 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.353884 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.353949 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.354007 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.354071 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.354131 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.354194 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.354253 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.354340 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.354407 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.354468 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_0.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.354529 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.354589 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.354653 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.354713 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.354776 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.354835 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.354895 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.354955 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.355018 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.355078 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.355141 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.355200 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.355267 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.355328 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.355389 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.355449 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.355508 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_1.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.355568 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.355628 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.355691 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.355751 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.355814 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.355873 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.355932 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.355991 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.356055 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.356116 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.356183 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.356245 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.356308 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.356367 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.356431 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.356491 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.356549 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_2.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.356610 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.356669 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.356731 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.356790 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.356853 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.356913 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.356971 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.357031 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.357100 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.357161 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.357224 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.357283 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.357345 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.357404 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.357467 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.357527 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.357586 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_3.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.357645 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.357705 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.357767 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.357827 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.357888 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.357948 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.358012 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.358073 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.358137 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.358197 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.358274 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.358340 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.358405 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.358464 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.358527 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.358586 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.358645 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_4.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.358704 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.358763 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.358825 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.358884 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.358952 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.359013 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.359072 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.359131 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.359193 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.359252 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.359314 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.359373 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.359434 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.359493 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.359555 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.359613 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.359672 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_5.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.359731 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.359794 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.359857 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.359916 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.359983 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.360044 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.360104 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.360163 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.360226 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.360285 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.360347 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.360407 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.360469 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.360528 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.360591 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.360651 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.360714 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_6.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.360774 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.360834 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.360896 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.360955 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.361017 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.361076 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.361135 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.361193 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.361255 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.361314 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.361376 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.361435 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.361497 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.361555 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.361621 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.361681 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.361739 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_0.encoder_7.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.361797 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.361857 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.361918 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.361977 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.362039 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.362098 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.362158 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.362217 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.362302 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.362365 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.362428 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.362493 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.362556 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.362615 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.362678 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.362737 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.362796 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_10.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.362856 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.362915 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.362976 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.363036 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.363099 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.363160 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.363219 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.363278 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.363341 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.363405 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.363470 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.363529 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.363592 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.363651 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.363713 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.363772 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.363831 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_11.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.363889 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.363948 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.364011 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.364100 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.364195 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.364269 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.364329 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.364396 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.364461 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.364521 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.364583 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.364643 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.364705 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.364764 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.364826 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.364886 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.364945 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_12.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.365003 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.365061 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.365126 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.365185 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.365251 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.365312 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.365371 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.365430 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.365493 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.365560 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.365625 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.365685 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.365748 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.365806 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.365868 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.365927 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.365986 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_13.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.366044 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.366104 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.366171 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.366231 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.366313 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.366375 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.366435 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.366494 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.366557 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.366617 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.366680 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.366739 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.366802 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.366861 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.366923 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.366983 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.367046 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_14.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.367105 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.367164 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.367227 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.367286 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.367348 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.367407 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.367466 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.367525 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.367589 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.367648 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.367710 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.367769 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.367832 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.367891 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.367957 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.368017 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.368076 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_15.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.368134 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.368194 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.368256 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.368315 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.368376 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.368436 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.368495 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.368555 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.368617 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.368676 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.368738 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.368797 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.368863 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.368923 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.368984 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.369042 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.369101 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_8.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.369159 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.369218 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.369279 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.369338 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.369400 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.369458 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.369516 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.369575 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.369638 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.369698 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.369771 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.369832 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.369894 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.369953 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.370020 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.370079 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.370138 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_1.encoder_9.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.370196 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.370268 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.370340 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.370400 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.370464 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.370523 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.370582 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.370646 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.370712 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.370771 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.370834 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.370894 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.370957 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.371016 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.371078 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.371138 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.371196 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_16.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.371255 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.371315 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.371377 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.371436 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.371499 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.371561 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.371621 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.371680 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.371743 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.371802 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.371866 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.371925 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.371987 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.372047 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.372109 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.372169 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.372227 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_17.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.372287 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.372347 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.372410 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.372473 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.372537 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.372596 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.372655 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.372714 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.372778 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.372838 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.372901 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.372961 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.373022 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.373081 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.373143 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.373203 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.373261 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_18.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.373323 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.373383 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.373445 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.373503 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.373566 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.373625 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.373684 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.373743 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.373807 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.373866 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.373928 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.373988 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.374051 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.374110 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.374171 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.374234 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.374319 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_19.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.374380 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.374439 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.374502 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.374562 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.374624 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.374682 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.374741 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.374800 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.374864 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.374923 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.374987 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.375045 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.375113 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.375173 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.375235 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.375294 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.375353 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_20.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.375411 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.375470 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.375533 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.375592 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.375653 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.375713 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.375772 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.375832 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.375895 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.375955 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.376021 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.376081 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.376144 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.376204 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.376266 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.376326 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.376385 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_21.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.376443 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.376503 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.376565 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.376624 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.376686 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.376745 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.376804 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.376862 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.376931 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.376991 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.377053 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.377112 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.377175 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.377235 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.377296 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.377355 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.377414 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_22.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.377473 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.377532 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.377593 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.377652 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.377714 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.377773 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.377835 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.377896 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.377959 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.378018 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.378079 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.378138 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.378200 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.378275 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.378344 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.378404 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.378463 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_2.encoder_23.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.378522 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.378581 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.378642 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.378706 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.378769 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.378829 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.378887 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.378947 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.379009 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.379069 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.379132 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.379191 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.379253 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.379311 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.379373 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.379432 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.379491 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_24.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.379550 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.379614 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.379677 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.379737 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.379799 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.379858 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.379917 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.379976 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.380043 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.380105 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.380167 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.380227 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.380289 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.380348 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.380409 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.380471 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.380531 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_25.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.380589 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.380648 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.380710 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.380770 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.380831 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.380890 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.380949 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.381008 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.381071 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.381129 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.381192 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.381251 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.381314 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.381377 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.381439 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.381498 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.381557 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_26.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.381616 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.381676 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.381739 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.381799 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.381861 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.381920 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.381979 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.382037 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.382100 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.382160 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.382223 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.382303 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.382369 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.382429 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.382490 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.382549 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.382608 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_27.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.382667 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.382726 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.382788 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.382848 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.382910 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.382969 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.383027 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.383085 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.383152 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.383213 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.383275 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.383335 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.383397 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.383456 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.383518 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.383577 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.383635 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_28.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.383693 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.383751 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.383813 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.383872 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.383932 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.383991 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.384053 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.384113 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.384175 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.384235 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.384297 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.384356 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.384418 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.384477 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.384539 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.384598 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.384656 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_29.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.384715 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.384775 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.384838 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.384899 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.384968 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.385029 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.385088 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.385147 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.385211 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.385270 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.385331 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.385390 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.385452 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.385512 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.385573 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.385632 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.385691 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_30.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.385749 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var:0' shape=(8192,) dtype=float32_ref>
I0930 09:19:09.385813 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_0.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var:0' shape=(2048, 8192) dtype=float32_ref>
I0930 09:19:09.385875 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.385934 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.fflayer.fc_1.w: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var:0' shape=(8192, 2048) dtype=float32_ref>
I0930 09:19:09.385996 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.386055 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.fflayer.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.386114 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.atten.per_dim_scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var:0' shape=(128,) dtype=float32_ref>
I0930 09:19:09.386172 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.386234 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_post_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.386317 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.386382 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.ctx_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.386442 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.386506 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.query_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.386564 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var:0' shape=(2048, 2048) dtype=float32_ref>
I0930 09:19:09.386626 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.atten.source_proj_b: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.386685 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.bias: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.386749 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.encoder_31.self_atten.layer_norm.scale: <tf.Variable '1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var:0' shape=(2048,) dtype=float32_ref>
I0930 09:19:09.386808 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.386868 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.386927 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.386986 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387044 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387101 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387159 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387217 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387275 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387334 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387392 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387449 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387506 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387565 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387621 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387679 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.bias_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var:0' shape=(2000,) dtype=float32_ref>
I0930 09:19:09.387736 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_0: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.387803 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_1: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.387866 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_10: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.387928 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_11: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.387989 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_12: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388051 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_13: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388112 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_14: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388175 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_15: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388236 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_2: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388298 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_3: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388360 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_4: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388422 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_5: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388484 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_6: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388547 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_7: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388608 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_8: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:09.388670 140009111922496 py_utils.py:1982] AdjustGradientsWithLpLoss: lm.stack.cell_3.softmax.weight_9: <tf.Variable '1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var:0' shape=(2048, 2000) dtype=float32_ref>
I0930 09:19:18.647808 140009111922496 learner.py:279] gradient_adjuster=<bound method LanguageModel.AdjustGradients of <lingvo.tasks.lm.model.FixedShapeInputLanguageModel object at 0x7f54fb1a5780>>
I0930 09:19:22.190230 140009111922496 cluster.py:515] Place variable beta1_power on /job:local/replica:0/task:0/device:CPU:0 6970291220
I0930 09:19:22.193259 140009111922496 cluster.py:515] Place variable beta2_power on /job:local/replica:0/task:0/device:CPU:0 6970291224
I0930 09:19:22.197942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7232435224
I0930 09:19:22.203081 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494579224
I0930 09:19:22.208106 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7494611992
I0930 09:19:22.213001 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7494644760
I0930 09:19:22.218064 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7561753624
I0930 09:19:22.222983 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628862488
I0930 09:19:22.228000 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7628870680
I0930 09:19:22.232889 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7628878872
I0930 09:19:22.237902 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7695987736
I0930 09:19:22.242834 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763096600
I0930 09:19:22.247867 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763104792
I0930 09:19:22.252790 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763112984
I0930 09:19:22.258464 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763121176
I0930 09:19:22.263441 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763129368
I0930 09:19:22.267186 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7763129880
I0930 09:19:22.270850 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7763130392
I0930 09:19:22.275797 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7779907608
I0930 09:19:22.280704 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796684824
I0930 09:19:22.302195 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7796693016
I0930 09:19:22.307236 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7796701208
I0930 09:19:22.312274 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7813478424
I0930 09:19:22.317289 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830255640
I0930 09:19:22.322202 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7830263832
I0930 09:19:22.327203 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7830272024
I0930 09:19:22.332076 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7847049240
I0930 09:19:22.337089 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863826456
I0930 09:19:22.341995 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7863834648
I0930 09:19:22.347034 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7863842840
I0930 09:19:22.351933 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7880620056
I0930 09:19:22.356930 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897397272
I0930 09:19:22.361833 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897405464
I0930 09:19:22.366939 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897413656
I0930 09:19:22.371982 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897421848
I0930 09:19:22.376912 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897430040
I0930 09:19:22.382439 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897438232
I0930 09:19:22.387330 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897446424
I0930 09:19:22.392319 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7897479192
I0930 09:19:22.397297 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 7897511960
I0930 09:19:22.402292 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 7964620824
I0930 09:19:22.407214 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031729688
I0930 09:19:22.412199 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8031737880
I0930 09:19:22.417083 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8031746072
I0930 09:19:22.422107 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8098854936
I0930 09:19:22.427040 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165963800
I0930 09:19:22.432016 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165971992
I0930 09:19:22.437036 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165980184
I0930 09:19:22.441922 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165988376
I0930 09:19:22.446953 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165996568
I0930 09:19:22.450689 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8165997080
I0930 09:19:22.454357 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8165997592
I0930 09:19:22.459258 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8182774808
I0930 09:19:22.464160 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199552024
I0930 09:19:22.469154 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8199560216
I0930 09:19:22.474159 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8199568408
I0930 09:19:22.479100 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8216345624
I0930 09:19:22.484116 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233122840
I0930 09:19:22.489006 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8233131032
I0930 09:19:22.494528 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8233139224
I0930 09:19:22.499468 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8249916440
I0930 09:19:22.504504 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266693656
I0930 09:19:22.509416 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8266701848
I0930 09:19:22.514494 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8266710040
I0930 09:19:22.519443 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8283487256
I0930 09:19:22.524537 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300264472
I0930 09:19:22.529632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300272664
I0930 09:19:22.534604 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300280856
I0930 09:19:22.539640 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300289048
I0930 09:19:22.544625 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300297240
I0930 09:19:22.549724 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300305432
I0930 09:19:22.554680 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300313624
I0930 09:19:22.559702 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8300346392
I0930 09:19:22.564603 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8300379160
I0930 09:19:22.569633 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8367488024
I0930 09:19:22.574722 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434596888
I0930 09:19:22.579753 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8434605080
I0930 09:19:22.584825 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8434613272
I0930 09:19:22.589840 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8501722136
I0930 09:19:22.594928 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568831000
I0930 09:19:22.599838 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568839192
I0930 09:19:22.605372 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568847384
I0930 09:19:22.610333 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568855576
I0930 09:19:22.615437 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568863768
I0930 09:19:22.619338 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8568864280
I0930 09:19:22.623011 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8568864792
I0930 09:19:22.627998 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8585642008
I0930 09:19:22.633034 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602419224
I0930 09:19:22.637983 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8602427416
I0930 09:19:22.643038 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8602435608
I0930 09:19:22.647961 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8619212824
I0930 09:19:22.652993 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8635990040
I0930 09:19:22.657900 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8635998232
I0930 09:19:22.662941 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8636006424
I0930 09:19:22.667881 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8652783640
I0930 09:19:22.672919 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669560856
I0930 09:19:22.677848 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8669569048
I0930 09:19:22.682905 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8669577240
I0930 09:19:22.687931 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8686354456
I0930 09:19:22.692877 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703131672
I0930 09:19:22.697925 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703139864
I0930 09:19:22.702874 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703148056
I0930 09:19:22.707965 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703156248
I0930 09:19:22.712947 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703164440
I0930 09:19:22.718534 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703172632
I0930 09:19:22.723457 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703180824
I0930 09:19:22.728491 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8703213592
I0930 09:19:22.733449 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8703246360
I0930 09:19:22.738501 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8770355224
I0930 09:19:22.743436 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837464088
I0930 09:19:22.748447 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8837472280
I0930 09:19:22.753484 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8837480472
I0930 09:19:22.758456 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8904589336
I0930 09:19:22.763523 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971698200
I0930 09:19:22.768438 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971706392
I0930 09:19:22.773574 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971714584
I0930 09:19:22.778547 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971722776
I0930 09:19:22.783603 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971730968
I0930 09:19:22.787376 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8971731480
I0930 09:19:22.791077 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 8971731992
I0930 09:19:22.796026 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 8988509208
I0930 09:19:22.801079 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005286424
I0930 09:19:22.806004 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9005294616
I0930 09:19:22.811089 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9005302808
I0930 09:19:22.816063 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9022080024
I0930 09:19:22.821137 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038857240
I0930 09:19:22.826083 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9038865432
I0930 09:19:22.831678 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9038873624
I0930 09:19:22.836631 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9055650840
I0930 09:19:22.841702 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072428056
I0930 09:19:22.846754 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9072436248
I0930 09:19:22.851812 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9072444440
I0930 09:19:22.856888 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9089221656
I0930 09:19:22.861904 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9105998872
I0930 09:19:22.866986 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106007064
I0930 09:19:22.871908 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106015256
I0930 09:19:22.876920 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106023448
I0930 09:19:22.881861 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106031640
I0930 09:19:22.886910 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106039832
I0930 09:19:22.891849 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106048024
I0930 09:19:22.896959 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9106080792
I0930 09:19:22.901889 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9106113560
I0930 09:19:22.906930 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9173222424
I0930 09:19:22.911905 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240331288
I0930 09:19:22.916933 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9240339480
I0930 09:19:22.921978 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9240347672
I0930 09:19:22.926955 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9307456536
I0930 09:19:22.931982 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374565400
I0930 09:19:22.936916 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374573592
I0930 09:19:22.942480 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374581784
I0930 09:19:22.947422 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374589976
I0930 09:19:22.952449 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374598168
I0930 09:19:22.956249 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9374598680
I0930 09:19:22.959942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9374599192
I0930 09:19:22.964934 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9391376408
I0930 09:19:22.969950 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408153624
I0930 09:19:22.975008 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9408161816
I0930 09:19:22.980057 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9408170008
I0930 09:19:22.985003 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9424947224
I0930 09:19:22.990053 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441724440
I0930 09:19:22.995024 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9441732632
I0930 09:19:23.000061 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9441740824
I0930 09:19:23.004995 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9458518040
I0930 09:19:23.010033 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475295256
I0930 09:19:23.015061 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9475303448
I0930 09:19:23.020090 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9475311640
I0930 09:19:23.025151 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9492088856
I0930 09:19:23.030117 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508866072
I0930 09:19:23.035193 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508874264
I0930 09:19:23.040214 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508882456
I0930 09:19:23.045242 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508890648
I0930 09:19:23.050219 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508898840
I0930 09:19:23.055852 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508907032
I0930 09:19:23.061005 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508915224
I0930 09:19:23.066065 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9508947992
I0930 09:19:23.071050 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9508980760
I0930 09:19:23.076085 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9576089624
I0930 09:19:23.081037 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643198488
I0930 09:19:23.086054 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9643206680
I0930 09:19:23.091156 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9643214872
I0930 09:19:23.096079 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9710323736
I0930 09:19:23.101166 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777432600
I0930 09:19:23.106103 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777440792
I0930 09:19:23.111143 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777448984
I0930 09:19:23.116093 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777457176
I0930 09:19:23.121147 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777465368
I0930 09:19:23.124935 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9777465880
I0930 09:19:23.128624 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9777466392
I0930 09:19:23.133621 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9794243608
I0930 09:19:23.138699 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811020824
I0930 09:19:23.143688 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9811029016
I0930 09:19:23.148783 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9811037208
I0930 09:19:23.153774 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9827814424
I0930 09:19:23.158870 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844591640
I0930 09:19:23.163880 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9844599832
I0930 09:19:23.169471 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9844608024
I0930 09:19:23.174566 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9861385240
I0930 09:19:23.179621 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878162456
I0930 09:19:23.184563 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9878170648
I0930 09:19:23.189637 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9878178840
I0930 09:19:23.194721 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9894956056
I0930 09:19:23.199701 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911733272
I0930 09:19:23.204758 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911741464
I0930 09:19:23.209709 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911749656
I0930 09:19:23.214759 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911757848
I0930 09:19:23.219795 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911766040
I0930 09:19:23.224836 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911774232
I0930 09:19:23.229792 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911782424
I0930 09:19:23.234873 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9911815192
I0930 09:19:23.239832 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 9911847960
I0930 09:19:23.244887 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 9978956824
I0930 09:19:23.249871 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046065688
I0930 09:19:23.254946 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10046073880
I0930 09:19:23.259984 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10046082072
I0930 09:19:23.264942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10113190936
I0930 09:19:23.269997 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180299800
I0930 09:19:23.275000 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180307992
I0930 09:19:23.280584 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180316184
I0930 09:19:23.285546 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180324376
I0930 09:19:23.290604 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180332568
I0930 09:19:23.294473 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10180333080
I0930 09:19:23.298151 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10180333592
I0930 09:19:23.303136 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10197110808
I0930 09:19:23.308211 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213888024
I0930 09:19:23.313193 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10213896216
I0930 09:19:23.318254 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10213904408
I0930 09:19:23.323252 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10230681624
I0930 09:19:23.328303 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247458840
I0930 09:19:23.333266 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10247467032
I0930 09:19:23.338340 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10247475224
I0930 09:19:23.343317 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10264252440
I0930 09:19:23.348386 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281029656
I0930 09:19:23.353356 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10281037848
I0930 09:19:23.358417 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10281046040
I0930 09:19:23.363473 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10297823256
I0930 09:19:23.368552 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314600472
I0930 09:19:23.373689 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314608664
I0930 09:19:23.378689 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314616856
I0930 09:19:23.383755 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314625048
I0930 09:19:23.388715 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314633240
I0930 09:19:23.394376 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314641432
I0930 09:19:23.399344 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314649624
I0930 09:19:23.404438 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10314682392
I0930 09:19:23.409412 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10314715160
I0930 09:19:23.414563 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10381824024
I0930 09:19:23.419592 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448932888
I0930 09:19:23.424623 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10448941080
I0930 09:19:23.429684 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10448949272
I0930 09:19:23.434668 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10516058136
I0930 09:19:23.439761 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583167000
I0930 09:19:23.444772 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583175192
I0930 09:19:23.449826 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583183384
I0930 09:19:23.454818 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583191576
I0930 09:19:23.459866 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583199768
I0930 09:19:23.463697 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10583200280
I0930 09:19:23.467383 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10583200792
I0930 09:19:23.472374 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10599978008
I0930 09:19:23.477438 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616755224
I0930 09:19:23.482408 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10616763416
I0930 09:19:23.487465 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10616771608
I0930 09:19:23.492431 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10633548824
I0930 09:19:23.497545 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650326040
I0930 09:19:23.502518 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10650334232
I0930 09:19:23.508110 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10650342424
I0930 09:19:23.513094 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10667119640
I0930 09:19:23.518158 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683896856
I0930 09:19:23.523200 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10683905048
I0930 09:19:23.528254 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10683913240
I0930 09:19:23.533392 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10700690456
I0930 09:19:23.538419 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717467672
I0930 09:19:23.543483 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717475864
I0930 09:19:23.548521 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717484056
I0930 09:19:23.553568 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717492248
I0930 09:19:23.558571 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717500440
I0930 09:19:23.563638 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717508632
I0930 09:19:23.568581 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717516824
I0930 09:19:23.573841 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10717549592
I0930 09:19:23.578829 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10717582360
I0930 09:19:23.583897 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10784691224
I0930 09:19:23.588857 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851800088
I0930 09:19:23.593901 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10851808280
I0930 09:19:23.598996 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10851816472
I0930 09:19:23.603952 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10918925336
I0930 09:19:23.609036 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986034200
I0930 09:19:23.614035 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986042392
I0930 09:19:23.619926 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986050584
I0930 09:19:23.625279 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986058776
I0930 09:19:23.630809 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986066968
I0930 09:19:23.634981 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 10986067480
I0930 09:19:23.638895 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 10986067992
I0930 09:19:23.644175 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11002845208
I0930 09:19:23.649509 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019622424
I0930 09:19:23.654801 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11019630616
I0930 09:19:23.660144 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11019638808
I0930 09:19:23.665298 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11036416024
I0930 09:19:23.670459 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053193240
I0930 09:19:23.675443 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11053201432
I0930 09:19:23.680531 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11053209624
I0930 09:19:23.685528 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11069986840
I0930 09:19:23.690621 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086764056
I0930 09:19:23.695672 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11086772248
I0930 09:19:23.700704 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11086780440
I0930 09:19:23.705847 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11103557656
I0930 09:19:23.710864 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120334872
I0930 09:19:23.715951 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120343064
I0930 09:19:23.720947 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120351256
I0930 09:19:23.726010 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120359448
I0930 09:19:23.731125 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120367640
I0930 09:19:23.736761 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120375832
I0930 09:19:23.741777 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120384024
I0930 09:19:23.746925 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11120416792
I0930 09:19:23.751920 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11120449560
I0930 09:19:23.757050 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11187558424
I0930 09:19:23.762067 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254667288
I0930 09:19:23.767205 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11254675480
I0930 09:19:23.772279 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11254683672
I0930 09:19:23.777478 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11321792536
I0930 09:19:23.782854 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388901400
I0930 09:19:23.787891 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388909592
I0930 09:19:23.793012 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388917784
I0930 09:19:23.797997 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388925976
I0930 09:19:23.803075 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388934168
I0930 09:19:23.806920 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11388934680
I0930 09:19:23.810650 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11388935192
I0930 09:19:23.815631 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11405712408
I0930 09:19:23.820734 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422489624
I0930 09:19:23.825710 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11422497816
I0930 09:19:23.830832 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11422506008
I0930 09:19:23.835804 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11439283224
I0930 09:19:23.840933 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456060440
I0930 09:19:23.845922 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11456068632
I0930 09:19:23.851526 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11456076824
I0930 09:19:23.856516 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11472854040
I0930 09:19:23.861593 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489631256
I0930 09:19:23.866588 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11489639448
I0930 09:19:23.871637 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11489647640
I0930 09:19:23.876696 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11506424856
I0930 09:19:23.881727 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523202072
I0930 09:19:23.886815 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523210264
I0930 09:19:23.891779 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523218456
I0930 09:19:23.896990 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523226648
I0930 09:19:23.901980 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523234840
I0930 09:19:23.907096 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523243032
I0930 09:19:23.912074 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523251224
I0930 09:19:23.917173 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11523283992
I0930 09:19:23.922154 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11523316760
I0930 09:19:23.927221 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11590425624
I0930 09:19:23.932207 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657534488
I0930 09:19:23.937270 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11657542680
I0930 09:19:23.942393 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11657550872
I0930 09:19:23.947349 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11724659736
I0930 09:19:23.952432 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791768600
I0930 09:19:23.957424 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791776792
I0930 09:19:23.963003 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791784984
I0930 09:19:23.967994 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791793176
I0930 09:19:23.973132 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791801368
I0930 09:19:23.976949 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11791801880
I0930 09:19:23.980672 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11791802392
I0930 09:19:23.985673 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11808579608
I0930 09:19:23.990790 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825356824
I0930 09:19:23.995759 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11825365016
I0930 09:19:24.000827 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11825373208
I0930 09:19:24.005822 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11842150424
I0930 09:19:24.010982 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858927640
I0930 09:19:24.015963 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11858935832
I0930 09:19:24.021045 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11858944024
I0930 09:19:24.026051 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11875721240
I0930 09:19:24.031213 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892498456
I0930 09:19:24.036244 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11892506648
I0930 09:19:24.041319 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11892514840
I0930 09:19:24.046432 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11909292056
I0930 09:19:24.051437 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926069272
I0930 09:19:24.056538 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926077464
I0930 09:19:24.061536 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926085656
I0930 09:19:24.066639 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926093848
I0930 09:19:24.071649 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926102040
I0930 09:19:24.077224 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926110232
I0930 09:19:24.082200 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926118424
I0930 09:19:24.087328 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11926151192
I0930 09:19:24.092310 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 11926183960
I0930 09:19:24.097379 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 11993292824
I0930 09:19:24.102416 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060401688
I0930 09:19:24.107528 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12060409880
I0930 09:19:24.112646 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12060418072
I0930 09:19:24.117662 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12127526936
I0930 09:19:24.122814 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194635800
I0930 09:19:24.127855 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194643992
I0930 09:19:24.133037 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194652184
I0930 09:19:24.138085 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194660376
I0930 09:19:24.143205 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194668568
I0930 09:19:24.147049 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12194669080
I0930 09:19:24.150796 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12194669592
I0930 09:19:24.155815 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12211446808
I0930 09:19:24.160922 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228224024
I0930 09:19:24.165945 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12228232216
I0930 09:19:24.171076 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12228240408
I0930 09:19:24.176242 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12245017624
I0930 09:19:24.181402 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261794840
I0930 09:19:24.186468 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12261803032
I0930 09:19:24.192102 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12261811224
I0930 09:19:24.197133 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12278588440
I0930 09:19:24.202218 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295365656
I0930 09:19:24.207229 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12295373848
I0930 09:19:24.212299 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12295382040
I0930 09:19:24.217442 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12312159256
I0930 09:19:24.222467 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328936472
I0930 09:19:24.227531 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328944664
I0930 09:19:24.232534 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328952856
I0930 09:19:24.237607 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328961048
I0930 09:19:24.242714 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328969240
I0930 09:19:24.247801 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12328977432
I0930 09:19:24.252774 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12328985624
I0930 09:19:24.257851 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12329018392
I0930 09:19:24.262875 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12329051160
I0930 09:19:24.267942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12396160024
I0930 09:19:24.272953 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463268888
I0930 09:19:24.278025 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12463277080
I0930 09:19:24.283126 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12463285272
I0930 09:19:24.288122 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12530394136
I0930 09:19:24.293248 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597503000
I0930 09:19:24.298235 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597511192
I0930 09:19:24.303853 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597519384
I0930 09:19:24.308856 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597527576
I0930 09:19:24.313952 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597535768
I0930 09:19:24.317795 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12597536280
I0930 09:19:24.321541 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12597536792
I0930 09:19:24.326572 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12614314008
I0930 09:19:24.331690 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631091224
I0930 09:19:24.336655 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12631099416
I0930 09:19:24.341757 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12631107608
I0930 09:19:24.346756 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12647884824
I0930 09:19:24.351836 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664662040
I0930 09:19:24.356832 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12664670232
I0930 09:19:24.361934 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12664678424
I0930 09:19:24.367005 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12681455640
I0930 09:19:24.372099 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698232856
I0930 09:19:24.377171 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12698241048
I0930 09:19:24.382278 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12698249240
I0930 09:19:24.387369 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12715026456
I0930 09:19:24.392383 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731803672
I0930 09:19:24.397485 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731811864
I0930 09:19:24.402493 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731820056
I0930 09:19:24.407577 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731828248
I0930 09:19:24.412605 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731836440
I0930 09:19:24.418240 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731844632
I0930 09:19:24.423263 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731852824
I0930 09:19:24.428403 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12731885592
I0930 09:19:24.433398 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12731918360
I0930 09:19:24.438505 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12799027224
I0930 09:19:24.443495 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866136088
I0930 09:19:24.448567 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12866144280
I0930 09:19:24.453665 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 12866152472
I0930 09:19:24.458685 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 12933261336
I0930 09:19:24.463776 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000370200
I0930 09:19:24.468804 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000378392
I0930 09:19:24.473906 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000386584
I0930 09:19:24.478945 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000394776
I0930 09:19:24.484030 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000402968
I0930 09:19:24.487884 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13000403480
I0930 09:19:24.491618 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13000403992
I0930 09:19:24.496699 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13017181208
I0930 09:19:24.501816 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033958424
I0930 09:19:24.506840 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13033966616
I0930 09:19:24.511946 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13033974808
I0930 09:19:24.516950 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13050752024
I0930 09:19:24.522072 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067529240
I0930 09:19:24.527109 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13067537432
I0930 09:19:24.532800 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13067545624
I0930 09:19:24.537796 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13084322840
I0930 09:19:24.542932 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101100056
I0930 09:19:24.548057 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13101108248
I0930 09:19:24.553138 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13101116440
I0930 09:19:24.558303 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13117893656
I0930 09:19:24.563325 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134670872
I0930 09:19:24.568428 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134679064
I0930 09:19:24.573596 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134687256
I0930 09:19:24.578720 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134695448
I0930 09:19:24.583748 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134703640
I0930 09:19:24.588840 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134711832
I0930 09:19:24.593849 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134720024
I0930 09:19:24.598976 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13134752792
I0930 09:19:24.603987 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13134785560
I0930 09:19:24.609060 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13201894424
I0930 09:19:24.614064 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269003288
I0930 09:19:24.619183 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13269011480
I0930 09:19:24.624291 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13269019672
I0930 09:19:24.629288 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13336128536
I0930 09:19:24.634415 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403237400
I0930 09:19:24.639433 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403245592
I0930 09:19:24.645004 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403253784
I0930 09:19:24.650004 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403261976
I0930 09:19:24.655153 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403270168
I0930 09:19:24.658988 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13403270680
I0930 09:19:24.662776 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13403271192
I0930 09:19:24.667806 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13420048408
I0930 09:19:24.672907 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436825624
I0930 09:19:24.677950 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13436833816
I0930 09:19:24.683068 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13436842008
I0930 09:19:24.688082 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13453619224
I0930 09:19:24.693324 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470396440
I0930 09:19:24.698436 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13470404632
I0930 09:19:24.703656 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13470412824
I0930 09:19:24.708730 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13487190040
I0930 09:19:24.713865 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503967256
I0930 09:19:24.718917 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13503975448
I0930 09:19:24.724059 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13503983640
I0930 09:19:24.729161 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13520760856
I0930 09:19:24.734209 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537538072
I0930 09:19:24.739371 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537546264
I0930 09:19:24.744385 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537554456
I0930 09:19:24.749512 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537562648
I0930 09:19:24.754602 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537570840
I0930 09:19:24.760252 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537579032
I0930 09:19:24.765241 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537587224
I0930 09:19:24.770362 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13537619992
I0930 09:19:24.775463 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13537652760
I0930 09:19:24.780551 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13604761624
I0930 09:19:24.785575 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671870488
I0930 09:19:24.790702 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13671878680
I0930 09:19:24.795814 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13671886872
I0930 09:19:24.800836 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13738995736
I0930 09:19:24.805940 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806104600
I0930 09:19:24.810989 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806112792
I0930 09:19:24.816096 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806120984
I0930 09:19:24.821156 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806129176
I0930 09:19:24.826496 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806137368
I0930 09:19:24.830615 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13806137880
I0930 09:19:24.834457 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13806138392
I0930 09:19:24.839601 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13822915608
I0930 09:19:24.844878 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839692824
I0930 09:19:24.850017 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13839701016
I0930 09:19:24.855283 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13839709208
I0930 09:19:24.860392 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13856486424
I0930 09:19:24.865627 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873263640
I0930 09:19:24.870768 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13873271832
I0930 09:19:24.876418 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13873280024
I0930 09:19:24.881501 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13890057240
I0930 09:19:24.886632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906834456
I0930 09:19:24.891665 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13906842648
I0930 09:19:24.896842 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13906850840
I0930 09:19:24.901948 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13923628056
I0930 09:19:24.907105 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940405272
I0930 09:19:24.912207 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940413464
I0930 09:19:24.917207 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940421656
I0930 09:19:24.922361 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940429848
I0930 09:19:24.927392 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940438040
I0930 09:19:24.932519 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940446232
I0930 09:19:24.937503 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940454424
I0930 09:19:24.942675 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 13940487192
I0930 09:19:24.947682 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 13940519960
I0930 09:19:24.952762 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14007628824
I0930 09:19:24.957801 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074737688
I0930 09:19:24.962952 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14074745880
I0930 09:19:24.968049 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14074754072
I0930 09:19:24.973081 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14141862936
I0930 09:19:24.978310 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208971800
I0930 09:19:24.983316 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208979992
I0930 09:19:24.988933 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14208988184
I0930 09:19:24.993958 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14208996376
I0930 09:19:24.999121 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209004568
I0930 09:19:25.002990 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14209005080
I0930 09:19:25.006765 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14209005592
I0930 09:19:25.011799 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14225782808
I0930 09:19:25.016934 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242560024
I0930 09:19:25.021980 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14242568216
I0930 09:19:25.027150 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14242576408
I0930 09:19:25.032157 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14259353624
I0930 09:19:25.037315 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276130840
I0930 09:19:25.042358 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14276139032
I0930 09:19:25.047487 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14276147224
I0930 09:19:25.052525 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14292924440
I0930 09:19:25.057661 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309701656
I0930 09:19:25.062754 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14309709848
I0930 09:19:25.067960 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14309718040
I0930 09:19:25.073111 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14326495256
I0930 09:19:25.078149 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343272472
I0930 09:19:25.083319 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343280664
I0930 09:19:25.088396 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343288856
I0930 09:19:25.093508 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343297048
I0930 09:19:25.098596 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343305240
I0930 09:19:25.104206 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343313432
I0930 09:19:25.109239 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343321624
I0930 09:19:25.114396 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14343354392
I0930 09:19:25.119417 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14343387160
I0930 09:19:25.124535 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14410496024
I0930 09:19:25.129552 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477604888
I0930 09:19:25.134693 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14477613080
I0930 09:19:25.139805 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14477621272
I0930 09:19:25.144823 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14544730136
I0930 09:19:25.149936 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611839000
I0930 09:19:25.154988 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611847192
I0930 09:19:25.160122 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611855384
I0930 09:19:25.165156 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611863576
I0930 09:19:25.170307 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611871768
I0930 09:19:25.174301 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14611872280
I0930 09:19:25.178058 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14611872792
I0930 09:19:25.183109 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14628650008
I0930 09:19:25.188256 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645427224
I0930 09:19:25.193278 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14645435416
I0930 09:19:25.198411 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14645443608
I0930 09:19:25.203462 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14662220824
I0930 09:19:25.208582 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14678998040
I0930 09:19:25.213594 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14679006232
I0930 09:19:25.219222 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14679014424
I0930 09:19:25.224264 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14695791640
I0930 09:19:25.229378 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712568856
I0930 09:19:25.234420 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14712577048
I0930 09:19:25.239525 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14712585240
I0930 09:19:25.244684 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14729362456
I0930 09:19:25.249735 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746139672
I0930 09:19:25.254882 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746147864
I0930 09:19:25.259890 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746156056
I0930 09:19:25.265021 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746164248
I0930 09:19:25.270034 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746172440
I0930 09:19:25.275181 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746180632
I0930 09:19:25.280180 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746188824
I0930 09:19:25.285306 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14746221592
I0930 09:19:25.290353 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14746254360
I0930 09:19:25.295494 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14813363224
I0930 09:19:25.300538 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880472088
I0930 09:19:25.305634 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14880480280
I0930 09:19:25.310771 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 14880488472
I0930 09:19:25.315801 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 14947597336
I0930 09:19:25.320929 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014706200
I0930 09:19:25.325974 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014714392
I0930 09:19:25.331607 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014722584
I0930 09:19:25.336630 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014730776
I0930 09:19:25.341739 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014738968
I0930 09:19:25.345616 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15014739480
I0930 09:19:25.349410 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15014739992
I0930 09:19:25.354504 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15031517208
I0930 09:19:25.359633 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048294424
I0930 09:19:25.364661 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15048302616
I0930 09:19:25.369818 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15048310808
I0930 09:19:25.374951 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15065088024
I0930 09:19:25.380106 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081865240
I0930 09:19:25.385160 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15081873432
I0930 09:19:25.390299 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15081881624
I0930 09:19:25.395373 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15098658840
I0930 09:19:25.400484 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115436056
I0930 09:19:25.405517 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15115444248
I0930 09:19:25.410677 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15115452440
I0930 09:19:25.415816 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15132229656
I0930 09:19:25.420895 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149006872
I0930 09:19:25.426021 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149015064
I0930 09:19:25.431068 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149023256
I0930 09:19:25.436215 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149031448
I0930 09:19:25.441273 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149039640
I0930 09:19:25.446998 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149047832
I0930 09:19:25.452015 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149056024
I0930 09:19:25.457148 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15149088792
I0930 09:19:25.462191 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15149121560
I0930 09:19:25.467311 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15216230424
I0930 09:19:25.472355 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283339288
I0930 09:19:25.477498 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15283347480
I0930 09:19:25.482652 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15283355672
I0930 09:19:25.487686 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15350464536
I0930 09:19:25.492851 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417573400
I0930 09:19:25.497892 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417581592
I0930 09:19:25.503060 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417589784
I0930 09:19:25.508091 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417597976
I0930 09:19:25.513238 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417606168
I0930 09:19:25.517135 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15417606680
I0930 09:19:25.520895 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15417607192
I0930 09:19:25.525948 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15434384408
I0930 09:19:25.531166 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451161624
I0930 09:19:25.536194 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15451169816
I0930 09:19:25.541316 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15451178008
I0930 09:19:25.546449 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15467955224
I0930 09:19:25.551598 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484732440
I0930 09:19:25.556680 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15484740632
I0930 09:19:25.562311 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15484748824
I0930 09:19:25.567366 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15501526040
I0930 09:19:25.572594 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518303256
I0930 09:19:25.577871 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15518311448
I0930 09:19:25.583035 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15518319640
I0930 09:19:25.588186 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15535096856
I0930 09:19:25.593229 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551874072
I0930 09:19:25.598398 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551882264
I0930 09:19:25.603439 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551890456
I0930 09:19:25.608552 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551898648
I0930 09:19:25.613592 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551906840
I0930 09:19:25.618741 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551915032
I0930 09:19:25.623805 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551923224
I0930 09:19:25.628928 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15551955992
I0930 09:19:25.633986 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15551988760
I0930 09:19:25.639178 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15619097624
I0930 09:19:25.644217 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686206488
I0930 09:19:25.649369 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15686214680
I0930 09:19:25.654589 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15686222872
I0930 09:19:25.659653 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15753331736
I0930 09:19:25.664817 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820440600
I0930 09:19:25.669866 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820448792
I0930 09:19:25.675568 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820456984
I0930 09:19:25.680642 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820465176
I0930 09:19:25.685793 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820473368
I0930 09:19:25.689677 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15820473880
I0930 09:19:25.693484 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15820474392
I0930 09:19:25.698551 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15837251608
I0930 09:19:25.703682 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854028824
I0930 09:19:25.708719 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15854037016
I0930 09:19:25.713865 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15854045208
I0930 09:19:25.718942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15870822424
I0930 09:19:25.724094 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887599640
I0930 09:19:25.729155 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15887607832
I0930 09:19:25.734323 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15887616024
I0930 09:19:25.739342 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15904393240
I0930 09:19:25.744531 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921170456
I0930 09:19:25.749558 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15921178648
I0930 09:19:25.754729 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15921186840
I0930 09:19:25.759858 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15937964056
I0930 09:19:25.764953 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954741272
I0930 09:19:25.770088 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954749464
I0930 09:19:25.775210 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954757656
I0930 09:19:25.780356 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954765848
I0930 09:19:25.785447 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954774040
I0930 09:19:25.791072 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954782232
I0930 09:19:25.796103 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954790424
I0930 09:19:25.801247 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 15954823192
I0930 09:19:25.806382 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 15954855960
I0930 09:19:25.811527 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16021964824
I0930 09:19:25.816612 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089073688
I0930 09:19:25.821777 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16089081880
I0930 09:19:25.826964 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16089090072
I0930 09:19:25.832021 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16156198936
I0930 09:19:25.837179 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223307800
I0930 09:19:25.842236 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223315992
I0930 09:19:25.847439 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223324184
I0930 09:19:25.852477 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223332376
I0930 09:19:25.857606 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223340568
I0930 09:19:25.861514 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16223341080
I0930 09:19:25.865351 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16223341592
I0930 09:19:25.870425 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16240118808
I0930 09:19:25.875583 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256896024
I0930 09:19:25.880614 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16256904216
I0930 09:19:25.885807 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16256912408
I0930 09:19:25.890890 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16273689624
I0930 09:19:25.896140 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290466840
I0930 09:19:25.901206 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16290475032
I0930 09:19:25.906871 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16290483224
I0930 09:19:25.912015 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16307260440
I0930 09:19:25.917156 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324037656
I0930 09:19:25.922199 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16324045848
I0930 09:19:25.927397 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16324054040
I0930 09:19:25.932546 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16340831256
I0930 09:19:25.937644 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357608472
I0930 09:19:25.942842 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357616664
I0930 09:19:25.947884 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357624856
I0930 09:19:25.953028 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357633048
I0930 09:19:25.958066 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357641240
I0930 09:19:25.963257 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357649432
I0930 09:19:25.968302 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357657624
I0930 09:19:25.973464 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16357690392
I0930 09:19:25.978654 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16357723160
I0930 09:19:25.983807 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16424832024
I0930 09:19:25.988884 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491940888
I0930 09:19:25.994035 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16491949080
I0930 09:19:25.999199 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16491957272
I0930 09:19:26.004294 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16559066136
I0930 09:19:26.009474 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626175000
I0930 09:19:26.014555 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626183192
I0930 09:19:26.020224 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626191384
I0930 09:19:26.025274 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626199576
I0930 09:19:26.030460 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626207768
I0930 09:19:26.034363 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16626208280
I0930 09:19:26.038140 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16626208792
I0930 09:19:26.043241 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16642986008
I0930 09:19:26.048426 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659763224
I0930 09:19:26.053494 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16659771416
I0930 09:19:26.058684 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16659779608
I0930 09:19:26.063726 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16676556824
I0930 09:19:26.068902 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693334040
I0930 09:19:26.073950 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16693342232
I0930 09:19:26.079131 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16693350424
I0930 09:19:26.084185 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16710127640
I0930 09:19:26.089333 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726904856
I0930 09:19:26.094432 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16726913048
I0930 09:19:26.099591 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16726921240
I0930 09:19:26.104744 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16743698456
I0930 09:19:26.109839 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760475672
I0930 09:19:26.115015 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760483864
I0930 09:19:26.120205 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760492056
I0930 09:19:26.125348 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760500248
I0930 09:19:26.130426 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760508440
I0930 09:19:26.136047 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760516632
I0930 09:19:26.141107 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760524824
I0930 09:19:26.146291 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16760557592
I0930 09:19:26.151352 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16760590360
I0930 09:19:26.156502 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16827699224
I0930 09:19:26.161618 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894808088
I0930 09:19:26.166774 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16894816280
I0930 09:19:26.171905 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 16894824472
I0930 09:19:26.177130 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 16961933336
I0930 09:19:26.182315 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029042200
I0930 09:19:26.187370 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029050392
I0930 09:19:26.192561 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029058584
I0930 09:19:26.197627 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029066776
I0930 09:19:26.202802 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029074968
I0930 09:19:26.206697 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17029075480
I0930 09:19:26.210516 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17029075992
I0930 09:19:26.215567 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17045853208
I0930 09:19:26.220740 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062630424
I0930 09:19:26.225804 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17062638616
I0930 09:19:26.230967 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17062646808
I0930 09:19:26.236021 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17079424024
I0930 09:19:26.241199 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096201240
I0930 09:19:26.246288 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17096209432
I0930 09:19:26.251966 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17096217624
I0930 09:19:26.257034 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17112994840
I0930 09:19:26.262217 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129772056
I0930 09:19:26.267324 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17129780248
I0930 09:19:26.272497 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17129788440
I0930 09:19:26.277704 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17146565656
I0930 09:19:26.282855 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163342872
I0930 09:19:26.288064 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163351064
I0930 09:19:26.293138 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163359256
I0930 09:19:26.298422 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163367448
I0930 09:19:26.303520 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163375640
I0930 09:19:26.308708 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163383832
I0930 09:19:26.313764 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163392024
I0930 09:19:26.318972 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17163424792
I0930 09:19:26.324042 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17163457560
I0930 09:19:26.329212 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17230566424
I0930 09:19:26.334335 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297675288
I0930 09:19:26.339493 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17297683480
I0930 09:19:26.344655 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17297691672
I0930 09:19:26.349742 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17364800536
I0930 09:19:26.354939 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431909400
I0930 09:19:26.360019 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431917592
I0930 09:19:26.365687 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431925784
I0930 09:19:26.370791 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431933976
I0930 09:19:26.376114 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431942168
I0930 09:19:26.380038 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17431942680
I0930 09:19:26.383868 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17431943192
I0930 09:19:26.388958 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17448720408
I0930 09:19:26.394145 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465497624
I0930 09:19:26.399266 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17465505816
I0930 09:19:26.404458 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17465514008
I0930 09:19:26.409540 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17482291224
I0930 09:19:26.414754 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499068440
I0930 09:19:26.419848 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17499076632
I0930 09:19:26.425004 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17499084824
I0930 09:19:26.430102 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17515862040
I0930 09:19:26.435311 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532639256
I0930 09:19:26.440404 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17532647448
I0930 09:19:26.445577 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17532655640
I0930 09:19:26.450788 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17549432856
I0930 09:19:26.455885 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566210072
I0930 09:19:26.461067 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566218264
I0930 09:19:26.466130 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566226456
I0930 09:19:26.471331 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566234648
I0930 09:19:26.476422 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566242840
I0930 09:19:26.482089 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566251032
I0930 09:19:26.487184 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566259224
I0930 09:19:26.492347 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17566291992
I0930 09:19:26.497441 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17566324760
I0930 09:19:26.502614 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17633433624
I0930 09:19:26.507749 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700542488
I0930 09:19:26.512955 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17700550680
I0930 09:19:26.518246 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17700558872
I0930 09:19:26.523360 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17767667736
I0930 09:19:26.528578 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834776600
I0930 09:19:26.533730 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834784792
I0930 09:19:26.538952 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834792984
I0930 09:19:26.544036 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834801176
I0930 09:19:26.549302 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834809368
I0930 09:19:26.553255 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17834809880
I0930 09:19:26.557110 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17834810392
I0930 09:19:26.562219 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17851587608
I0930 09:19:26.567440 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868364824
I0930 09:19:26.572605 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17868373016
I0930 09:19:26.577844 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17868381208
I0930 09:19:26.582982 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17885158424
I0930 09:19:26.588256 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901935640
I0930 09:19:26.593315 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17901943832
I0930 09:19:26.599007 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17901952024
I0930 09:19:26.604080 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17918729240
I0930 09:19:26.609282 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935506456
I0930 09:19:26.614406 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17935514648
I0930 09:19:26.619602 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17935522840
I0930 09:19:26.624774 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17952300056
I0930 09:19:26.629872 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969077272
I0930 09:19:26.635086 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969085464
I0930 09:19:26.640158 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969093656
I0930 09:19:26.645344 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969101848
I0930 09:19:26.650487 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969110040
I0930 09:19:26.655669 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969118232
I0930 09:19:26.660746 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969126424
I0930 09:19:26.666041 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 17969159192
I0930 09:19:26.671155 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 17969191960
I0930 09:19:26.676334 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18036300824
I0930 09:19:26.681437 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103409688
I0930 09:19:26.686640 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18103417880
I0930 09:19:26.691808 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18103426072
I0930 09:19:26.696908 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18170534936
I0930 09:19:26.702103 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237643800
I0930 09:19:26.707213 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237651992
I0930 09:19:26.712905 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237660184
I0930 09:19:26.717994 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237668376
I0930 09:19:26.723193 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237676568
I0930 09:19:26.727133 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18237677080
I0930 09:19:26.730998 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18237677592
I0930 09:19:26.736100 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18254454808
I0930 09:19:26.741295 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271232024
I0930 09:19:26.746535 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18271240216
I0930 09:19:26.751727 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18271248408
I0930 09:19:26.756823 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18288025624
I0930 09:19:26.762020 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304802840
I0930 09:19:26.767171 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18304811032
I0930 09:19:26.772437 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18304819224
I0930 09:19:26.777618 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18321596440
I0930 09:19:26.782846 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338373656
I0930 09:19:26.787941 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18338381848
I0930 09:19:26.793130 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18338390040
I0930 09:19:26.798389 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18355167256
I0930 09:19:26.803508 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371944472
I0930 09:19:26.808702 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371952664
I0930 09:19:26.813821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371960856
I0930 09:19:26.819039 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371969048
I0930 09:19:26.824176 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371977240
I0930 09:19:26.829859 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18371985432
I0930 09:19:26.834972 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18371993624
I0930 09:19:26.840151 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18372026392
I0930 09:19:26.845265 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18372059160
I0930 09:19:26.850507 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18439168024
I0930 09:19:26.855628 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506276888
I0930 09:19:26.860832 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18506285080
I0930 09:19:26.866022 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18506293272
I0930 09:19:26.871150 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18573402136
I0930 09:19:26.876387 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640511000
I0930 09:19:26.881465 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640519192
I0930 09:19:26.886704 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640527384
I0930 09:19:26.891794 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640535576
I0930 09:19:26.897059 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640543768
I0930 09:19:26.901018 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18640544280
I0930 09:19:26.904862 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18640544792
I0930 09:19:26.910010 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18657322008
I0930 09:19:26.915237 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674099224
I0930 09:19:26.920345 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18674107416
I0930 09:19:26.925516 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18674115608
I0930 09:19:26.930654 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18690892824
I0930 09:19:26.935861 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707670040
I0930 09:19:26.940969 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18707678232
I0930 09:19:26.946784 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18707686424
I0930 09:19:26.952172 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18724463640
I0930 09:19:26.957737 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741240856
I0930 09:19:26.962983 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18741249048
I0930 09:19:26.968185 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18741257240
I0930 09:19:26.973390 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18758034456
I0930 09:19:26.978638 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774811672
I0930 09:19:26.983848 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774819864
I0930 09:19:26.988923 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774828056
I0930 09:19:26.994107 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774836248
I0930 09:19:26.999284 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774844440
I0930 09:19:27.004579 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774852632
I0930 09:19:27.009689 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774860824
I0930 09:19:27.014919 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18774893592
I0930 09:19:27.020017 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18774926360
I0930 09:19:27.025210 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18842035224
I0930 09:19:27.030416 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909144088
I0930 09:19:27.035600 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18909152280
I0930 09:19:27.040821 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 18909160472
I0930 09:19:27.045907 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 18976269336
I0930 09:19:27.051158 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043378200
I0930 09:19:27.056266 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043386392
I0930 09:19:27.061963 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043394584
I0930 09:19:27.067094 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043402776
I0930 09:19:27.072295 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043410968
I0930 09:19:27.076246 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19043411480
I0930 09:19:27.080112 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19043411992
I0930 09:19:27.085291 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19060189208
I0930 09:19:27.090538 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076966424
I0930 09:19:27.095651 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19076974616
I0930 09:19:27.100852 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19076982808
I0930 09:19:27.105953 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19093760024
I0930 09:19:27.111194 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110537240
I0930 09:19:27.116280 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19110545432
I0930 09:19:27.121458 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19110553624
I0930 09:19:27.126576 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19127330840
I0930 09:19:27.131782 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144108056
I0930 09:19:27.136898 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19144116248
I0930 09:19:27.142102 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19144124440
I0930 09:19:27.147338 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19160901656
I0930 09:19:27.152455 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177678872
I0930 09:19:27.157668 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177687064
I0930 09:19:27.162817 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177695256
I0930 09:19:27.168027 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177703448
I0930 09:19:27.173232 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177711640
I0930 09:19:27.179029 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177719832
I0930 09:19:27.184154 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177728024
I0930 09:19:27.189369 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19177760792
I0930 09:19:27.194515 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19177793560
I0930 09:19:27.199695 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19244902424
I0930 09:19:27.204808 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312011288
I0930 09:19:27.209993 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19312019480
I0930 09:19:27.215328 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19312027672
I0930 09:19:27.220446 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19379136536
I0930 09:19:27.225657 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446245400
I0930 09:19:27.230766 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446253592
I0930 09:19:27.236043 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446261784
I0930 09:19:27.241148 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446269976
I0930 09:19:27.246402 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446278168
I0930 09:19:27.250358 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19446278680
I0930 09:19:27.254232 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19446279192
I0930 09:19:27.259374 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19463056408
I0930 09:19:27.264599 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479833624
I0930 09:19:27.269734 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19479841816
I0930 09:19:27.274942 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19479850008
I0930 09:19:27.280061 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19496627224
I0930 09:19:27.285282 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513404440
I0930 09:19:27.290426 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19513412632
I0930 09:19:27.296141 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19513420824
I0930 09:19:27.301249 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19530198040
I0930 09:19:27.306501 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546975256
I0930 09:19:27.311605 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19546983448
I0930 09:19:27.316818 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19546991640
I0930 09:19:27.322023 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19563768856
I0930 09:19:27.327181 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580546072
I0930 09:19:27.332398 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580554264
I0930 09:19:27.337499 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580562456
I0930 09:19:27.342728 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580570648
I0930 09:19:27.347934 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580578840
I0930 09:19:27.353146 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580587032
I0930 09:19:27.358240 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580595224
I0930 09:19:27.363477 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19580627992
I0930 09:19:27.368623 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19580660760
I0930 09:19:27.373838 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19647769624
I0930 09:19:27.379079 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714878488
I0930 09:19:27.384304 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19714886680
I0930 09:19:27.389506 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19714894872
I0930 09:19:27.394660 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19782003736
I0930 09:19:27.399905 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849112600
I0930 09:19:27.405032 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849120792
I0930 09:19:27.410782 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849128984
I0930 09:19:27.415904 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849137176
I0930 09:19:27.421093 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849145368
I0930 09:19:27.425065 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19849145880
I0930 09:19:27.428935 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19849146392
I0930 09:19:27.434061 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19865923608
I0930 09:19:27.439306 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882700824
I0930 09:19:27.444423 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19882709016
I0930 09:19:27.449645 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19882717208
I0930 09:19:27.454789 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19899494424
I0930 09:19:27.460036 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916271640
I0930 09:19:27.465157 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19916279832
I0930 09:19:27.470391 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19916288024
I0930 09:19:27.475534 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19933065240
I0930 09:19:27.480736 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949842456
I0930 09:19:27.485870 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19949850648
I0930 09:19:27.491139 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19949858840
I0930 09:19:27.496365 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19966636056
I0930 09:19:27.501509 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983413272
I0930 09:19:27.506782 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983421464
I0930 09:19:27.511899 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983429656
I0930 09:19:27.517119 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983437848
I0930 09:19:27.522290 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983446040
I0930 09:19:27.528025 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983454232
I0930 09:19:27.533203 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983462424
I0930 09:19:27.538429 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 19983495192
I0930 09:19:27.543576 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 19983527960
I0930 09:19:27.548861 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20050636824
I0930 09:19:27.554055 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117745688
I0930 09:19:27.559280 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20117753880
I0930 09:19:27.564504 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20117762072
I0930 09:19:27.569629 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20184870936
I0930 09:19:27.575118 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251979800
I0930 09:19:27.580260 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20251987992
I0930 09:19:27.585491 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20251996184
I0930 09:19:27.590627 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252004376
I0930 09:19:27.595870 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252012568
I0930 09:19:27.599841 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20252013080
I0930 09:19:27.603746 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20252013592
I0930 09:19:27.608889 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20268790808
I0930 09:19:27.614130 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285568024
I0930 09:19:27.619270 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20285576216
I0930 09:19:27.624484 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20285584408
I0930 09:19:27.629678 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20302361624
I0930 09:19:27.635036 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319138840
I0930 09:19:27.640181 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20319147032
I0930 09:19:27.645913 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20319155224
I0930 09:19:27.651061 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20335932440
I0930 09:19:27.656298 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352709656
I0930 09:19:27.661422 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20352717848
I0930 09:19:27.666676 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20352726040
I0930 09:19:27.671908 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20369503256
I0930 09:19:27.677054 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386280472
I0930 09:19:27.682287 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386288664
I0930 09:19:27.687415 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386296856
I0930 09:19:27.692629 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386305048
I0930 09:19:27.697793 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386313240
I0930 09:19:27.703038 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386321432
I0930 09:19:27.708168 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386329624
I0930 09:19:27.713400 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386337624
I0930 09:19:27.718610 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386345624
I0930 09:19:27.723803 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386353624
I0930 09:19:27.728950 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386361624
I0930 09:19:27.734154 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386369624
I0930 09:19:27.739395 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386377624
I0930 09:19:27.744527 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386385624
I0930 09:19:27.749756 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386393624
I0930 09:19:27.754910 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386401624
I0930 09:19:27.760632 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386409624
I0930 09:19:27.765768 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386417624
I0930 09:19:27.771034 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386425624
I0930 09:19:27.776243 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386433624
I0930 09:19:27.781491 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386441624
I0930 09:19:27.786649 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386449624
I0930 09:19:27.791859 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386457624
I0930 09:19:27.797007 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386465624
I0930 09:19:27.802226 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386473624
I0930 09:19:27.807474 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386481624
I0930 09:19:27.812583 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386489624
I0930 09:19:27.817832 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386497624
I0930 09:19:27.822987 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386505624
I0930 09:19:27.828218 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386513624
I0930 09:19:27.833323 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386521624
I0930 09:19:27.838564 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386529624
I0930 09:19:27.843680 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386537624
I0930 09:19:27.849093 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386545624
I0930 09:19:27.854596 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386553624
I0930 09:19:27.860101 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386561624
I0930 09:19:27.865319 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386569624
I0930 09:19:27.870642 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20386577624
I0930 09:19:27.876461 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20386585624
I0930 09:19:27.881638 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20402969624
I0930 09:19:27.886957 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20419353624
I0930 09:19:27.892121 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20435737624
I0930 09:19:27.897456 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20452121624
I0930 09:19:27.902629 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20468505624
I0930 09:19:27.907910 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20484889624
I0930 09:19:27.913054 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20501273624
I0930 09:19:27.918319 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20517657624
I0930 09:19:27.923458 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20534041624
I0930 09:19:27.928708 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20550425624
I0930 09:19:27.933962 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20566809624
I0930 09:19:27.939135 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20583193624
I0930 09:19:27.944392 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20599577624
I0930 09:19:27.949556 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20615961624
I0930 09:19:27.954811 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20632345624
I0930 09:19:27.959980 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20648729624
I0930 09:19:27.965216 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20665113624
I0930 09:19:27.970386 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20681497624
I0930 09:19:27.975708 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20697881624
I0930 09:19:27.980882 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20714265624
I0930 09:19:27.986217 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20730649624
I0930 09:19:27.991944 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20747033624
I0930 09:19:27.997095 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20763417624
I0930 09:19:28.002547 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20779801624
I0930 09:19:28.007711 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20796185624
I0930 09:19:28.013004 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20812569624
I0930 09:19:28.018152 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20828953624
I0930 09:19:28.023461 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20845337624
I0930 09:19:28.028604 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20861721624
I0930 09:19:28.033844 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20878105624
I0930 09:19:28.039060 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam on /job:local/replica:0/task:0/device:CPU:0 20894489624
I0930 09:19:28.044306 140009111922496 cluster.py:515] Place variable 1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1 on /job:local/replica:0/task:0/device:CPU:0 20910873624
I0930 09:19:28.779919 140009111922496 cluster.py:515] Place variable total_nan_gradients/var on /job:local/replica:0/task:0/device:CPU:0 20910873632
I0930 09:19:28.782002 140009111922496 py_utils.py:1389] Creating var total_nan_gradients/var:0 shape=() on device /job:local/replica:0/task:0/device:CPU:0
I0930 09:19:28.976790 140009111922496 trainer.py:400] Trainer number of enqueue ops: 0
I0930 09:19:28.976973 140009111922496 trainer.py:409] AttributeError. Expected for single task models.
I0930 09:19:49.348643 140009111922496 trainer.py:1577] Starting runners
I0930 09:19:49.349031 140002471614208 base_runner.py:167] controller started.
I0930 09:19:49.349395 140002463221504 base_runner.py:167] trainer started.
I0930 09:19:49.349523 140009111922496 trainer.py:1596] Waiting for runners to finish...
I0930 09:20:01.085589 140002463221504 trainer.py:466] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 09:20:02.092231 140002463221504 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1388, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 1.50 seconds before retrying.
I0930 09:20:02.093266 140002463221504 trainer.py:466] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 09:20:03.595846 140002463221504 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1388, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 2.28 seconds before retrying.
I0930 09:20:03.597005 140002463221504 trainer.py:466] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 09:20:05.875589 140002463221504 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1388, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 3.49 seconds before retrying.
I0930 09:20:05.876543 140002463221504 trainer.py:466] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 09:20:08.009932 140002471614208 checkpointer.py:133] Uninitialized var list: [b'global_step', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var', b'1bwds_wpm_level_lm/total_samples/var', b'beta1_power', b'beta2_power', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam', b'1bwds_wpm_level_lm/transformerlm/emb/src_token_emb/wm/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_0/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_1/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_2/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_3/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_4/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_5/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_6/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_7/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_10/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_11/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_12/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_13/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_14/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_15/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_8/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_9/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_16/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_17/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_18/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_19/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_20/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_21/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_22/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_23/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_24/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_25/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_26/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_27/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_28/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_29/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_30/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_0/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer/fflayer_1/w/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/tr_fflayer/fflayer_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/inner_att/per_dim_scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_post_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/ctx_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/query_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/multihead_atten/source_proj_b/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/bias/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam', b'1bwds_wpm_level_lm/transformerlm/encoder_31/multihead_self_atten/atten_ln/scale/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/bias_9/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_0/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_1/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_10/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_11/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_12/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_13/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_14/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_15/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_2/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_3/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_4/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_5/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_6/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_7/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_8/var/Adam_1', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam', b'1bwds_wpm_level_lm/transformerlm/softmax/weight_9/var/Adam_1', b'total_nan_gradients/var'] 
I0930 09:20:08.012735 140002471614208 checkpointer.py:88] Load from checkpoint /tmp/mnist/log/train/ckpt-00001837.
INFO:tensorflow:Restoring parameters from /tmp/mnist/log/train/ckpt-00001837
I0930 09:20:08.013825 140002471614208 saver.py:1284] Restoring parameters from /tmp/mnist/log/train/ckpt-00001837
I0930 09:20:09.375602 140002463221504 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1388, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 5.35 seconds before retrying.
I0930 09:20:09.376810 140002463221504 trainer.py:466] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
2019-09-30 09:20:12.729847: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
E0930 09:20:13.086209 140002471614208 base_runner.py:212] controller done (fatal error): <class 'tensorflow.python.framework.errors_impl.NotFoundError'>
I0930 09:20:13.090888 140002471614208 base_runner.py:106] controller exception: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

From /job:local/replica:0/task:0:
Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
	 [[node save/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save/RestoreV2':
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1824, in <module>
    tf.app.run(main)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1820, in main
    RunnerManager(FLAGS.model).Start()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1812, in Start
    self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1560, in CreateRunners
    trial)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1508, in _CreateRunner
    return self.Controller(cfg, *common_args)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 268, in __init__
    self._model)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 281, in _CreateCheckpointer
    return checkpointer.Checkpointer(train_dir, model)
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 70, in __init__
    self._saver = self._GetSaver()
  File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 83, in _GetSaver
    write_version=tf.train.SaverDef.V2)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 502, in _build_internal
    restore_sequentially, reshape)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 381, in _AddShardedRestoreOps
    name="restore_shard"))
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
    op_def=op_def)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()


E0930 09:20:13.094031 140002471614208 base_runner.py:219] Traceback (most recent call last):
E0930 09:20:13.094120 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1369, in _do_call
E0930 09:20:13.094179 140002471614208 base_runner.py:219]     return fn(*args)
E0930 09:20:13.094233 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1352, in _run_fn
E0930 09:20:13.094325 140002471614208 base_runner.py:219]     target_list, run_metadata)
E0930 09:20:13.094378 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1447, in _call_tf_sessionrun
E0930 09:20:13.094424 140002471614208 base_runner.py:219]     run_metadata)
E0930 09:20:13.094470 140002471614208 base_runner.py:219] tensorflow.python.framework.errors_impl.NotFoundError: From /job:local/replica:0/task:0:
E0930 09:20:13.094515 140002471614208 base_runner.py:219] Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
E0930 09:20:13.094560 140002471614208 base_runner.py:219] 	 [[{{node save/RestoreV2}}]]
E0930 09:20:13.094605 140002471614208 base_runner.py:219] 
E0930 09:20:13.094650 140002471614208 base_runner.py:219] During handling of the above exception, another exception occurred:
E0930 09:20:13.094696 140002471614208 base_runner.py:219] 
E0930 09:20:13.094740 140002471614208 base_runner.py:219] Traceback (most recent call last):
E0930 09:20:13.094785 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 1290, in restore
E0930 09:20:13.094830 140002471614208 base_runner.py:219]     {self.saver_def.filename_tensor_name: save_path})
E0930 09:20:13.094875 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
E0930 09:20:13.094919 140002471614208 base_runner.py:219]     run_metadata_ptr)
E0930 09:20:13.094964 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
E0930 09:20:13.095008 140002471614208 base_runner.py:219]     feed_dict_tensor, options, run_metadata)
E0930 09:20:13.095052 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
E0930 09:20:13.095097 140002471614208 base_runner.py:219]     run_metadata)
E0930 09:20:13.095141 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1388, in _do_call
E0930 09:20:13.095192 140002471614208 base_runner.py:219]     raise type(e)(node_def, op, message)
E0930 09:20:13.095239 140002471614208 base_runner.py:219] tensorflow.python.framework.errors_impl.NotFoundError: From /job:local/replica:0/task:0:
E0930 09:20:13.095283 140002471614208 base_runner.py:219] Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
E0930 09:20:13.095328 140002471614208 base_runner.py:219] 	 [[node save/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
E0930 09:20:13.095372 140002471614208 base_runner.py:219] 
E0930 09:20:13.095417 140002471614208 base_runner.py:219] Original stack trace for 'save/RestoreV2':
E0930 09:20:13.095461 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1824, in <module>
E0930 09:20:13.095505 140002471614208 base_runner.py:219]     tf.app.run(main)
E0930 09:20:13.095549 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
E0930 09:20:13.095593 140002471614208 base_runner.py:219]     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
E0930 09:20:13.095638 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
E0930 09:20:13.095683 140002471614208 base_runner.py:219]     _run_main(main, args)
E0930 09:20:13.095727 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
E0930 09:20:13.095771 140002471614208 base_runner.py:219]     sys.exit(main(argv))
E0930 09:20:13.095815 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1820, in main
E0930 09:20:13.095859 140002471614208 base_runner.py:219]     RunnerManager(FLAGS.model).Start()
E0930 09:20:13.095903 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1812, in Start
E0930 09:20:13.095947 140002471614208 base_runner.py:219]     self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
E0930 09:20:13.095991 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1560, in CreateRunners
E0930 09:20:13.096035 140002471614208 base_runner.py:219]     trial)
E0930 09:20:13.096079 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1508, in _CreateRunner
E0930 09:20:13.096124 140002471614208 base_runner.py:219]     return self.Controller(cfg, *common_args)
E0930 09:20:13.096168 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 268, in __init__
E0930 09:20:13.096211 140002471614208 base_runner.py:219]     self._model)
E0930 09:20:13.096256 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 281, in _CreateCheckpointer
E0930 09:20:13.096300 140002471614208 base_runner.py:219]     return checkpointer.Checkpointer(train_dir, model)
E0930 09:20:13.096345 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 70, in __init__
E0930 09:20:13.096389 140002471614208 base_runner.py:219]     self._saver = self._GetSaver()
E0930 09:20:13.096433 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 83, in _GetSaver
E0930 09:20:13.096477 140002471614208 base_runner.py:219]     write_version=tf.train.SaverDef.V2)
E0930 09:20:13.096521 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
E0930 09:20:13.096569 140002471614208 base_runner.py:219]     self.build()
E0930 09:20:13.096613 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 840, in build
E0930 09:20:13.096658 140002471614208 base_runner.py:219]     self._build(self._filename, build_save=True, build_restore=True)
E0930 09:20:13.096701 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 878, in _build
E0930 09:20:13.096745 140002471614208 base_runner.py:219]     build_restore=build_restore)
E0930 09:20:13.096790 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 502, in _build_internal
E0930 09:20:13.096833 140002471614208 base_runner.py:219]     restore_sequentially, reshape)
E0930 09:20:13.096877 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 381, in _AddShardedRestoreOps
E0930 09:20:13.096921 140002471614208 base_runner.py:219]     name="restore_shard"))
E0930 09:20:13.096965 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
E0930 09:20:13.097009 140002471614208 base_runner.py:219]     restore_sequentially)
E0930 09:20:13.097054 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
E0930 09:20:13.097098 140002471614208 base_runner.py:219]     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
E0930 09:20:13.097148 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
E0930 09:20:13.097193 140002471614208 base_runner.py:219]     name=name)
E0930 09:20:13.097238 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
E0930 09:20:13.097283 140002471614208 base_runner.py:219]     op_def=op_def)
E0930 09:20:13.097326 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
E0930 09:20:13.097371 140002471614208 base_runner.py:219]     return func(*args, **kwargs)
E0930 09:20:13.097415 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
E0930 09:20:13.097460 140002471614208 base_runner.py:219]     attrs, op_def, compute_device)
E0930 09:20:13.097504 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
E0930 09:20:13.097548 140002471614208 base_runner.py:219]     op_def=op_def)
E0930 09:20:13.097592 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
E0930 09:20:13.097636 140002471614208 base_runner.py:219]     self._traceback = tf_stack.extract_stack()
E0930 09:20:13.097681 140002471614208 base_runner.py:219] 
E0930 09:20:13.097725 140002471614208 base_runner.py:219] 
E0930 09:20:13.097769 140002471614208 base_runner.py:219] During handling of the above exception, another exception occurred:
E0930 09:20:13.097814 140002471614208 base_runner.py:219] 
E0930 09:20:13.097858 140002471614208 base_runner.py:219] Traceback (most recent call last):
E0930 09:20:13.097902 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 1300, in restore
E0930 09:20:13.097946 140002471614208 base_runner.py:219]     names_to_keys = object_graph_key_mapping(save_path)
E0930 09:20:13.097991 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 1618, in object_graph_key_mapping
E0930 09:20:13.098043 140002471614208 base_runner.py:219]     object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY)
E0930 09:20:13.098088 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/pywrap_tensorflow_internal.py", line 915, in get_tensor
E0930 09:20:13.098133 140002471614208 base_runner.py:219]     return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str))
E0930 09:20:13.098176 140002471614208 base_runner.py:219] tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint
E0930 09:20:13.098221 140002471614208 base_runner.py:219] 
E0930 09:20:13.098299 140002471614208 base_runner.py:219] During handling of the above exception, another exception occurred:
E0930 09:20:13.098357 140002471614208 base_runner.py:219] 
E0930 09:20:13.098402 140002471614208 base_runner.py:219] Traceback (most recent call last):
E0930 09:20:13.098447 140002471614208 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
E0930 09:20:13.098490 140002471614208 base_runner.py:219]     loop_func(*loop_args)
E0930 09:20:13.098535 140002471614208 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 318, in _Loop
E0930 09:20:13.098579 140002471614208 base_runner.py:219]     self.checkpointer.RestoreIfNeeded(sess)
E0930 09:20:13.098623 140002471614208 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 134, in RestoreIfNeeded
E0930 09:20:13.098667 140002471614208 base_runner.py:219]     if self._Restore(sess):
E0930 09:20:13.098711 140002471614208 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 119, in _Restore
E0930 09:20:13.098756 140002471614208 base_runner.py:219]     self.RestoreFromPath(sess, path)
E0930 09:20:13.098800 140002471614208 base_runner.py:219]   File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 89, in RestoreFromPath
E0930 09:20:13.098845 140002471614208 base_runner.py:219]     self._saver.restore(sess, checkpoint_path)
E0930 09:20:13.098889 140002471614208 base_runner.py:219]   File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 1306, in restore
E0930 09:20:13.098933 140002471614208 base_runner.py:219]     err, "a Variable name or other graph key that is missing")
E0930 09:20:13.098977 140002471614208 base_runner.py:219] tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
E0930 09:20:13.099023 140002471614208 base_runner.py:219] 
E0930 09:20:13.099068 140002471614208 base_runner.py:219] From /job:local/replica:0/task:0:
E0930 09:20:13.099113 140002471614208 base_runner.py:219] Key 1bwds_wpm_level_lm/total_samples/var not found in checkpoint
E0930 09:20:13.099157 140002471614208 base_runner.py:219] 	 [[node save/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
E0930 09:20:13.099201 140002471614208 base_runner.py:219] 
E0930 09:20:13.099246 140002471614208 base_runner.py:219] Original stack trace for 'save/RestoreV2':
E0930 09:20:13.099290 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1824, in <module>
E0930 09:20:13.099335 140002471614208 base_runner.py:219]     tf.app.run(main)
E0930 09:20:13.099379 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py", line 40, in run
E0930 09:20:13.099423 140002471614208 base_runner.py:219]     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
E0930 09:20:13.099472 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 299, in run
E0930 09:20:13.099517 140002471614208 base_runner.py:219]     _run_main(main, args)
E0930 09:20:13.099561 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/absl/app.py", line 250, in _run_main
E0930 09:20:13.099605 140002471614208 base_runner.py:219]     sys.exit(main(argv))
E0930 09:20:13.099649 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1820, in main
E0930 09:20:13.099694 140002471614208 base_runner.py:219]     RunnerManager(FLAGS.model).Start()
E0930 09:20:13.099738 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1812, in Start
E0930 09:20:13.099782 140002471614208 base_runner.py:219]     self.StartRunners(self.CreateRunners(FLAGS.job.split(','), FLAGS.logdir))
E0930 09:20:13.099826 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1560, in CreateRunners
E0930 09:20:13.099870 140002471614208 base_runner.py:219]     trial)
E0930 09:20:13.099914 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 1508, in _CreateRunner
E0930 09:20:13.099958 140002471614208 base_runner.py:219]     return self.Controller(cfg, *common_args)
E0930 09:20:13.100003 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 268, in __init__
E0930 09:20:13.100047 140002471614208 base_runner.py:219]     self._model)
E0930 09:20:13.100091 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 281, in _CreateCheckpointer
E0930 09:20:13.100136 140002471614208 base_runner.py:219]     return checkpointer.Checkpointer(train_dir, model)
E0930 09:20:13.100180 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 70, in __init__
E0930 09:20:13.100224 140002471614208 base_runner.py:219]     self._saver = self._GetSaver()
E0930 09:20:13.100268 140002471614208 base_runner.py:219]   File "home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/checkpointer.py", line 83, in _GetSaver
E0930 09:20:13.100312 140002471614208 base_runner.py:219]     write_version=tf.train.SaverDef.V2)
E0930 09:20:13.100356 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
E0930 09:20:13.100400 140002471614208 base_runner.py:219]     self.build()
E0930 09:20:13.100444 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 840, in build
E0930 09:20:13.100488 140002471614208 base_runner.py:219]     self._build(self._filename, build_save=True, build_restore=True)
E0930 09:20:13.100532 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 878, in _build
E0930 09:20:13.100577 140002471614208 base_runner.py:219]     build_restore=build_restore)
E0930 09:20:13.100620 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 502, in _build_internal
E0930 09:20:13.100664 140002471614208 base_runner.py:219]     restore_sequentially, reshape)
E0930 09:20:13.100708 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 381, in _AddShardedRestoreOps
E0930 09:20:13.100752 140002471614208 base_runner.py:219]     name="restore_shard"))
E0930 09:20:13.100796 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
E0930 09:20:13.100844 140002471614208 base_runner.py:219]     restore_sequentially)
E0930 09:20:13.100889 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
E0930 09:20:13.100933 140002471614208 base_runner.py:219]     return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
E0930 09:20:13.100978 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
E0930 09:20:13.101022 140002471614208 base_runner.py:219]     name=name)
E0930 09:20:13.101066 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py", line 793, in _apply_op_helper
E0930 09:20:13.101110 140002471614208 base_runner.py:219]     op_def=op_def)
E0930 09:20:13.101154 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
E0930 09:20:13.101198 140002471614208 base_runner.py:219]     return func(*args, **kwargs)
E0930 09:20:13.101242 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
E0930 09:20:13.101286 140002471614208 base_runner.py:219]     attrs, op_def, compute_device)
E0930 09:20:13.101330 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
E0930 09:20:13.101374 140002471614208 base_runner.py:219]     op_def=op_def)
E0930 09:20:13.101419 140002471614208 base_runner.py:219]   File "usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
E0930 09:20:13.101463 140002471614208 base_runner.py:219]     self._traceback = tf_stack.extract_stack()
E0930 09:20:13.101507 140002471614208 base_runner.py:219] 
E0930 09:20:13.101552 140002471614208 base_runner.py:219] 
I0930 09:20:14.737253 140002463221504 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1388, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 8.13 seconds before retrying.
I0930 09:20:14.738214 140002463221504 trainer.py:466] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
I0930 09:20:22.873297 140002463221504 retry.py:71] Retry: caught exception: _WaitTillInit while running tensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
. Call failed at (most recent call last):
  File "/usr/lib/python3.6/threading.py", line 884, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 430, in Start
    self._RunLoop('trainer', self._Loop)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/base_runner.py", line 168, in _RunLoop
    loop_func(*loop_args)
Traceback for above exception (most recent call last):
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/core/retry.py", line 53, in Wrapper
    return func(*args, **kwargs)
  File "/home/ssy/lingvo/bazel-bin/lingvo/trainer.runfiles/__main__/lingvo/trainer.py", line 463, in _WaitTillInit
    global_step = sess.run(py_utils.GetGlobalStep())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 956, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1361, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py", line 1388, in _do_call
    raise type(e)(node_def, op, message)
Waiting for 12.59 seconds before retrying.
I0930 09:20:22.874253 140002463221504 trainer.py:466] Probably the expected race on global_step: From /job:local/replica:0/task:0:
Attempting to use uninitialized value global_step
	 [[{{node _send_global_step_0}}]]
